# ========== å¯¼å…¥åº“ ==========
import os
import math
import glob
import argparse
import yaml

import torch
import torch.nn.functional as F
from PIL import Image
from tqdm.auto import tqdm

from torchvision import transforms

from transformers import CLIPTextModel, CLIPTokenizer

from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel
from diffusers.optimization import get_scheduler
from diffusers.training_utils import compute_snr

from peft import LoraConfig, get_peft_model, PeftModel

from utils.config_manager import load_config


# è®¾ç½®è®¾å¤‡
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")

# å›¾ç‰‡åç¼€åˆ—è¡¨
IMAGE_EXTENSIONS = [".png", ".jpg", ".jpeg", ".webp", ".bmp", ".PNG", ".JPG", ".JPEG", ".WEBP", ".BMP"]

class Text2ImageDataset(torch.utils.data.Dataset):
    """
    ç”¨äºæ„å»ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¾®è°ƒæ•°æ®é›†

    å‚æ•°:
    - images_folder: str, å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
    - captions_folder: str, æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„
    - transform: function, å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º torch.Tensor
    - tokenizer: CLIPTokenizer, å°†æ–‡æœ¬æ ‡æ³¨è½¬ä¸º token ids

    è¿”å›:
    - 
    """
    def __init__(self, images_folder, captions_folder, transform, tokenizer):
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶è·¯å¾„
        self.image_paths = []
        for ext in IMAGE_EXTENSIONS:
            self.image_paths.extend(glob.glob(os.path.join(images_folder, f"*{ext}")))
        self.image_paths = sorted(self.image_paths)

        # åŠ è½½å¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨
        caption_paths = sorted(glob.glob(os.path.join(captions_folder, "*.txt")))
        captions = []
        for p in caption_paths:
            with open(p, "r", encoding="utf-8") as f:
                captions.append(f.readline().strip())

        # ç¡®ä¿å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸€è‡´
        if len(captions) != len(self.image_paths):
            raise ValueError("å›¾åƒæ•°é‡ä¸æ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ã€‚")

        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º tokens
        inputs = tokenizer(
            captions, max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
        )
        self.input_ids = inputs.input_ids
        self.transform = transform

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        input_id = self.input_ids[idx]
        try:
            # åŠ è½½å›¾åƒå¹¶è½¬æ¢ä¸º RGB æ¨¡å¼ï¼Œç„¶ååº”ç”¨æ•°æ®å¢å¼º
            image = Image.open(img_path).convert("RGB")
            tensor = self.transform(image)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒè·¯å¾„: {img_path}, é”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªå…¨é›¶çš„å¼ é‡å’Œç©ºçš„è¾“å…¥ ID ä»¥é¿å…å´©æºƒ
            tensor = torch.zeros((3, resolution, resolution))
            input_id = torch.zeros_like(input_id)
        
        return tensor, input_id

    def __len__(self):
        return len(self.image_paths)

def prepare_lora_model(lora_config, pretrained_model_name_or_path, model_path, weight_dtype, resume=False):
    """
    åŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ LoRA å±‚ã€‚

    å‚æ•°:
    - lora_config: LoraConfig, LoRA çš„é…ç½®å¯¹è±¡
    - pretrained_model_name_or_path: str, Hugging Face ä¸Šçš„æ¨¡å‹åç§°æˆ–è·¯å¾„
    - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
    - weight_dtype: torch.dtype, æ¨¡å‹æƒé‡çš„æ•°æ®ç±»å‹
    - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤

    è¿”å›:
    - tokenizer: CLIPTokenizer
    - noise_scheduler: DDPMScheduler
    - unet: UNet2DConditionModel
    - vae: AutoencoderKL
    - text_encoder: CLIPTextModel
    """
    # åŠ è½½å™ªå£°è°ƒåº¦å™¨
    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder="scheduler")

    # åŠ è½½ Tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="tokenizer"
    )

    # åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨
    text_encoder = CLIPTextModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="text_encoder"
    )

    # åŠ è½½ VAE æ¨¡å‹
    vae = AutoencoderKL.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="vae"
    )

    # åŠ è½½ UNet æ¨¡å‹
    unet = UNet2DConditionModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="unet"
    )
    
    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡
    if resume:
        if model_path is None or not os.path.exists(model_path):
            raise ValueError("å½“ resume è®¾ç½®ä¸º True æ—¶ï¼Œå¿…é¡»æä¾›æœ‰æ•ˆçš„ model_path")
        # ä½¿ç”¨ PEFT çš„ from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹
        text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, "text_encoder"))
        unet = PeftModel.from_pretrained(unet, os.path.join(model_path, "unet"))

        # ç¡®ä¿ UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True
        for param in unet.parameters():
            if not param.requires_grad:
                param.requires_grad = True
        for param in text_encoder.parameters():
            if not param.requires_grad:
                param.requires_grad = True
                
        print(f"âœ… å·²ä» {model_path} æ¢å¤æ¨¡å‹æƒé‡")

    else:
        # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet
        text_encoder = get_peft_model(text_encoder, lora_config)
        unet = get_peft_model(unet, lora_config)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        print("ğŸ“Š Text Encoder å¯è®­ç»ƒå‚æ•°:")
        text_encoder.print_trainable_parameters()
        print("ğŸ“Š UNet å¯è®­ç»ƒå‚æ•°:")
        unet.print_trainable_parameters()
    
    # å†»ç»“ VAE å‚æ•°
    vae.requires_grad_(False)

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹
    unet.to(DEVICE, dtype=weight_dtype)
    vae.to(DEVICE, dtype=weight_dtype)
    text_encoder.to(DEVICE, dtype=weight_dtype)
    
    return tokenizer, noise_scheduler, unet, vae, text_encoder

def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):
    """
    ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°åˆ†åˆ«è®¾ç½®ä¼˜åŒ–å™¨ï¼Œå¹¶æŒ‡å®šä¸åŒçš„å­¦ä¹ ç‡ã€‚

    å‚æ•°:
    - unet: UNet2DConditionModel, Hugging Face çš„ UNet æ¨¡å‹
    - text_encoder: CLIPTextModel, Hugging Face çš„æ–‡æœ¬ç¼–ç å™¨
    - unet_learning_rate: float, UNet çš„å­¦ä¹ ç‡
    - text_encoder_learning_rate: float, æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡

    è¿”å›:
    - ä¼˜åŒ–å™¨ Optimizer
    """
    # ç­›é€‰å‡º UNet ä¸­éœ€è¦è®­ç»ƒçš„ LoRA å±‚å‚æ•°
    unet_lora_layers = [p for p in unet.parameters() if p.requires_grad]
    
    # ç­›é€‰å‡ºæ–‡æœ¬ç¼–ç å™¨ä¸­éœ€è¦è®­ç»ƒçš„ LoRA å±‚å‚æ•°
    text_encoder_lora_layers = [p for p in text_encoder.parameters() if p.requires_grad]
    
    # å°†éœ€è¦è®­ç»ƒçš„å‚æ•°åˆ†ç»„å¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    trainable_params = [
        {"params": unet_lora_layers, "lr": unet_learning_rate},
        {"params": text_encoder_lora_layers, "lr": text_encoder_learning_rate}
    ]
    
    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(trainable_params)
    
    return optimizer

def collate_fn(examples):
    pixel_values = []
    input_ids = []
    
    for tensor, input_id in examples:
        pixel_values.append(tensor)
        input_ids.append(input_id)
    
    pixel_values = torch.stack(pixel_values, dim=0).float()
    input_ids = torch.stack(input_ids, dim=0)
    
    return {"pixel_values": pixel_values, "input_ids": input_ids}

# ========== å‚æ•°è®¾ç½® ==========

def parse_args(config):
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ LoRA å¾®è°ƒ Stable Diffusion æ¨¡å‹",
        epilog="ç¤ºä¾‹: python train_lora.py -d ./data/Brad -c ./data/Brad/captions"
    )

    # ç®€çŸ­é€‰é¡¹å’Œé•¿é€‰é¡¹
    parser.add_argument("-d", "--dataset_path", type=str, default=config.get('dataset_path'),
                        help="æ•°æ®é›†è·¯å¾„ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.dataset_path'")
    parser.add_argument("-c", "--captions_folder", type=str, default=None,
                        help="æ–‡æœ¬æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º dataset_path")
    parser.add_argument("-r", "--root", type=str, default=config.get('root', './SD'),
                        help="æ ¹è·¯å¾„ï¼Œé»˜è®¤ä¸º './SD'")
    parser.add_argument("-m", "--model_path", type=str, default=None,
                        help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œé»˜è®¤ä¸º root + dataset_name + 'logs/checkpoint-last'")
    parser.add_argument("-p", "--pretrained_model_name_or_path", type=str, default=config.get('pretrained_model_name_or_path'),
                        help="é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.pretrained_model_name_or_path'")
    parser.add_argument("-s", "--seed", type=int, default=config.get('seed', 1126),
                        help="éšæœºæ•°ç§å­ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.seed' æˆ– 1126")
    parser.add_argument("-w", "--weight_dtype", type=str, default=config.get('weight_dtype', 'torch.bfloat16'),
                        help="æƒé‡æ•°æ®ç±»å‹ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.weight_dtype'")
    parser.add_argument("-b", "--batch_size", type=int, default=config.get('batch_size', 2),
                        help="è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.batch_size'")
    parser.add_argument("-e", "--max_train_steps", type=int, default=config.get('max_train_steps', 200),
                        help="æ€»è®­ç»ƒæ­¥æ•°ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.max_train_steps'")
    parser.add_argument("-u", "--unet_learning_rate", type=float, default=config.get('unet_learning_rate', 1e-4),
                        help="UNet çš„å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.unet_learning_rate'")
    parser.add_argument("-t", "--text_encoder_learning_rate", type=float, default=config.get('text_encoder_learning_rate', 1e-4),
                        help="æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.text_encoder_learning_rate'")
    parser.add_argument("-g", "--snr_gamma", type=float, default=config.get('snr_gamma', 5),
                        help="SNR å‚æ•°ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.snr_gamma'")
    parser.add_argument("-l", "--lr_scheduler_name", type=str, default=config.get('lr_scheduler_name', "cosine_with_restarts"),
                        help="å­¦ä¹ ç‡è°ƒåº¦å™¨åç§°ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.lr_scheduler_name'")
    parser.add_argument("-warmup", "--lr_warmup_steps", type=int, default=config.get('lr_warmup_steps', 100),
                        help="å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.lr_warmup_steps'")
    parser.add_argument("-cycle", "--num_cycles", type=int, default=config.get('num_cycles', 3),
                        help="å­¦ä¹ ç‡è°ƒåº¦å™¨çš„å‘¨æœŸæ•°é‡ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'train.num_cycles'")
    parser.add_argument("--resume", action="store_true", default=config.get('resume', False),
                        help="æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤ï¼Œé»˜è®¤ä¸º False")
    
    args = parser.parse_args()

    # å¦‚æœ captions_folder æœªæŒ‡å®šï¼Œåˆ™é»˜è®¤ä¸º dataset_pathï¼Œæ„æ€æ˜¯å›¾ç‰‡å’Œæ ‡æ³¨åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹          
    if args.captions_folder is None:
        args.captions_folder = args.dataset_path

    # æƒé‡æ•°æ®ç±»å‹æ£€æŸ¥
    if args.weight_dtype:
        try:
            args.weight_dtype = getattr(torch, args.weight_dtype.split('.')[-1])
        except AttributeError:
            print(f"âš ï¸ æ— æ•ˆçš„ weight_dtype '{args.weight_dtype}'ï¼Œä½¿ç”¨é»˜è®¤ 'torch.float32'")
            args.weight_dtype = torch.float32
            
    # è‡ªåŠ¨è®¾ç½® model_pathï¼Œå¦‚æœæœªæŒ‡å®š
    if args.model_path is None:
        dataset_name = os.path.basename(os.path.abspath(args.dataset_path))
        args.model_path = os.path.join(args.root, dataset_name, "logs", "checkpoint-last")

    return args


def main():
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šè„šæœ¬åç§°ä¸º 'train'
    config = load_config('config.yaml', script_name='train')
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args(config)
    
    # è®¾ç½®éšæœºæ•°ç§å­
    seed = args.seed
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # è‡ªåŠ¨è®¾ç½®é¡¹ç›®åç§°ï¼ˆæ•°æ®é›†æ–‡ä»¶å¤¹åç§°ï¼‰
    dataset_name = os.path.basename(os.path.abspath(args.dataset_path))
    
    # è¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = os.path.dirname(args.model_path)
    os.makedirs(output_folder, exist_ok=True)
    
    # è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡
    global resolution
    resolution = 512
    
    # æ•°æ®å¢å¼ºæ“ä½œ
    train_transform = transforms.Compose(
        [
            transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°
            transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ
            transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
            transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
        ]
    )
    
    # LoRA é…ç½®
    lora_config = LoraConfig(
        r=32,
        lora_alpha=16,
        target_modules=[
            "q_proj", "v_proj", "k_proj", "out_proj",
            "to_k", "to_q", "to_v", "to_out.0"
        ],
        lora_dropout=0
    )
    
    # ========== å¾®è°ƒå‰çš„å‡†å¤‡ ==========
    
    # åˆå§‹åŒ– tokenizerï¼Œç”¨äºåŠ è½½æ•°æ®é›†
    tokenizer = CLIPTokenizer.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="tokenizer"
    )
    
    # å‡†å¤‡æ•°æ®é›†
    dataset = Text2ImageDataset(
        images_folder=args.dataset_path,
        captions_folder=args.captions_folder,
        transform=train_transform,
        tokenizer=tokenizer,
    )
    
    train_dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle=True,
        collate_fn=collate_fn,
        batch_size=args.batch_size,
        num_workers=4,
    )
    
    print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    
    # å‡†å¤‡æ¨¡å‹
    tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(
        lora_config,
        pretrained_model_name_or_path=args.pretrained_model_name_or_path,
        model_path=args.model_path,
        weight_dtype = args.weight_dtype,
        resume=args.resume
    )
    
    # å‡†å¤‡ä¼˜åŒ–å™¨
    optimizer = prepare_optimizer(
        unet, 
        text_encoder, 
        unet_learning_rate=args.unet_learning_rate, 
        text_encoder_learning_rate=args.text_encoder_learning_rate
    )
    
    # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        args.lr_scheduler_name,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps,
        num_training_steps=args.max_train_steps,
        num_cycles=args.num_cycles
    )
    
    print("âœ… æ¨¡å‹å’Œä¼˜åŒ–å™¨å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    
    # ========== å¼€å§‹å¾®è°ƒ ==========
    
    # ç¦ç”¨å¹¶è¡ŒåŒ–ï¼Œé¿å…è­¦å‘Š
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # åˆå§‹åŒ–
    global_step = 0
    
    # è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    progress_bar = tqdm(
        range(args.max_train_steps),
        desc="è®­ç»ƒæ­¥éª¤",
    )
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(math.ceil(args.max_train_steps / len(train_dataloader))):
        unet.train()
        text_encoder.train()
        
        for step, batch in enumerate(train_dataloader):
            if global_step >= args.max_train_steps:
                break
            
            # ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰
            latents = vae.encode(batch["pixel_values"].to(DEVICE, dtype=args.weight_dtype)).latent_dist.sample()
            latents = latents * vae.config.scaling_factor  # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨ç©ºé—´

            # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ
            noise = torch.randn_like(latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

            # è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º
            encoder_hidden_states = text_encoder(batch["input_ids"].to(DEVICE))[0]

            # è®¡ç®—ç›®æ ‡å€¼
            if noise_scheduler.config.prediction_type == "epsilon":
                target = noise  # é¢„æµ‹å™ªå£°
            elif noise_scheduler.config.prediction_type == "v_prediction":
                target = noise_scheduler.get_velocity(latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡

            # UNet æ¨¡å‹é¢„æµ‹
            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states)[0]

            # è®¡ç®—æŸå¤±
            if not args.snr_gamma:
                loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
            else:
                # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±
                snr = compute_snr(noise_scheduler, timesteps)
                mse_loss_weights = torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]
                if noise_scheduler.config.prediction_type == "epsilon":
                    mse_loss_weights = mse_loss_weights / snr
                elif noise_scheduler.config.prediction_type == "v_prediction":
                    mse_loss_weights = mse_loss_weights / (snr + 1)
                
                # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±
                loss = F.mse_loss(model_pred.float(), target.float(), reduction="none")
                loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                loss = loss.mean()

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)
            global_step += 1

            # æ‰“å°è®­ç»ƒæŸå¤±
            if global_step % 50 == 0 or global_step == args.max_train_steps:
                print(f"ğŸ”¥ æ­¥éª¤ {global_step}, æŸå¤±: {loss.item():.4f}")

            # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œæ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡
            if global_step % 100 == 0:
                save_path = os.path.join(output_folder, f"checkpoint-{global_step}")
                os.makedirs(save_path, exist_ok=True)

                # ä½¿ç”¨ save_pretrained ä¿å­˜ PeftModel
                unet.save_pretrained(os.path.join(save_path, "unet"))
                text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
                print(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´æ¨¡å‹åˆ° {save_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° checkpoint-last
    save_path = args.model_path
    os.makedirs(save_path, exist_ok=True)
    unet.save_pretrained(os.path.join(save_path, "unet"))
    text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
    print(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {save_path}")

    print("ğŸ‰ å¾®è°ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()