# ========== å¯¼å…¥åº“ ==========
import os
import glob
import argparse

import torch
from tqdm.auto import tqdm

from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor

from diffusers import AutoencoderKL, UNet2DConditionModel, DiffusionPipeline

from peft import PeftModel

from PIL import Image
import numpy as np
import cv2

from utils.config_manager import load_config

# è®¾ç½®å…¨å±€è®¾å¤‡
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")


def load_validation_prompts(validation_prompt_path):
    """
    åŠ è½½éªŒè¯æç¤ºæ–‡æœ¬ã€‚
    
    å‚æ•°:
    - validation_prompt_path: str, éªŒè¯æç¤ºæ–‡ä»¶çš„è·¯å¾„ï¼Œæ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ª prompt
    
    è¿”å›:
    - validation_prompt: list, prompt åˆ—è¡¨
    """
    with open(validation_prompt_path, "r", encoding="utf-8") as f:
        validation_prompt = [line.strip() for line in f.readlines()]
    return validation_prompt

def generate_images(pipeline, prompts, num_inference_steps=50, guidance_scale=7.5, save_folder="inference", generator=None):
    """
    ä½¿ç”¨ DiffusionPipeline ç”Ÿæˆå›¾åƒï¼Œä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹å¹¶è¿”å›ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ã€‚

    å‚æ•°:
    - pipeline: DiffusionPipeline, å·²åŠ è½½å¹¶é…ç½®å¥½çš„ Pipeline
    - prompts: list, æ–‡æœ¬æç¤ºåˆ—è¡¨
    - num_inference_steps: int, æ¨ç†æ­¥éª¤æ•°
    - guidance_scale: float, æŒ‡å¯¼å°ºåº¦
    - save_folder: str, ä¿å­˜ç”Ÿæˆå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„
    - generator: torch.Generator, æ§åˆ¶ç”Ÿæˆéšæœºæ•°çš„ç§å­

    è¿”å›:
    - generated_images: list, ç”Ÿæˆçš„ PIL å›¾åƒå¯¹è±¡åˆ—è¡¨
    """
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
    os.makedirs(save_folder, exist_ok=True)
    generated_images = []
    
    for i, prompt in enumerate(tqdm(prompts, desc="ç”Ÿæˆå›¾åƒä¸­")):
        # ä½¿ç”¨ pipeline ç”Ÿæˆå›¾åƒ
        image = pipeline(prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=generator).images[0]
        
        # ä¿å­˜å›¾åƒåˆ°æŒ‡å®šæ–‡ä»¶å¤¹
        save_file = os.path.join(save_folder, f"generated_{i+1}.png")
        image.save(save_file)
        
        # å°†å›¾åƒä¿å­˜åˆ°åˆ—è¡¨ä¸­ï¼Œç¨åè¿”å›
        generated_images.append(image)
    
    print(f"âœ… å·²ç”Ÿæˆå¹¶ä¿å­˜ {len(prompts)} å¼ å›¾åƒåˆ° {save_folder}")
    
    return generated_images

def prepare_lora_model(pretrained_model_name_or_path, model_path, weight_dtype):
    """
    åŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ LoRA å±‚ï¼Œå¹¶åˆå¹¶æƒé‡ã€‚
    
    å‚æ•°:
    - pretrained_model_name_or_path: str, é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„
    - model_path: str, å¾®è°ƒæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
    - weight_dtype: torch.dtype, æ¨¡å‹æƒé‡çš„æ•°æ®ç±»å‹
    
    è¿”å›:
    - tokenizer: CLIPTokenizer
    - unet: UNet2DConditionModel
    - text_encoder: CLIPTextModel
    """
    # åŠ è½½ Tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="tokenizer"
    )

    # åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨
    text_encoder = CLIPTextModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="text_encoder"
    )

    # åŠ è½½ UNet æ¨¡å‹
    unet = UNet2DConditionModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="unet"
    )
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if model_path is None or not os.path.exists(model_path):
        raise ValueError("å¿…é¡»æä¾›æœ‰æ•ˆçš„ model_path")
    
    # ä½¿ç”¨ PEFT çš„ from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹
    text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, "text_encoder"))
    unet = PeftModel.from_pretrained(unet, os.path.join(model_path, "unet"))

    # åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹
    text_encoder = text_encoder.merge_and_unload()
    unet = unet.merge_and_unload()

    # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
    text_encoder.eval()
    unet.eval()

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹
    unet.to(DEVICE, dtype=weight_dtype)
    text_encoder.to(DEVICE, dtype=weight_dtype)
    
    return tokenizer, unet, text_encoder


def parse_args(config):
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼ã€‚

    å‚æ•°:
    - config: dict, é…ç½®æ–‡ä»¶å†…å®¹

    è¿”å›:
    - args: argparse.Namespace, è§£æåçš„å‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆå›¾åƒ",
        epilog="ç¤ºä¾‹: python generate_images.py -i ./prompts/validation_prompt.txt"
    )
    
    # ç®€çŸ­é€‰é¡¹å’Œé•¿é€‰é¡¹
    parser.add_argument("-i", "--prompts_path", type=str, default=config.get('prompts_path'),
                        help="éªŒè¯æç¤ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'generate.prompts_path'")
    parser.add_argument("-r", "--root", type=str, default=config.get('root', './SD'),
                        help="æ ¹è·¯å¾„ï¼Œé»˜è®¤ä¸º './SD'")
    parser.add_argument("-m", "--model_path", type=str, default=None,
                        help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœä¸ºç©ºåˆ™æ ¹æ® root å’Œ train.dataset_name æ„é€ ")
    parser.add_argument("-s", "--save_folder", type=str, default=config.get('save_folder'),
                        help="ä¿å­˜ç”Ÿæˆå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º root + train.dataset_name + '/inference'")
    parser.add_argument("-p", "--pretrained_model_name_or_path", type=str, default=config.get('pretrained_model_name_or_path'),
                        help="é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'generate.pretrained_model_name_or_path'")
    parser.add_argument("-n", "--num_inference_steps", type=int, default=config.get('num_inference_steps', 50),
                        help="æ¨ç†æ­¥éª¤æ•°ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'generate.num_inference_steps'")
    parser.add_argument("-g", "--guidance_scale", type=float, default=config.get('guidance_scale', 7.5),
                        help="æŒ‡å¯¼å°ºåº¦ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'generate.guidance_scale'")
    parser.add_argument("-w", "--weight_dtype", type=str, default=config.get('weight_dtype'),
                        help="æƒé‡æ•°æ®ç±»å‹ï¼Œå¦‚ 'torch.bfloat16' æˆ– 'torch.float32'ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'generate.weight_dtype'")
    parser.add_argument("-e", "--seed", type=int, default=config.get('seed', 1126),
                        help="éšæœºæ•°ç§å­ï¼Œé»˜è®¤ä¸º config.yaml ä¸­çš„ 'generate.seed' æˆ– 1126")
    
    args = parser.parse_args()

    # æƒé‡æ•°æ®ç±»å‹æ£€æŸ¥
    if args.weight_dtype:
        try:
            args.weight_dtype = getattr(torch, args.weight_dtype.split('.')[-1])
        except AttributeError:
            print(f"âš ï¸ æ— æ•ˆçš„ weight_dtype '{args.weight_dtype}'ï¼Œä½¿ç”¨é»˜è®¤ 'torch.float32'")
            args.weight_dtype = torch.float32

    # æœ‰éœ€è¦çš„è¯åŠ è½½ train_config è¿›è¡Œé»˜è®¤é…ç½®
    if args.model_path is None or args.save_folder is None:
        train_config = load_config(script_name='train')
    
        # è‡ªåŠ¨è®¾ç½® model_pathï¼Œå¦‚æœæœªæŒ‡å®š
        if args.model_path is None:
            # å°è¯•ä» config.yaml ä¸­è·å– dataset_path
            dataset_path = train_config.get('dataset_path')
            if dataset_path is None:
                raise ValueError("model_path ä¸ºç©ºä¸”æ— æ³•ä» config.yaml ä¸­è·å– dataset_pathï¼Œè¯·æŒ‡å®š model_path æˆ–åœ¨ config.yaml ä¸­æä¾› dataset_path")
            dataset_name = os.path.basename(os.path.abspath(dataset_path))
            args.model_path = os.path.join(args.root, dataset_name, "logs", "checkpoint-last")
    
        # è‡ªåŠ¨è®¾ç½® save_folderï¼Œå¦‚æœæœªæŒ‡å®š
        if args.save_folder is None:
            dataset_name = os.path.basename(os.path.abspath(dataset_path))
            args.save_folder = os.path.join(args.root, dataset_name, "inference")
    
    return args


def main():
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šè„šæœ¬åç§°ä¸º 'generate'
    config= load_config(script_name='generate')
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args(config)
    
    # è®¾ç½®éšæœºæ•°ç§å­
    seed = args.seed
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # å‡†å¤‡ LoRA æ¨¡å‹
    print("âœ… å‡†å¤‡ LoRA æ¨¡å‹...")
    tokenizer, unet, text_encoder = prepare_lora_model(
        pretrained_model_name_or_path=args.pretrained_model_name_or_path,
        model_path=args.model_path,
        weight_dtype=args.weight_dtype,
    )
    
    # åˆ›å»º DiffusionPipeline å¹¶æ›´æ–°å…¶ç»„ä»¶
    print("ğŸ”„ åˆ›å»º DiffusionPipeline...")
    pipeline = DiffusionPipeline.from_pretrained(
        args.pretrained_model_name_or_path,
        unet=unet,
        text_encoder=text_encoder,
        torch_dtype=args.weight_dtype,
        safety_checker=None,
    )
    pipeline = pipeline.to(DEVICE)
    
    # è®¾ç½®éšæœºæ•°ç§å­
    generator = torch.Generator(device=DEVICE)
    generator.manual_seed(seed)
    
    # åŠ è½½éªŒè¯æç¤º
    print("ğŸ“‚ åŠ è½½éªŒè¯æç¤º...")
    validation_prompts = load_validation_prompts(args.prompts_path)
    
    # ç”Ÿæˆå›¾åƒ
    generate_images(
        pipeline=pipeline,
        prompts=validation_prompts,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        save_folder=args.save_folder,
        generator=generator
    )

if __name__ == "__main__":
    main()