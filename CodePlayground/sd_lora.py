# ===============================================================
# è¿™æ˜¯ä¸€ä¸ª Toy AI ç»˜ç”»å·¥å…·ï¼Œç”¨äºåœ¨å‘½ä»¤è¡Œä½¿ç”¨ LoRA å¾®è°ƒ Stable Diffusion å¹¶ç”Ÿæˆå›¾åƒ
# å¦‚æœé‡åˆ°é”™è¯¯ï¼Œæ¬¢è¿é€šè¿‡ Issues æˆ– Discussions æäº¤åé¦ˆã€‚ä¸ºäº†æ›´å¿«è§£å†³é—®é¢˜ï¼Œè¯·å°½å¯èƒ½é™„ä¸Šè¿è¡Œç¯å¢ƒå’Œå¯å¤ç°çš„å‘½ä»¤ã€‚

# å¯¹åº”æ–‡ç« ï¼šã€Š16. ç”¨ LoRA å¾®è°ƒ Stable Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡ AI ç»˜ç”»ï¼‰ã€‹
# https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/16.%20ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡%20AI%20ç»˜ç”».md

# ä½¿ç”¨æ–¹æ³•ï¼š
#   python sd_lora.py [å‚æ•°å¯é€‰]
# ç¤ºä¾‹ï¼š
#   python sd_lora.py -d ./Datasets/Brad -gp ./Datasets/prompts/validation_prompt.txt

# æŸ¥çœ‹å®Œæ•´å¸®åŠ©ï¼šä½¿ç”¨ -h æˆ– --help
# ===============================================================

import os
import math
import glob
import argparse
import yaml

import torch
import torch.nn.functional as F
from PIL import Image
from tqdm.auto import tqdm

from torchvision import transforms

from transformers import CLIPTextModel, CLIPTokenizer

from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, DiffusionPipeline
from diffusers.optimization import get_scheduler
from diffusers.training_utils import compute_snr

from peft import LoraConfig, get_peft_model, PeftModel

from utils.config_manager import load_config

# è®¾ç½®è®¾å¤‡
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")

# å›¾ç‰‡åç¼€åˆ—è¡¨
IMAGE_EXTENSIONS = [".png", ".jpg", ".jpeg", ".webp", ".bmp", ".PNG", ".JPG", ".JPEG", ".WEBP", ".BMP"]

# æŒ‡å®šè®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡ï¼Œæ‰€æœ‰å›¾åƒéƒ½ä¼š resize å¤„ç†
resolution = 512


class Text2ImageDataset(torch.utils.data.Dataset):
    """
    ç”¨äºæ„å»ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¾®è°ƒæ•°æ®é›†ã€‚
    
    ä½ å¯ä»¥æ ¹æ®éœ€æ±‚å®šåˆ¶è¿™ä¸ªç±»ï¼Œä¾‹å¦‚é€‚é…ç‰¹å®šæ ¼å¼çš„æ•°æ®é›†æˆ–æ›´æ”¹æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä¿æŒè¿”å›å½¢å¼ä¸€è‡´å°±å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒã€‚

    å‚æ•°:
    - images_folder: str, å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
    - captions_folder: str, æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„
    - transform: function, å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º torch.Tensor
    - tokenizer: CLIPTokenizer, å°†æ–‡æœ¬æ ‡æ³¨è½¬ä¸º token ids

    è¿”å›:
    - (image_tensor, input_ids): ä¸€ä¸ªåŒ…å«å›¾åƒ Tensor å’Œå¯¹åº”æ–‡æœ¬ token ids çš„å…ƒç»„ã€‚
    """
    def __init__(self, images_folder, captions_folder, transform, tokenizer):
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶è·¯å¾„
        self.image_paths = []
        for ext in IMAGE_EXTENSIONS:
            self.image_paths.extend(glob.glob(os.path.join(images_folder, f"*{ext}")))
        self.image_paths = sorted(self.image_paths)

        # åŠ è½½å¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨
        caption_paths = sorted(glob.glob(os.path.join(captions_folder, "*.txt")))
        captions = []
        for p in caption_paths:
            with open(p, "r", encoding="utf-8") as f:
                captions.append(f.readline().strip())

        # ç¡®ä¿å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸€è‡´
        if len(captions) != len(self.image_paths):
            raise ValueError("å›¾åƒæ•°é‡ä¸æ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ã€‚")

        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º tokens
        inputs = tokenizer(
            captions, max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
        )
        self.input_ids = inputs.input_ids
        self.transform = transform

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        input_id = self.input_ids[idx]
        try:
            # åŠ è½½å›¾åƒå¹¶è½¬æ¢ä¸º RGB æ¨¡å¼ï¼Œç„¶ååº”ç”¨æ•°æ®å¢å¼º
            image = Image.open(img_path).convert("RGB")
            tensor = self.transform(image)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒè·¯å¾„: {img_path}, é”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªå…¨é›¶çš„å¼ é‡å’Œç©ºçš„è¾“å…¥ ID ä»¥é¿å…å´©æºƒ
            tensor = torch.zeros((3, resolution, resolution))
            input_id = torch.zeros_like(input_id)
        
        return tensor, input_id

    def __len__(self):
        return len(self.image_paths)

def prepare_lora_model(lora_config, pretrained_model_name_or_path, model_path, weight_dtype, resume=False):
    """
    åŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ LoRA å±‚ã€‚

    å‚æ•°:
    - lora_config: LoraConfig, LoRA çš„é…ç½®å¯¹è±¡
    - pretrained_model_name_or_path: str, Hugging Face ä¸Šçš„æ¨¡å‹åç§°æˆ–è·¯å¾„
    - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
    - weight_dtype: torch.dtype, æ¨¡å‹æƒé‡çš„æ•°æ®ç±»å‹
    - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤

    è¿”å›:
    - tokenizer: CLIPTokenizer
    - noise_scheduler: DDPMScheduler
    - unet: UNet2DConditionModel
    - vae: AutoencoderKL
    - text_encoder: CLIPTextModel
    """
    # åŠ è½½å™ªå£°è°ƒåº¦å™¨
    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder="scheduler")

    # åŠ è½½ Tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="tokenizer"
    )

    # åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨
    text_encoder = CLIPTextModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="text_encoder"
    )

    # åŠ è½½ VAE æ¨¡å‹
    vae = AutoencoderKL.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="vae"
    )

    # åŠ è½½ UNet æ¨¡å‹
    unet = UNet2DConditionModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="unet"
    )
    
    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡
    if resume:
        if model_path is None or not os.path.exists(model_path):
            raise ValueError("å½“ resume è®¾ç½®ä¸º True æ—¶ï¼Œå¿…é¡»æä¾›æœ‰æ•ˆçš„ model_path")
        # ä½¿ç”¨ PEFT çš„ from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹
        text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, "text_encoder"))
        unet = PeftModel.from_pretrained(unet, os.path.join(model_path, "unet"))

        # ç¡®ä¿ UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True
        for param in unet.parameters():
            if not param.requires_grad:
                param.requires_grad = True
        for param in text_encoder.parameters():
            if not param.requires_grad:
                param.requires_grad = True
                
        print(f"âœ… å·²ä» {model_path} æ¢å¤æ¨¡å‹æƒé‡")

    else:
        # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet
        text_encoder = get_peft_model(text_encoder, lora_config)
        unet = get_peft_model(unet, lora_config)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        print("ğŸ“Š Text Encoder å¯è®­ç»ƒå‚æ•°:")
        text_encoder.print_trainable_parameters()
        print("ğŸ“Š UNet å¯è®­ç»ƒå‚æ•°:")
        unet.print_trainable_parameters()
    
    # å†»ç»“ VAE å‚æ•°
    vae.requires_grad_(False)

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹
    unet.to(DEVICE, dtype=weight_dtype)
    vae.to(DEVICE, dtype=weight_dtype)
    text_encoder.to(DEVICE, dtype=weight_dtype)
    
    return tokenizer, noise_scheduler, unet, vae, text_encoder

def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):
    """
    ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°åˆ†åˆ«è®¾ç½®ä¼˜åŒ–å™¨ï¼Œå¹¶æŒ‡å®šä¸åŒçš„å­¦ä¹ ç‡ã€‚

    å‚æ•°:
    - unet: UNet2DConditionModel, Hugging Face çš„ UNet æ¨¡å‹
    - text_encoder: CLIPTextModel, Hugging Face çš„æ–‡æœ¬ç¼–ç å™¨
    - unet_learning_rate: float, UNet çš„å­¦ä¹ ç‡
    - text_encoder_learning_rate: float, æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡

    è¿”å›:
    - ä¼˜åŒ–å™¨ Optimizer
    """
    # ç­›é€‰å‡º UNet ä¸­éœ€è¦è®­ç»ƒçš„ LoRA å±‚å‚æ•°
    unet_lora_layers = [p for p in unet.parameters() if p.requires_grad]
    
    # ç­›é€‰å‡ºæ–‡æœ¬ç¼–ç å™¨ä¸­éœ€è¦è®­ç»ƒçš„ LoRA å±‚å‚æ•°
    text_encoder_lora_layers = [p for p in text_encoder.parameters() if p.requires_grad]
    
    # å°†éœ€è¦è®­ç»ƒçš„å‚æ•°åˆ†ç»„å¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    trainable_params = [
        {"params": unet_lora_layers, "lr": unet_learning_rate},
        {"params": text_encoder_lora_layers, "lr": text_encoder_learning_rate}
    ]
    
    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(trainable_params)
    
    return optimizer

def collate_fn(examples):
    pixel_values = []
    input_ids = []
    
    for tensor, input_id in examples:
        pixel_values.append(tensor)
        input_ids.append(input_id)
    
    pixel_values = torch.stack(pixel_values, dim=0).float()
    input_ids = torch.stack(input_ids, dim=0)
    
    return {"pixel_values": pixel_values, "input_ids": input_ids}

def load_validation_prompts(validation_prompt_path):
    """
    åŠ è½½éªŒè¯æç¤ºæ–‡æœ¬ã€‚
    
    å‚æ•°:
    - validation_prompt_path: str, éªŒè¯æç¤ºæ–‡ä»¶çš„è·¯å¾„ï¼Œæ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ª promptï¼Œå‚è€ƒç¤ºä¾‹æ–‡ä»¶
    
    è¿”å›:
    - validation_prompt: list, prompt åˆ—è¡¨
    """
    with open(validation_prompt_path, "r", encoding="utf-8") as f:
        validation_prompt = [line.strip() for line in f.readlines()]
    return validation_prompt

def generate_images(pipeline, prompts, num_inference_steps=50, guidance_scale=7.5, save_folder="inference", generator=None):
    """
    ä½¿ç”¨ DiffusionPipeline ç”Ÿæˆå›¾åƒï¼Œä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹å¹¶è¿”å›ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ã€‚

    å‚æ•°:
    - pipeline: DiffusionPipeline, å·²åŠ è½½å¹¶é…ç½®å¥½çš„ Pipeline
    - prompts: list, æ–‡æœ¬æç¤ºåˆ—è¡¨
    - num_inference_steps: int, æ¨ç†æ­¥éª¤æ•°
    - guidance_scale: float, æŒ‡å¯¼å°ºåº¦
    - save_folder: str, ä¿å­˜ç”Ÿæˆå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„
    - generator: torch.Generator, æ§åˆ¶ç”Ÿæˆéšæœºæ•°çš„ç§å­

    è¿”å›:
    - generated_images: list, ç”Ÿæˆçš„ PIL å›¾åƒå¯¹è±¡åˆ—è¡¨
    """
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
    os.makedirs(save_folder, exist_ok=True)
    generated_images = []
    
    for i, prompt in enumerate(tqdm(prompts, desc="ç”Ÿæˆå›¾åƒä¸­")):
        # ä½¿ç”¨ pipeline ç”Ÿæˆå›¾åƒ
        image = pipeline(prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=generator).images[0]
        
        # ä¿å­˜å›¾åƒåˆ°æŒ‡å®šæ–‡ä»¶å¤¹
        save_file = os.path.join(save_folder, f"generated_{i+1}.png")
        image.save(save_file)
        
        # å°†å›¾åƒä¿å­˜åˆ°åˆ—è¡¨ä¸­ï¼Œç¨åè¿”å›
        generated_images.append(image)
    
    print(f"âœ… å·²ç”Ÿæˆå¹¶ä¿å­˜ {len(prompts)} å¼ å›¾åƒåˆ° {save_folder}")
    
    return generated_images

# ========== å‚æ•°è®¾ç½® ==========
def parse_args(train_config, generate_config):
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºè®­ç»ƒå’Œç”Ÿæˆæµç¨‹ã€‚
    
    é»˜è®¤å€¼å–è‡ª config.yaml çš„ 'train' å’Œ 'generate' é…ç½®ã€‚æ‰€æœ‰å‚æ•°å‡ä¸ºå¯é€‰é¡¹ã€‚
    """
    parser = argparse.ArgumentParser(
        description=(
            "ä½¿ç”¨ LoRA å¾®è°ƒ Stable Diffusion æ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒåç”Ÿæˆå›¾åƒã€‚\n"
            "å¦‚æœä¸ä¼ å…¥å¯¹åº”å‚æ•°ï¼Œåˆ™é»˜è®¤åœ¨ç›¸åº”é˜¶æ®µä½¿ç”¨ config.yaml ä¸­çš„é…ç½®ã€‚"
        ),
        epilog="ç¤ºä¾‹: python sd_lora.py -d ./Datasets/Brad -gp ./Datasets/prompts/validation_prompt.txt"
    )

    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument("-d", "--dataset_path", type=str, default=train_config.get('dataset_path'),
                        help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("-c", "--captions_folder", type=str, default=None,
                        help="æ–‡æœ¬æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆé»˜è®¤ä¸æ•°æ®é›†è·¯å¾„ç›¸åŒï¼‰")
    parser.add_argument("-r", "--root", type=str, default=train_config.get('root', './SD'),
                        help="æ ¹è·¯å¾„")
    parser.add_argument("-m", "--model_path", type=str, default=None,
                        help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œé»˜è®¤ä¸º root + dataset_name + 'logs/checkpoint-last'")
    parser.add_argument("-p", "--pretrained_model_name_or_path", type=str, default=train_config.get('pretrained_model_name_or_path'),
                        help="é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("-s", "--seed", type=int, default=train_config.get('seed', 1126),
                        help="éšæœºæ•°ç§å­")
    parser.add_argument("-w", "--weight_dtype", type=str, default=train_config.get('weight_dtype', 'torch.bfloat16'),
                        help="æƒé‡æ•°æ®ç±»å‹")
    parser.add_argument("-b", "--batch_size", type=int, default=train_config.get('batch_size', 2),
                        help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("-e", "--max_train_steps", type=int, default=train_config.get('max_train_steps', 200),
                        help="æ€»è®­ç»ƒæ­¥æ•°")
    parser.add_argument("-u", "--unet_learning_rate", type=float, default=train_config.get('unet_learning_rate', 1e-4),
                        help="UNet çš„å­¦ä¹ ç‡")
    parser.add_argument("-t", "--text_encoder_learning_rate", type=float, default=train_config.get('text_encoder_learning_rate', 1e-4),
                        help="æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡")
    parser.add_argument("-g", "--snr_gamma", type=float, default=train_config.get('snr_gamma', 5),
                        help="SNR å‚æ•°")
    parser.add_argument("-l", "--lr_scheduler_name", type=str, default=train_config.get('lr_scheduler_name', "cosine_with_restarts"),
                        help="å­¦ä¹ ç‡è°ƒåº¦å™¨åç§°")
    parser.add_argument("-warmup", "--lr_warmup_steps", type=int, default=train_config.get('lr_warmup_steps', 100),
                        help="å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°")
    parser.add_argument("-cycle", "--num_cycles", type=int, default=train_config.get('num_cycles', 3),
                        help="å­¦ä¹ ç‡è°ƒåº¦å™¨çš„å‘¨æœŸæ•°é‡")
    parser.add_argument("--resume", action="store_true", default=train_config.get('resume', False),
                        help="æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤")
    parser.add_argument("--no-train", action="store_true", default=False,
                        help="æ˜¯å¦è·³è¿‡è®­ç»ƒè¿‡ç¨‹ï¼Œç›´æ¥è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œé»˜è®¤è®­ç»ƒ")
    parser.add_argument("--no-generate", action="store_true", default=False,
                        help="è®­ç»ƒå®Œæˆåæ˜¯å¦è·³è¿‡å›¾åƒç”Ÿæˆï¼Œé»˜è®¤ç”Ÿæˆ")
    
    # ç”Ÿæˆç›¸å…³å‚æ•°ï¼Œå‰ç¼€ g ä»£æŒ‡ generateï¼Œå’Œä¹‹å‰çš„çŸ­å‚æ•°åšåŒºåˆ†
    parser.add_argument("-gp", "--prompts_path", type=str, default=generate_config.get('prompts_path'),
                        help="éªŒè¯æç¤ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-gs", "--save_folder", type=str, default=generate_config.get('save_folder'),
                        help="ä¿å­˜ç”Ÿæˆå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º root + dataset_name + '/inference'")
    parser.add_argument("-gn", "--num_inference_steps", type=int, default=generate_config.get('num_inference_steps', 50),
                        help="æ¨ç†æ­¥éª¤æ•°")
    parser.add_argument("-gg", "--guidance_scale", type=float, default=generate_config.get('guidance_scale', 7.5),
                        help="æŒ‡å¯¼å°ºåº¦")
    
    args = parser.parse_args()

    # å¦‚æœ captions_folder æœªæŒ‡å®šï¼Œåˆ™é»˜è®¤ä¸º dataset_pathï¼Œæ„æ€æ˜¯å›¾ç‰‡å’Œæ ‡æ³¨åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹          
    if args.captions_folder is None:
        args.captions_folder = args.dataset_path

    # æƒé‡æ•°æ®ç±»å‹æ£€æŸ¥
    if args.weight_dtype:
        try:
            args.weight_dtype = getattr(torch, args.weight_dtype.split('.')[-1])
        except AttributeError:
            print(f"âš ï¸ æ— æ•ˆçš„ weight_dtype '{args.weight_dtype}'ï¼Œä½¿ç”¨é»˜è®¤ 'torch.float32'")
            args.weight_dtype = torch.float32

    # è‡ªåŠ¨è®¾ç½® model_pathï¼Œå¦‚æœæœªæŒ‡å®š
    if args.model_path is None:
        dataset_name = os.path.basename(os.path.abspath(args.dataset_path))
        args.model_path = os.path.join(args.root, dataset_name, "logs", "checkpoint-last")

    # è‡ªåŠ¨è®¾ç½® save_folderï¼Œå¦‚æœæœªæŒ‡å®š
    if args.save_folder is None:
        dataset_name = os.path.basename(os.path.abspath(args.dataset_path))
        args.save_folder = os.path.join(args.root, dataset_name, "inference")

    return args

# ========== ä¸»å‡½æ•° ==========
def main():
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼Œè·å– train å’Œ generate éƒ¨åˆ†çš„é…ç½®
    train_config, generate_config = load_config(script_name='train'), load_config(script_name='generate')

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args(train_config, generate_config)

    # è®¾ç½®éšæœºæ•°ç§å­
    seed = args.seed
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    # è‡ªåŠ¨è®¾ç½®é¡¹ç›®åç§°ï¼ˆæ•°æ®é›†æ–‡ä»¶å¤¹åç§°ï¼‰
    dataset_name = os.path.basename(os.path.abspath(args.dataset_path))

    # è¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = os.path.dirname(args.model_path)
    os.makedirs(output_folder, exist_ok=True)

    if not args.no_train:
        # æ•°æ®å¢å¼ºæ“ä½œ
        train_transform = transforms.Compose(
            [
                transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°
                transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ
                transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
                transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
            ]
        )
    
        # LoRA é…ç½®
        lora_config = LoraConfig(
            r=32,
            lora_alpha=16,
            target_modules=[
                "q_proj", "v_proj", "k_proj", "out_proj",
                "to_k", "to_q", "to_v", "to_out.0"
            ],
            lora_dropout=0
        )
    
        # ========== å¾®è°ƒå‰çš„å‡†å¤‡ ==========
    
        # åˆå§‹åŒ– tokenizerï¼Œç”¨äºåŠ è½½æ•°æ®é›†
        tokenizer = CLIPTokenizer.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="tokenizer"
        )
    
        # å‡†å¤‡æ•°æ®é›†
        dataset = Text2ImageDataset(
            images_folder=args.dataset_path,
            captions_folder=args.captions_folder,
            transform=train_transform,
            tokenizer=tokenizer,
        )
    
        train_dataloader = torch.utils.data.DataLoader(
            dataset,
            shuffle=True,
            collate_fn=collate_fn,
            batch_size=args.batch_size,
            num_workers=4,
        )
    
        print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    
        # å‡†å¤‡æ¨¡å‹
        tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(
            lora_config,
            pretrained_model_name_or_path=args.pretrained_model_name_or_path,
            model_path=args.model_path,
            weight_dtype = args.weight_dtype,
            resume=args.resume
        )
    
        # å‡†å¤‡ä¼˜åŒ–å™¨
        optimizer = prepare_optimizer(
            unet, 
            text_encoder, 
            unet_learning_rate=args.unet_learning_rate, 
            text_encoder_learning_rate=args.text_encoder_learning_rate
        )
    
        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = get_scheduler(
            args.lr_scheduler_name,
            optimizer=optimizer,
            num_warmup_steps=args.lr_warmup_steps,
            num_training_steps=args.max_train_steps,
            num_cycles=args.num_cycles
        )
    
        print("âœ… æ¨¡å‹å’Œä¼˜åŒ–å™¨å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    
        # ========== å¼€å§‹å¾®è°ƒ ==========
    
        # ç¦ç”¨å¹¶è¡ŒåŒ–ï¼Œé¿å…è­¦å‘Š
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
        # åˆå§‹åŒ–
        global_step = 0
    
        # è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        progress_bar = tqdm(
            range(args.max_train_steps),
            desc="è®­ç»ƒæ­¥éª¤",
        )
    
        # è®­ç»ƒå¾ªç¯
        for epoch in range(math.ceil(args.max_train_steps / len(train_dataloader))):
            unet.train()
            text_encoder.train()
            
            for step, batch in enumerate(train_dataloader):
                if global_step >= args.max_train_steps:
                    break
                
                # ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰
                latents = vae.encode(batch["pixel_values"].to(DEVICE, dtype=args.weight_dtype)).latent_dist.sample()
                latents = latents * vae.config.scaling_factor  # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨ç©ºé—´
    
                # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ
                noise = torch.randn_like(latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
    
                # è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º
                encoder_hidden_states = text_encoder(batch["input_ids"].to(DEVICE))[0]
    
                # è®¡ç®—ç›®æ ‡å€¼
                if noise_scheduler.config.prediction_type == "epsilon":
                    target = noise  # é¢„æµ‹å™ªå£°
                elif noise_scheduler.config.prediction_type == "v_prediction":
                    target = noise_scheduler.get_velocity(latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡
    
                # UNet æ¨¡å‹é¢„æµ‹
                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states)[0]
    
                # è®¡ç®—æŸå¤±
                if not args.snr_gamma:
                    loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
                else:
                    # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±
                    snr = compute_snr(noise_scheduler, timesteps)
                    mse_loss_weights = torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]
                    if noise_scheduler.config.prediction_type == "epsilon":
                        mse_loss_weights = mse_loss_weights / snr
                    elif noise_scheduler.config.prediction_type == "v_prediction":
                        mse_loss_weights = mse_loss_weights / (snr + 1)
                    
                    # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±
                    loss = F.mse_loss(model_pred.float(), target.float(), reduction="none")
                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                    loss = loss.mean()
    
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
                progress_bar.update(1)
                global_step += 1
    
                # æ‰“å°è®­ç»ƒæŸå¤±
                if global_step % 50 == 0 or global_step == args.max_train_steps:
                    print(f"ğŸ”¥ æ­¥éª¤ {global_step}, æŸå¤±: {loss.item():.4f}")
    
                # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œæ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡
                if global_step % 100 == 0:
                    save_path = os.path.join(output_folder, f"checkpoint-{global_step}")
                    os.makedirs(save_path, exist_ok=True)
    
                    # ä½¿ç”¨ save_pretrained ä¿å­˜ PeftModel
                    unet.save_pretrained(os.path.join(save_path, "unet"))
                    text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
                    print(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´æ¨¡å‹åˆ° {save_path}")
    
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° checkpoint-last
        save_path = args.model_path
        os.makedirs(save_path, exist_ok=True)
        unet.save_pretrained(os.path.join(save_path, "unet"))
        text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
        print(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {save_path}")
    
        print("ğŸ‰ å¾®è°ƒå®Œæˆï¼")
    else:
        print("ğŸš« å·²è·³è¿‡è®­ç»ƒè¿‡ç¨‹ã€‚")

    # ========== è®­ç»ƒå®Œæˆåç”Ÿæˆå›¾åƒ ==========
    if not args.no_generate:
        print("ğŸ–¼ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
        generate_after_training(args)
    else:
        print("ğŸš« å·²è·³è¿‡å›¾åƒç”Ÿæˆã€‚")

def generate_after_training(args):
    """
    è®­ç»ƒå®Œæˆåç”Ÿæˆå›¾åƒçš„å‡½æ•°
    """
    # è®¾ç½®è®¾å¤‡
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # è®¾ç½®å‚æ•°
    prompts_path = args.prompts_path
    save_folder = args.save_folder
    num_inference_steps = args.num_inference_steps
    guidance_scale = args.guidance_scale
    weight_dtype = args.weight_dtype
    seed = args.seed
    root = args.root

    # è®¾ç½®éšæœºæ•°ç§å­
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    # å‡†å¤‡ LoRA æ¨¡å‹
    print("âœ… å‡†å¤‡ LoRA æ¨¡å‹ç”¨äºç”Ÿæˆå›¾åƒ...")
    tokenizer = CLIPTokenizer.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="tokenizer"
    )

    text_encoder = CLIPTextModel.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="text_encoder",
        torch_dtype=weight_dtype
    )

    unet = UNet2DConditionModel.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="unet",
        torch_dtype=weight_dtype
    )

    # åŠ è½½å¾®è°ƒåçš„æƒé‡
    model_path = args.model_path
    text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, "text_encoder"))
    unet = PeftModel.from_pretrained(unet, os.path.join(model_path, "unet"))

    # åˆå¹¶ LoRA æƒé‡
    text_encoder = text_encoder.merge_and_unload()
    unet = unet.merge_and_unload()

    # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
    text_encoder.eval()
    unet.eval()

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
    text_encoder.to(DEVICE, dtype=weight_dtype)
    unet.to(DEVICE, dtype=weight_dtype)

    # åˆ›å»ºç®¡é“
    pipeline = DiffusionPipeline.from_pretrained(
        args.pretrained_model_name_or_path,
        unet=unet,
        text_encoder=text_encoder,
        torch_dtype=weight_dtype,
        safety_checker=None,
    )
    pipeline = pipeline.to(DEVICE)

    # è®¾ç½®éšæœºæ•°ç”Ÿæˆå™¨
    generator = torch.Generator(device=DEVICE)
    generator.manual_seed(seed)

    # åŠ è½½æç¤º
    validation_prompts = load_validation_prompts(prompts_path)

    # ç”Ÿæˆå›¾åƒ
    generate_images(
        pipeline=pipeline,
        prompts=validation_prompts,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        save_folder=save_folder,
        generator=generator
    )

if __name__ == "__main__":
    main()