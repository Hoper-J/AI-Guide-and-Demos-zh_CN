# PEFT：在大模型中快速应用 LoRA

>如果对 LoRA 还没有一个直观的概念，可以回看这篇文章：[《03. 认识 LoRA：从线性层到注意力机制》](../Guide/03.%20进阶指南：自定义%20Prompt%20提升大模型解题能力.md)。
>
>我们将在这里进一步探讨**如何快速地在大型预训练模型中应用 LoRA**，并解答可能存在的问题，包括：
>
>- `peft` 和 `lora` 之间有什么关系？
>- `get_peft_model` 怎么使用？
>- 如何知道应用 LoRA 后模型的参数变化量？
>- 如何使用 `merge_and_unload()` 合并 LoRA 权重？
>- 认识报错：`TypeError: Expected state_dict to be dict-like...`
>- 认知一个非常刁钻的 Bug：应用 LoRA 前使用 `get_peft_model()`。
>
>[代码文件下载](../Demos/12.%20应用%20LoRA%20到大模型的简单示例（PEFT）.ipynb)
>
>在线链接：[Kaggle](https://www.kaggle.com/code/aidemos/12-lora-peft) | [Colab](https://colab.research.google.com/drive/1-gWfn9xslSq6WlYDS9cinnyDEhBhjte4?usp=sharing)

## 目录

- [PEFT 和 LoRA 的关系](#peft-和-lora-的关系)
- [在大模型中应用 LoRA](#在大模型中应用-lora)
  - [安装必要的库](#安装必要的库)
  - [加载预训练模型](#加载预训练模型)
  - [应用 LoRA](#应用-lora)
  - [查看当前模型架构](#查看当前模型架构)
  - [查看增加的参数量](#查看增加的参数量)
    - [理论计算](#理论计算)
    - [使用 PEFT 查看参数](#使用-peft-查看参数)
    - [自定义函数查看参数](#自定义函数查看参数)
  - [准备数据](#准备数据)
  - [开始微调](#开始微调)
  - [保存和加载 LoRA 微调的模型](#保存和加载-lora-微调的模型)
    - [合并 LoRA 权重并卸载 PEFT 包装](#合并-lora-权重并卸载-peft-包装)
- [可能的错误及解决方案（TypeError: Expected state_dict to be dict-like...）](#可能的错误及解决方案typeerror-expected-state_dict-to-be-dict-like)
  - [错误原因](#错误原因)
  - [错误重现](#错误重现)
  - [解决方法](#解决方法)
- [一个导致微调看似无效的 Bug：应用 LoRA 前使用 get_peft_model()](#一个导致微调看似无效的-bug应用-lora-前使用-get_peft_model)
- [参考链接](#参考链接)

## PEFT 和 LoRA 的关系

PEFT（Parameter-Efficient Fine-Tuning）是 Hugging Face 提供的专门用于参数高效微调的工具库。LoRA（Low-Rank Adaptation）是 PEFT 支持的多种微调方法之一，旨在通过减少可训练参数来提高微调大模型的效率。除此之外，PEFT 还支持其他几种常见的微调方法，包括：

- **Prefix-Tuning**：冻结原模型参数，为每一层添加可学习的前缀向量，只学习前缀参数。
- **Adapter-Tuning**：冻结原模型参数，在模型的层与层之间插入小型的 adapter 模块，仅对 adapter 模块进行训练。
- ...

## 在大模型中应用 LoRA

下面，我们以实际的例子来展示如何在大模型中快速应用 LoRA。

### 安装必要的库

首先，确保你已经安装了 `transformers` 和 `peft` 库。

```bash
# 项目依赖已在 pyproject.toml 中配置，运行 uv sync 即可安装
# 文章中重复的 uv add 是旧版本 pip install 的遗留（默认仅配置了 PyTorch 等基础深度学习环境）
uv add transformers
uv add peft
```

### 加载预训练模型

我们以 Hugging Face 的 `transformers` 库为例，加载一个预训练的 GPT-2 模型，其参数大小为 110M。

```python
import os
# 设置模型下载镜像
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from transformers import AutoTokenizer, AutoModelForCausalLM

# 加载预训练的 GPT-2 模型和分词器
tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

# 使用eos_token作为pad_token
tokenizer.pad_token = tokenizer.eos_token

model
```

打印 model，方便和应用 LoRA 后进行对比。

![image-20240921111524351](./assets/image-20240921111524351.png)

### 应用 LoRA

使用 `peft` 库，我们可以轻松地将 LoRA 集成到模型中：

```python
from peft import get_peft_model, LoraConfig, TaskType

# 配置 LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # 任务类型：因果语言模型
    inference_mode=False,          # 推理模式关闭，以进行训练
    r=8,                           # 低秩值 r
    lora_alpha=32,                 # LoRA 的缩放因子
    lora_dropout=0.1,              # Dropout 概率
)

# 将 LoRA 应用到模型中
model = get_peft_model(model, lora_config)
```

### 查看当前模型架构

```python
print(model)
```

可以看到 LoRA 已经成功应用。

![image-20240921111458757](./assets/image-20240921111458757.png)

### 查看增加的参数量

应用 LoRA 后，或许你希望了解模型参数量的变化。以下是理论计算和查看方式：

#### 理论计算

对于每个应用了 LoRA 的层，增加的参数量为：

$$
\text{增加的参数量} = r \times (\text{输入维度} + \text{输出维度})
$$

- **`r`**：LoRA 的低秩值。
- **输入维度**：层的输入特征数。
- **输出维度**：层的输出特征数。

#### 使用 PEFT 查看参数

`peft` 提供了查看模型参数的便捷方法：

```python
# 查看 LoRA 模块
model.print_trainable_parameters()
```

输出：

```python
trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364
```

#### 自定义函数查看参数

实际上直接计算所有可训练参数就行。

```python
def print_trainable_parameters(model):
    """
    打印模型的可训练参数信息。
    
    参数:
        model: 要分析的模型
    """
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        num_params = param.numel()
        all_params += num_params
        if param.requires_grad:
            trainable_params += num_params
    print(f"可训练参数量: {trainable_params}")
    print(f"总参数量: {all_params}")
    print(f"可训练参数占比: {100 * trainable_params / all_params:.2f}%")
    
print_trainable_parameters(model)
```

输出：

```python
可训练参数量: 294912
总参数量: 124734720
可训练参数占比: 0.24%
```

### 准备数据

下面使用公开数据集进行演示。

```python
from datasets import load_dataset
from transformers import DataCollatorForLanguageModeling

# 1. 使用英文小说数据集
# dataset = load_dataset("roneneldan/TinyStories", split="train[:1000]")  # 取前1000条

# 2. 使用 imdb 电影评论数据集
dataset = load_dataset("imdb", split="train[:500]")

print(f"数据集大小: {len(dataset)}")
print(f"数据集列名: {dataset.column_names}")
print(f"示例数据: {dataset[0]}")

def preprocess_function(examples):
    """
    数据预处理函数。
    
    参数:
        examples: 包含数据集样例的字典
        
    返回:
        处理后的模型输入
    """
    # 对于IMDB数据集，我们使用 'text' 字段
    # 你可以根据不同数据集调整字段名
    texts = examples['text']
    
    # Tokenization
    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=512,  # 根据需要调整最大长度
        return_tensors="pt" if len(texts) == 1 else None
    )
    
    # 对于因果语言模型，labels就是input_ids
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    
    return model_inputs

# 应用预处理
print("正在预处理数据...")
train_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset.column_names,  # 移除原始列，只保留模型需要的
    desc="Tokenizing dataset"
)

print(f"预处理后的数据集大小: {len(train_dataset)}")
print(f"预处理后的数据集特征: {train_dataset.features}")

# 创建数据整理器
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # 不使用masked language modeling，因为我们做的是causal LM
    pad_to_multiple_of=8  # 为了提高效率，填充到8的倍数
)
```

**输出**：

```
数据集大小: 500
数据集列名: ['text', 'label']
示例数据: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered "controversial" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\'t have much of a plot.', 'label': 0}
正在预处理数据...
预处理后的数据集大小: 500
预处理后的数据集特征: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}
```

### 开始微调

```python
from transformers import Trainer, TrainingArguments

# 定义训练参数
training_args = TrainingArguments(
    output_dir='./results',         # 模型保存和日志输出的目录路径
    num_train_epochs=3,             # 训练的总轮数（epochs）
    per_device_train_batch_size=4,  # 每个设备（如GPU或CPU）上的训练批次大小，4表示每次输入模型的数据数量
    learning_rate=5e-5,             # 学习率
    logging_steps=10,               # 每隔多少步（steps）进行一次日志记录
    save_steps=100,                 # 每隔多少步保存模型
)

# 创建 Trainer
trainer = Trainer(
    model=model,                    # 训练的模型对象，需要事先加载好
    args=training_args,             # 上面定义的训练参数配置
    train_dataset=train_dataset,    # 使用预处理后的数据集
    data_collator=data_collator     # 数据整理器
)

# 开始训练
trainer.train()
```

### 保存和加载 LoRA 微调的模型

训练完成后，你可以保存或者加载 LoRA 微调的参数，下面是个简单的示例。

```python
# 保存 LoRA 参数
model.save_pretrained('./lora_model')
```

在推理时，加载原始的预训练模型和 LoRA 参数。

```python
# 加载原始模型
base_model = AutoModelForCausalLM.from_pretrained("gpt2")

# 加载 LoRA 参数
from peft import PeftModel

model = PeftModel.from_pretrained(base_model, './lora_model')
```

#### 合并 LoRA 权重并卸载 PEFT 包装

在完成微调后，可以使用 `merge_and_unload()` 将 LoRA 的权重合并回原始模型。这在部署和推理阶段非常有用，因为这样可以：

- **减少依赖**：合并后，模型成为标准的 `transformers` 模型，不再需要 `peft` 库。
- **提高推理效率**：减少了额外的计算开销，推理速度可能会有所提升。
- **简化模型保存和加载**：不需要分别保存基础模型和 LoRA 参数。

运行下面的代码：

```python
# 对比合并前后的模型
print("合并前的模型结构：")
print(model)

# 合并并卸载 LoRA 权重
model = model.merge_and_unload()

print("合并后的模型结构：")
print(model)
```

输出：

```
合并前的模型结构：
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): lora.Linear(
                (base_layer): Conv1D()
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=2304, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): Conv1D()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D()
              (c_proj): Conv1D()
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )
  )
)
合并后的模型结构：
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
```

你应该注意到，合并后模型的 LoRA 层将被去除。

现在，你可以像保存普通模型一样保存：

```python
# 保存合并后的模型
model.save_pretrained('./merged_model')
tokenizer.save_pretrained('./merged_model')
```

在推理阶段，直接加载这个合并后的模型：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载合并后的模型
tokenizer = AutoTokenizer.from_pretrained('./merged_model')
model = AutoModelForCausalLM.from_pretrained('./merged_model')

# 进行推理
inputs = tokenizer("Hello, World！", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

**注意**：

- **不可逆操作**：合并操作是不可逆的。如果你之后还需要进一步微调 LoRA 参数，需确保在合并前备份模型。
- **无需 PEFT 库**：合并后的模型不再包含 LoRA 适配器（Adapter）的信息，因此在加载时无需使用 `PeftModel`。

## 可能的错误及解决方案（TypeError: Expected state_dict to be dict-like...）

在使用 `PEFT` 和 `LoRA` 进行模型微调和保存加载时，可能会遇到如下错误：

```
TypeError: Expected state_dict to be dict-like, got <class 'peft.peft_model.PeftModel'>.
```

### 错误原因

一般是因为混合使用不同的保存和加载方式，这个错误不局限于 `PeftModel`，问题出在你用 `torch.save(model)` 保存整个模型却用 `load_state_dict()` 去加载，注意模型加载和保存的一致性。

### 错误重现

下面我们来复现它，看是不是和你的操作一致（这里以 `PeftModel` 举例）：

1. 错误地保存整个 `PeftModel` 对象而不是其 `state_dict`：

   ```python
   import torch
   from transformers import AutoTokenizer, AutoModelForCausalLM
   from peft import get_peft_model, LoraConfig, TaskType
   
   # 加载预训练模型和分词器
   tokenizer = AutoTokenizer.from_pretrained('gpt2')
   model = AutoModelForCausalLM.from_pretrained('gpt2')
   
   # 配置 LoRA
   lora_config = LoraConfig(
       task_type=TaskType.CAUSAL_LM,
       inference_mode=False,
       r=8,
       lora_alpha=32,
       lora_dropout=0.1,
   )
   
   # 应用 LoRA
   model = get_peft_model(model, lora_config)
   
   # 错误地保存整个 PeftModel 对象
   torch.save(model, './model')

2. 加载时传入`PeftModel` 对象：

   ```python
   # 初始化模型
   model = AutoModelForCausalLM.from_pretrained("gpt2")
   
   # 错误地加载模型，期望接收 state_dict 但实际加载了整个模型对象
   model.load_state_dict(torch.load('./model'))  # 这里会报错
   ```

   ![image-20240929160334588](./assets/image-20240929160334588.png)

### 解决方法

确保你保存和加载的对象是一致的：

- `torch.save(model, '...')` 对应于 `torch.load(model, '...')`。
- `torch.save(model.state_dict(), '...')` 对应于 `model.load_state_dict(torch.load('...'))`

## 一个导致微调看似无效的 Bug：应用 LoRA 前使用 `get_peft_model()`

我花了三个小时排除了所有可能的问题才找到它，起因：将代码从 load stata_dict 转为 PEFT 以供学习。

```python
# 原始项目代码（正确）：
# 将 LoRA 配置应用到 text_encoder 和 unet
text_encoder = get_peft_model(text_encoder, lora_config)
unet = get_peft_model(unet, lora_config)

# 如果设置为继续训练，则加载上一次的模型权重，当然，你可以修改 model_path 来指定其他的路径
if resume:
    # 加载上次训练的模型权重，注意这里只加载权重，而不是覆盖整个模型，覆盖：model = torch.load(...)
    text_encoder = torch.load(os.path.join(model_path, "text_encoder.pt"))
    unet = torch.load(os.path.join(model_path, "unet.pt"))
```

转换为 PEFT 形式：

```python
# 错误的示范
# 将 LoRA 配置应用到 text_encoder 和 unet
text_encoder = get_peft_model(text_encoder, lora_config)
unet = get_peft_model(unet, lora_config)

# 如果设置为继续训练，则加载上一次的模型权重
if resume:
    # 使用 PEFT 的 from_pretrained 方法加载 LoRA 模型
    text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, "text_encoder"))
    unet = PeftModel.from_pretrained(unet, os.path.join(model_path, "unet"))
```

很好，现在我们获得了一个不会报错，但是效果和没加 LoRA **完全相同**的模型，真是太棒了（它真的太刁钻了😡）。

来看看它究竟有什么区别，为了清晰，定义一个简单的线性层进行演示：

```python
import torch
import torch.nn as nn
from torch.optim import Adam
from copy import deepcopy
from peft import get_peft_model, LoraConfig, PeftModel

# 固定随机数种子，确保结果可复现
torch.manual_seed(42)

# 定义一个简单的线性模型
class LinearModel(nn.Module):
    """
    简单的线性模型。
    
    属性:
        linear: 线性层
    """
    
    def __init__(self, input_size, output_size):
        """
        初始化线性模型。
        
        参数:
            input_size: 输入特征的维度
            output_size: 输出特征的维度
        """
        super(LinearModel, self).__init__()
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x):
        """
        前向传播。
        
        参数:
            x: 输入张量
            
        返回:
            线性层的输出
        """
        return self.linear(x)

# 实例化线性模型
model = LinearModel(input_size=10, output_size=1)

# 在应用 LoRA 之前深拷贝原始模型，确保后续公平比较
original_model = deepcopy(model)

# 配置 LoRA 参数
config = LoraConfig(
    inference_mode=False,
    r=4,
    lora_alpha=16,
    target_modules=['linear'],
)

# 将 LoRA 应用到模型中
lora_model = get_peft_model(model, config)

# 定义一个简单的损失函数和优化器
criterion = nn.MSELoss()
optimizer = Adam(lora_model.parameters(), lr=1e-3)

# 生成一些模拟的训练数据
input_data = torch.randn(100, 10)  # 100 个样本，每个样本有 10 个特征
target_data = torch.randn(100, 1)  # 对应的目标值

# 训练一个回合
lora_model.train()
for epoch in range(1):  # 训练 1 个回合
    optimizer.zero_grad()
    outputs = lora_model(input_data)
    loss = criterion(outputs, target_data)
    loss.backward()
    optimizer.step()

# 训练后保存 LoRA 权重
lora_model.save_pretrained('linear_lora_model')

# 方法 1：先使用 get_peft_model，再加载 LoRA 权重
model1 = PeftModel.from_pretrained(get_peft_model(deepcopy(original_model), config), 'linear_lora_model')

# 方法 2：直接加载 LoRA 权重
model2 = PeftModel.from_pretrained(deepcopy(original_model), 'linear_lora_model')

# 生成相同的输入数据以进行输出比较
test_input = torch.randn(1, 10)

# 比较四个模型的输出（原始模型，LoRA，方法1，方法2）
def compare_model_outputs(input_data):
    """
    比较四个模型的输出。
    
    参数:
        input_data: 输入数据张量
    """
    # 原始模型
    original_output = original_model(input_data)
    print("原始模型输出:", original_output.detach().numpy())

    # 训练后的 LoRA 模型
    lora_output = lora_model(input_data)
    print("训练后的 LoRA 模型输出:", lora_output.detach().numpy())

    # 方法 1：先使用 get_peft_model，再加载 LoRA
    output1 = model1(input_data)
    print("方法 1（先使用 get_peft_model，再加载 LoRA）输出:", output1.detach().numpy())

    # 方法 2：直接加载 LoRA
    output2 = model2(input_data)
    print("方法 2（直接加载 LoRA）输出:", output2.detach().numpy())

    if torch.allclose(original_output, output1):
        print("\n原始模型和方法 1 输出相同。")
    if torch.allclose(lora_output, output2):
        print("训练后的 LoRA 模型和方法 2 输出相同。\n")

# 比较两个模型的参数
def compare_params(m1, m2):
    """
    比较两个模型的参数是否一致。
    
    参数:
        m1: 第一个模型
        m2: 第二个模型
        
    返回:
        如果参数一致返回True，否则返回False
    """
    for (n1, p1), (n2, p2) in zip(m1.named_parameters(), m2.named_parameters()):
        if n1 != n2 or not torch.allclose(p1, p2):
            print(f"参数不匹配: \n{n1}\n{n2}")
            return False
    return True

# 比较四个模型的输出
compare_model_outputs(test_input)

# 检查方法 1 和方法 2 的参数是否一致
if compare_params(model1, model2):
    print("方法 1 和方法 2 的 LoRA 模型参数一致！")
else:
    print("方法 1 和方法 2 的 LoRA 模型参数不一致！")
```

输出：

```python
原始模型输出: [[-0.03600371]]
训练后的 LoRA 模型输出: [[-0.03428639]]
方法 1（先使用 get_peft_model，再加载 LoRA）输出: [[-0.03600371]]
方法 2（直接加载 LoRA）输出: [[-0.03428639]]

原始模型和方法 1 输出相同。
训练后的 LoRA 模型和方法 2 输出相同。

参数不匹配: 
base_model.model.base_model.model.linear.base_layer.weight
base_model.model.linear.base_layer.weight
方法 1 和方法 2 的 LoRA 模型参数不一致！
```

从输出中可以看到，**方法 1**（在加载 LoRA 之前使用 `get_peft_model()`）与原始模型的输出**完全相同**，这意味着 LoRA 没有被有效应用。而**方法 2**（直接使用 `PeftModel.from_pretrained()` 加载 LoRA 权重）的输出与训练后的 LoRA 模型输出**一致**，说明被正确加载。

另外，你还可以看到方法 1 的模型架构会多一个 base_model.model 包裹，如果感兴趣的话，可以使用 `print(model1)` 进一步查看。这证明了在加载 LoRA 之前使用 `get_peft_model()` 会干扰模型结构，导致 LoRA 应用失效。我已经向官方提出了 [issue#2115](https://github.com/huggingface/peft/issues/2115)，并得到了很积极的回复，这些开发人员很棒，当前 Bug 会出现在版本 <=0.12.0。

**更新**：在 2024 年 10 月，这个问题得到了修复（[PR#2118](https://github.com/huggingface/peft/pull/2118)）。


## 参考链接

[PEFT - Hugging Face](https://huggingface.co/docs/peft/index)
