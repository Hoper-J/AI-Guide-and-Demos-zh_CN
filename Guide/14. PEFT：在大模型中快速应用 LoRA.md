# PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRA

>å¦‚æœå¯¹ LoRA è¿˜æ²¡æœ‰ä¸€ä¸ªç›´è§‚çš„æ¦‚å¿µï¼Œå¯ä»¥å›çœ‹è¿™ç¯‡æ–‡ç« ï¼š[ã€Š03. è®¤è¯† LoRAï¼šä»çº¿æ€§å±‚åˆ°æ³¨æ„åŠ›æœºåˆ¶ã€‹](../Guide/03.%20è¿›é˜¶æŒ‡å—ï¼šè‡ªå®šä¹‰%20Prompt%20æå‡å¤§æ¨¡å‹è§£é¢˜èƒ½åŠ›.md)ã€‚
>
>æˆ‘ä»¬å°†åœ¨è¿™é‡Œè¿›ä¸€æ­¥æ¢è®¨**å¦‚ä½•å¿«é€Ÿåœ°åœ¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ä¸­åº”ç”¨ LoRA**ï¼Œå¹¶è§£ç­”å¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼š
>
>- `peft` å’Œ `lora` ä¹‹é—´æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
>- `get_peft_model` æ€ä¹ˆä½¿ç”¨ï¼Ÿ
>- å¦‚ä½•çŸ¥é“åº”ç”¨ LoRA åæ¨¡å‹çš„å‚æ•°å˜åŒ–é‡ï¼Ÿ
>- å¦‚ä½•ä½¿ç”¨ `merge_and_unload()` åˆå¹¶ LoRA æƒé‡ï¼Ÿ
>- è®¤è¯†æŠ¥é”™ï¼š`TypeError: Expected state_dict to be dict-like...`
>- è®¤çŸ¥ä¸€ä¸ªéå¸¸åˆé’»çš„ Bugï¼šåº”ç”¨ LoRA å‰ä½¿ç”¨ `get_peft_model()`ã€‚
>
>[ä»£ç æ–‡ä»¶ä¸‹è½½](../Demos/12.%20åº”ç”¨%20LoRA%20åˆ°å¤§æ¨¡å‹çš„ç®€å•ç¤ºä¾‹ï¼ˆPEFTï¼‰.ipynb)
>
>åœ¨çº¿é“¾æ¥ï¼š[Kaggle](https://www.kaggle.com/code/aidemos/12-lora-peft) | [Colab](https://colab.research.google.com/drive/1-gWfn9xslSq6WlYDS9cinnyDEhBhjte4?usp=sharing)

## ç›®å½•

- [PEFT å’Œ LoRA çš„å…³ç³»](#peft-å’Œ-lora-çš„å…³ç³»)
- [åœ¨å¤§æ¨¡å‹ä¸­åº”ç”¨ LoRA](#åœ¨å¤§æ¨¡å‹ä¸­åº”ç”¨-lora)
  - [å®‰è£…å¿…è¦çš„åº“](#å®‰è£…å¿…è¦çš„åº“)
  - [åŠ è½½é¢„è®­ç»ƒæ¨¡å‹](#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹)
  - [åº”ç”¨ LoRA](#åº”ç”¨-lora)
  - [æŸ¥çœ‹å½“å‰æ¨¡å‹æ¶æ„](#æŸ¥çœ‹å½“å‰æ¨¡å‹æ¶æ„)
  - [æŸ¥çœ‹å¢åŠ çš„å‚æ•°é‡](#æŸ¥çœ‹å¢åŠ çš„å‚æ•°é‡)
     - [ç†è®ºè®¡ç®—](#ç†è®ºè®¡ç®—)
     - [ä½¿ç”¨ PEFT æŸ¥çœ‹å‚æ•°](#ä½¿ç”¨-peft-æŸ¥çœ‹å‚æ•°)
     - [è‡ªå®šä¹‰å‡½æ•°æŸ¥çœ‹å‚æ•°](#è‡ªå®šä¹‰å‡½æ•°æŸ¥çœ‹å‚æ•°)
  - [å‡†å¤‡æ•°æ®å¹¶è¿›è¡Œå¾®è°ƒ](#å‡†å¤‡æ•°æ®å¹¶è¿›è¡Œå¾®è°ƒ)
  - [ä¿å­˜å’ŒåŠ è½½ LoRA å¾®è°ƒçš„æ¨¡å‹](#ä¿å­˜å’ŒåŠ è½½-lora-å¾®è°ƒçš„æ¨¡å‹)
     - [åˆå¹¶ LoRA æƒé‡å¹¶å¸è½½ PEFT åŒ…è£…](#åˆå¹¶-lora-æƒé‡å¹¶å¸è½½-peft-åŒ…è£…)
- [å¯èƒ½çš„é”™è¯¯åŠè§£å†³æ–¹æ¡ˆï¼ˆTypeError: Expected state_dict to be dict-like...ï¼‰](#å¯èƒ½çš„é”™è¯¯åŠè§£å†³æ–¹æ¡ˆtypeerror-expected-state_dict-to-be-dict-like)
  - [é”™è¯¯åŸå› ](#é”™è¯¯åŸå› )
  - [é”™è¯¯é‡ç°](#é”™è¯¯é‡ç°)
  - [è§£å†³æ–¹æ³•](#è§£å†³æ–¹æ³•)
- [ä¸€ä¸ªå¯¼è‡´å¾®è°ƒçœ‹ä¼¼æ— æ•ˆçš„ Bugï¼šåº”ç”¨ LoRA å‰ä½¿ç”¨ get_peft_model()](#ä¸€ä¸ªå¯¼è‡´å¾®è°ƒçœ‹ä¼¼æ— æ•ˆçš„-bugåº”ç”¨-lora-å‰ä½¿ç”¨-get_peft_model)
- [å‚è€ƒé“¾æ¥](#å‚è€ƒé“¾æ¥)

## PEFT å’Œ LoRA çš„å…³ç³»

PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ Hugging Face æä¾›çš„ä¸“é—¨ç”¨äºå‚æ•°é«˜æ•ˆå¾®è°ƒçš„å·¥å…·åº“ã€‚LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ PEFT æ”¯æŒçš„å¤šç§å¾®è°ƒæ–¹æ³•ä¹‹ä¸€ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘å¯è®­ç»ƒå‚æ•°æ¥æé«˜å¾®è°ƒå¤§æ¨¡å‹çš„æ•ˆç‡ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒPEFT è¿˜æ”¯æŒå…¶ä»–å‡ ç§å¸¸è§çš„å¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š

- **Prefix-Tuning**ï¼šå†»ç»“åŸæ¨¡å‹å‚æ•°ï¼Œä¸ºæ¯ä¸€å±‚æ·»åŠ å¯å­¦ä¹ çš„å‰ç¼€å‘é‡ï¼Œåªå­¦ä¹ å‰ç¼€å‚æ•°ã€‚
- **Adapter-Tuning**ï¼šå†»ç»“åŸæ¨¡å‹å‚æ•°ï¼Œåœ¨æ¨¡å‹çš„å±‚ä¸å±‚ä¹‹é—´æ’å…¥å°å‹çš„ adapter æ¨¡å—ï¼Œä»…å¯¹ adapter æ¨¡å—è¿›è¡Œè®­ç»ƒã€‚
- ...

## åœ¨å¤§æ¨¡å‹ä¸­åº”ç”¨ LoRA

ä¸‹é¢ï¼Œæˆ‘ä»¬ä»¥å®é™…çš„ä¾‹å­æ¥å±•ç¤ºå¦‚ä½•åœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRAã€‚

### å®‰è£…å¿…è¦çš„åº“

é¦–å…ˆï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº† `transformers` å’Œ `peft` åº“ã€‚

```bash
pip install transformers peft
```

### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

æˆ‘ä»¬ä»¥ Hugging Face çš„ `transformers` åº“ä¸ºä¾‹ï¼ŒåŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ GPT-2 æ¨¡å‹ï¼Œå…¶å‚æ•°å¤§å°ä¸º 110Mã€‚

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

print(model)
```

æ‰“å° modelï¼Œæ–¹ä¾¿å’Œåº”ç”¨ LoRA åè¿›è¡Œå¯¹æ¯”ã€‚

![image-20240921111524351](./assets/image-20240921111524351.png)

### åº”ç”¨ LoRA

ä½¿ç”¨ `peft` åº“ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°† LoRA é›†æˆåˆ°æ¨¡å‹ä¸­ï¼š

```python
from peft import get_peft_model, LoraConfig, TaskType

# é…ç½® LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹
    inference_mode=False,          # æ¨ç†æ¨¡å¼å…³é—­ï¼Œä»¥è¿›è¡Œè®­ç»ƒ
    r=8,                           # ä½ç§©å€¼ r
    lora_alpha=32,                 # LoRA çš„ç¼©æ”¾å› å­
    lora_dropout=0.1,              # Dropout æ¦‚ç‡
)

# å°† LoRA åº”ç”¨åˆ°æ¨¡å‹ä¸­
model = get_peft_model(model, lora_config)
```

### æŸ¥çœ‹å½“å‰æ¨¡å‹æ¶æ„

```python
print(model)
```

å¯ä»¥çœ‹åˆ° LoRA å·²ç»æˆåŠŸåº”ç”¨ã€‚

![image-20240921111458757](./assets/image-20240921111458757.png)

### æŸ¥çœ‹å¢åŠ çš„å‚æ•°é‡

åº”ç”¨ LoRA åï¼Œæˆ–è®¸ä½ å¸Œæœ›äº†è§£æ¨¡å‹å‚æ•°é‡çš„å˜åŒ–ã€‚ä»¥ä¸‹æ˜¯ç†è®ºè®¡ç®—å’ŒæŸ¥çœ‹æ–¹å¼ï¼š

#### ç†è®ºè®¡ç®—

å¯¹äºæ¯ä¸ªåº”ç”¨äº† LoRA çš„å±‚ï¼Œå¢åŠ çš„å‚æ•°é‡ä¸ºï¼š

$$
\text{å¢åŠ çš„å‚æ•°é‡} = r \times (\text{è¾“å…¥ç»´åº¦} + \text{è¾“å‡ºç»´åº¦})
$$

- **`r`**ï¼šLoRA çš„ä½ç§©å€¼ã€‚
- **è¾“å…¥ç»´åº¦**ï¼šå±‚çš„è¾“å…¥ç‰¹å¾æ•°ã€‚
- **è¾“å‡ºç»´åº¦**ï¼šå±‚çš„è¾“å‡ºç‰¹å¾æ•°ã€‚

#### ä½¿ç”¨ PEFT æŸ¥çœ‹å‚æ•°

`peft` æä¾›äº†æŸ¥çœ‹æ¨¡å‹å‚æ•°çš„ä¾¿æ·æ–¹æ³•ï¼š

```python
# æŸ¥çœ‹ LoRA æ¨¡å—
model.print_trainable_parameters()
```

è¾“å‡ºï¼š

```python
trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364
```

#### è‡ªå®šä¹‰å‡½æ•°æŸ¥çœ‹å‚æ•°

å®é™…ä¸Šç›´æ¥è®¡ç®—æ‰€æœ‰å¯è®­ç»ƒå‚æ•°å°±è¡Œã€‚

```python
def print_trainable_parameters(model):
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        num_params = param.numel()
        all_params += num_params
        if param.requires_grad:
            trainable_params += num_params
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params}")
    print(f"æ€»å‚æ•°é‡: {all_params}")
    print(f"å¯è®­ç»ƒå‚æ•°å æ¯”: {100 * trainable_params / all_params:.2f}%")
    
print_trainable_parameters(model)
```

è¾“å‡ºï¼š

```python
å¯è®­ç»ƒå‚æ•°é‡: 294912
æ€»å‚æ•°é‡: 124734720
å¯è®­ç»ƒå‚æ•°å æ¯”: 0.24%
```

### å‡†å¤‡æ•°æ®å¹¶è¿›è¡Œå¾®è°ƒ

å‡è®¾ä½ å·²ç»æœ‰äº†è®­ç»ƒæ•°æ®é›† `train_dataset`ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„æ ·ä¾‹ä»£ç ã€‚

```python
from transformers import Trainer, TrainingArguments

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',         # æ¨¡å‹ä¿å­˜å’Œæ—¥å¿—è¾“å‡ºçš„ç›®å½•è·¯å¾„
    num_train_epochs=3,             # è®­ç»ƒçš„æ€»è½®æ•°ï¼ˆepochsï¼‰
    per_device_train_batch_size=16, # æ¯ä¸ªè®¾å¤‡ï¼ˆå¦‚GPUæˆ–CPUï¼‰ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œ16è¡¨ç¤ºæ¯æ¬¡è¾“å…¥æ¨¡å‹çš„æ•°æ®æ•°é‡
    learning_rate=5e-5,             # å­¦ä¹ ç‡
    logging_steps=10,               # æ¯éš”å¤šå°‘æ­¥ï¼ˆstepsï¼‰è¿›è¡Œä¸€æ¬¡æ—¥å¿—è®°å½•
    save_steps=100,                 # æ¯éš”å¤šå°‘æ­¥ä¿å­˜æ¨¡å‹
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,                    # è®­ç»ƒçš„æ¨¡å‹å¯¹è±¡ï¼Œéœ€è¦äº‹å…ˆåŠ è½½å¥½
    args=training_args,             # ä¸Šé¢å®šä¹‰çš„è®­ç»ƒå‚æ•°é…ç½®
    train_dataset=train_dataset,    # éœ€è¦å¯¹åº”æ›¿æ¢æˆå·²ç»å¤„ç†è¿‡çš„dataset
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### ä¿å­˜å’ŒåŠ è½½ LoRA å¾®è°ƒçš„æ¨¡å‹

è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥ä¿å­˜æˆ–è€…åŠ è½½ LoRA å¾®è°ƒçš„å‚æ•°ï¼Œä¸‹é¢æ˜¯ä¸ªç®€å•çš„ç¤ºä¾‹ã€‚

```python
# ä¿å­˜ LoRA å‚æ•°
model.save_pretrained('./lora_model')
```

åœ¨æ¨ç†æ—¶ï¼ŒåŠ è½½åŸå§‹çš„é¢„è®­ç»ƒæ¨¡å‹å’Œ LoRA å‚æ•°ã€‚

```python
# åŠ è½½åŸå§‹æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("gpt2")

# åŠ è½½ LoRA å‚æ•°
from peft import PeftModel

model = PeftModel.from_pretrained(base_model, './lora_model')
```

#### åˆå¹¶ LoRA æƒé‡å¹¶å¸è½½ PEFT åŒ…è£…

åœ¨å®Œæˆå¾®è°ƒåï¼Œå¯ä»¥ä½¿ç”¨ `merge_and_unload()` å°† LoRA çš„æƒé‡åˆå¹¶å›åŸå§‹æ¨¡å‹ã€‚è¿™åœ¨éƒ¨ç½²å’Œæ¨ç†é˜¶æ®µéå¸¸æœ‰ç”¨ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥ï¼š

- **å‡å°‘ä¾èµ–**ï¼šåˆå¹¶åï¼Œæ¨¡å‹æˆä¸ºæ ‡å‡†çš„ `transformers` æ¨¡å‹ï¼Œä¸å†éœ€è¦ `peft` åº“ã€‚
- **æé«˜æ¨ç†æ•ˆç‡**ï¼šå‡å°‘äº†é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œæ¨ç†é€Ÿåº¦å¯èƒ½ä¼šæœ‰æ‰€æå‡ã€‚
- **ç®€åŒ–æ¨¡å‹ä¿å­˜å’ŒåŠ è½½**ï¼šä¸éœ€è¦åˆ†åˆ«ä¿å­˜åŸºç¡€æ¨¡å‹å’Œ LoRA å‚æ•°ã€‚

è¿è¡Œä¸‹é¢çš„ä»£ç ï¼š

```python
# å¯¹æ¯”åˆå¹¶å‰åçš„æ¨¡å‹
print("åˆå¹¶å‰çš„æ¨¡å‹ç»“æ„ï¼š")
print(model)

# åˆå¹¶å¹¶å¸è½½ LoRA æƒé‡
model = model.merge_and_unload()

print("åˆå¹¶åçš„æ¨¡å‹ç»“æ„ï¼š")
print(model)
```

è¾“å‡ºï¼š

```
åˆå¹¶å‰çš„æ¨¡å‹ç»“æ„ï¼š
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): lora.Linear(
                (base_layer): Conv1D()
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=2304, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): Conv1D()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D()
              (c_proj): Conv1D()
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )
  )
)
åˆå¹¶åçš„æ¨¡å‹ç»“æ„ï¼š
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
```

ä½ åº”è¯¥æ³¨æ„åˆ°ï¼Œåˆå¹¶åæ¨¡å‹çš„ LoRA å±‚å°†è¢«å»é™¤ã€‚

ç°åœ¨ï¼Œä½ å¯ä»¥åƒä¿å­˜æ™®é€šæ¨¡å‹ä¸€æ ·ä¿å­˜ï¼š

```python
# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
model.save_pretrained('./merged_model')
tokenizer.save_pretrained('./merged_model')
```

åœ¨æ¨ç†é˜¶æ®µï¼Œç›´æ¥åŠ è½½è¿™ä¸ªåˆå¹¶åçš„æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½åˆå¹¶åçš„æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained('./merged_model')
model = AutoModelForCausalLM.from_pretrained('./merged_model')

# è¿›è¡Œæ¨ç†
inputs = tokenizer("Hello, Worldï¼", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

**æ³¨æ„**ï¼š

- **ä¸å¯é€†æ“ä½œ**ï¼šåˆå¹¶æ“ä½œæ˜¯ä¸å¯é€†çš„ã€‚å¦‚æœä½ ä¹‹åè¿˜éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒ LoRA å‚æ•°ï¼Œéœ€ç¡®ä¿åœ¨åˆå¹¶å‰å¤‡ä»½æ¨¡å‹ã€‚
- **æ— éœ€ PEFT åº“**ï¼šåˆå¹¶åçš„æ¨¡å‹ä¸å†åŒ…å« LoRA é€‚é…å™¨ï¼ˆAdapterï¼‰çš„ä¿¡æ¯ï¼Œå› æ­¤åœ¨åŠ è½½æ—¶æ— éœ€ä½¿ç”¨ `PeftModel`ã€‚

## å¯èƒ½çš„é”™è¯¯åŠè§£å†³æ–¹æ¡ˆï¼ˆTypeError: Expected state_dict to be dict-like...ï¼‰

åœ¨ä½¿ç”¨ `PEFT` å’Œ `LoRA` è¿›è¡Œæ¨¡å‹å¾®è°ƒå’Œä¿å­˜åŠ è½½æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š

```
TypeError: Expected state_dict to be dict-like, got <class 'peft.peft_model.PeftModel'>.
```

### é”™è¯¯åŸå› 

ä¸€èˆ¬æ˜¯å› ä¸ºæ··åˆä½¿ç”¨ä¸åŒçš„ä¿å­˜å’ŒåŠ è½½æ–¹å¼ï¼Œè¿™ä¸ªé”™è¯¯ä¸å±€é™äº `PeftModel`ï¼Œé—®é¢˜å‡ºåœ¨ä½ ç”¨ `torch.save(model)` ä¿å­˜æ•´ä¸ªæ¨¡å‹å´ç”¨ `load_state_dict()` å»åŠ è½½ï¼Œæ³¨æ„æ¨¡å‹åŠ è½½å’Œä¿å­˜çš„ä¸€è‡´æ€§ã€‚

### é”™è¯¯é‡ç°

ä¸‹é¢æˆ‘ä»¬æ¥å¤ç°å®ƒï¼Œçœ‹æ˜¯ä¸æ˜¯å’Œä½ çš„æ“ä½œä¸€è‡´ï¼ˆè¿™é‡Œä»¥ `PeftModel` ä¸¾ä¾‹ï¼‰ï¼š

1. é”™è¯¯åœ°ä¿å­˜æ•´ä¸ª `PeftModel` å¯¹è±¡è€Œä¸æ˜¯å…¶ `state_dict`ï¼š

   ```python
   import torch
   from transformers import AutoTokenizer, AutoModelForCausalLM
   from peft import get_peft_model, LoraConfig, TaskType
   
   # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
   tokenizer = AutoTokenizer.from_pretrained('gpt2')
   model = AutoModelForCausalLM.from_pretrained('gpt2')
   
   # é…ç½® LoRA
   lora_config = LoraConfig(
       task_type=TaskType.CAUSAL_LM,
       inference_mode=False,
       r=8,
       lora_alpha=32,
       lora_dropout=0.1,
   )
   
   # åº”ç”¨ LoRA
   model = get_peft_model(model, lora_config)
   
   # é”™è¯¯åœ°ä¿å­˜æ•´ä¸ª PeftModel å¯¹è±¡
   torch.save(model, './model')

2. åŠ è½½æ—¶ä¼ å…¥`PeftModel` å¯¹è±¡ï¼š

   ```python
   # åˆå§‹åŒ–æ¨¡å‹
   model = AutoModelForCausalLM.from_pretrained("gpt2")
   
   # é”™è¯¯åœ°åŠ è½½æ¨¡å‹ï¼ŒæœŸæœ›æ¥æ”¶ state_dict ä½†å®é™…åŠ è½½äº†æ•´ä¸ªæ¨¡å‹å¯¹è±¡
   model.load_state_dict(torch.load('./model'))  # è¿™é‡Œä¼šæŠ¥é”™
   ```

   ![image-20240929160334588](./assets/image-20240929160334588.png)

### è§£å†³æ–¹æ³•

ç¡®ä¿ä½ ä¿å­˜å’ŒåŠ è½½çš„å¯¹è±¡æ˜¯ä¸€è‡´çš„ï¼š

- `torch.save(model, '...')` å¯¹åº”äº `torch.load(model, '...')`ã€‚
- `torch.save(model.state_dict(), '...')` å¯¹åº”äº `model.load_state_dict(torch.load('...'))`

## ä¸€ä¸ªå¯¼è‡´å¾®è°ƒçœ‹ä¼¼æ— æ•ˆçš„ Bugï¼šåº”ç”¨ LoRA å‰ä½¿ç”¨ `get_peft_model()`

æˆ‘èŠ±äº†ä¸‰ä¸ªå°æ—¶æ’é™¤äº†æ‰€æœ‰å¯èƒ½çš„é—®é¢˜æ‰æ‰¾åˆ°å®ƒï¼Œèµ·å› ï¼šå°†ä»£ç ä» load stata_dict è½¬ä¸º PEFT ä»¥ä¾›å­¦ä¹ ã€‚

```python
# åŸå§‹é¡¹ç›®ä»£ç ï¼ˆæ­£ç¡®ï¼‰ï¼š
# å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet
text_encoder = get_peft_model(text_encoder, lora_config)
unet = get_peft_model(unet, lora_config)

# å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡ï¼Œå½“ç„¶ï¼Œä½ å¯ä»¥ä¿®æ”¹ model_path æ¥æŒ‡å®šå…¶ä»–çš„è·¯å¾„
if resume:
    # åŠ è½½ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹æƒé‡ï¼Œæ³¨æ„è¿™é‡ŒåªåŠ è½½æƒé‡ï¼Œè€Œä¸æ˜¯è¦†ç›–æ•´ä¸ªæ¨¡å‹ï¼Œè¦†ç›–ï¼šmodel = torch.load(...)
    text_encoder = torch.load(os.path.join(model_path, "text_encoder.pt"))
    unet = torch.load(os.path.join(model_path, "unet.pt"))
```

è½¬æ¢ä¸º PEFT å½¢å¼ï¼š

```python
# é”™è¯¯çš„ç¤ºèŒƒ
# å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet
text_encoder = get_peft_model(text_encoder, lora_config)
unet = get_peft_model(unet, lora_config)

# å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡
if resume:
    # ä½¿ç”¨ PEFT çš„ from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹
    text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, "text_encoder"))
    unet = PeftModel.from_pretrained(unet, os.path.join(model_path, "unet"))
```

å¾ˆå¥½ï¼Œç°åœ¨æˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªä¸ä¼šæŠ¥é”™ï¼Œä½†æ˜¯æ•ˆæœå’Œæ²¡åŠ  LoRA **å®Œå…¨ç›¸åŒ**çš„æ¨¡å‹ï¼ŒçœŸæ˜¯å¤ªæ£’äº†ï¼ˆå®ƒçœŸçš„å¤ªåˆé’»äº†ğŸ˜¡ï¼‰ã€‚

æ¥çœ‹çœ‹å®ƒç©¶ç«Ÿæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œä¸ºäº†æ¸…æ™°ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚è¿›è¡Œæ¼”ç¤ºï¼š

```python
import torch
import torch.nn as nn
from torch.optim import Adam
from copy import deepcopy
from peft import get_peft_model, LoraConfig, PeftModel

# å›ºå®šéšæœºæ•°ç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)

# å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹
class LinearModel(nn.Module):
    def __init__(self, input_size, output_size):
        super(LinearModel, self).__init__()
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x):
        return self.linear(x)

# å®ä¾‹åŒ–çº¿æ€§æ¨¡å‹
model = LinearModel(input_size=10, output_size=1)

# åœ¨åº”ç”¨ LoRA ä¹‹å‰æ·±æ‹·è´åŸå§‹æ¨¡å‹ï¼Œç¡®ä¿åç»­å…¬å¹³æ¯”è¾ƒ
original_model = deepcopy(model)

# é…ç½® LoRA å‚æ•°
config = LoraConfig(
    inference_mode=False,
    r=4,
    lora_alpha=16,
    target_modules=['linear'],
)

# å°† LoRA åº”ç”¨åˆ°æ¨¡å‹ä¸­
lora_model = get_peft_model(model, config)

# å®šä¹‰ä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = Adam(lora_model.parameters(), lr=1e-3)

# ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿçš„è®­ç»ƒæ•°æ®
input_data = torch.randn(100, 10)  # 100 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 10 ä¸ªç‰¹å¾
target_data = torch.randn(100, 1)  # å¯¹åº”çš„ç›®æ ‡å€¼

# è®­ç»ƒä¸€ä¸ªå›åˆ
lora_model.train()
for epoch in range(1):  # è®­ç»ƒ 1 ä¸ªå›åˆ
    optimizer.zero_grad()
    outputs = lora_model(input_data)
    loss = criterion(outputs, target_data)
    loss.backward()
    optimizer.step()

# è®­ç»ƒåä¿å­˜ LoRA æƒé‡
lora_model.save_pretrained('linear_lora_model')

# æ–¹æ³• 1ï¼šå…ˆä½¿ç”¨ get_peft_modelï¼Œå†åŠ è½½ LoRA æƒé‡
model1 = PeftModel.from_pretrained(get_peft_model(deepcopy(original_model), config), 'linear_lora_model')

# æ–¹æ³• 2ï¼šç›´æ¥åŠ è½½ LoRA æƒé‡
model2 = PeftModel.from_pretrained(deepcopy(original_model), 'linear_lora_model')

# ç”Ÿæˆç›¸åŒçš„è¾“å…¥æ•°æ®ä»¥è¿›è¡Œè¾“å‡ºæ¯”è¾ƒ
test_input = torch.randn(1, 10)

# æ¯”è¾ƒå››ä¸ªæ¨¡å‹çš„è¾“å‡ºï¼ˆåŸå§‹æ¨¡å‹ï¼ŒLoRAï¼Œæ–¹æ³•1ï¼Œæ–¹æ³•2ï¼‰
def compare_model_outputs(input_data):
    # åŸå§‹æ¨¡å‹
    original_output = original_model(input_data)
    print("åŸå§‹æ¨¡å‹è¾“å‡º:", original_output.detach().numpy())

    # è®­ç»ƒåçš„ LoRA æ¨¡å‹
    lora_output = lora_model(input_data)
    print("è®­ç»ƒåçš„ LoRA æ¨¡å‹è¾“å‡º:", lora_output.detach().numpy())

    # æ–¹æ³• 1ï¼šå…ˆä½¿ç”¨ get_peft_modelï¼Œå†åŠ è½½ LoRA
    output1 = model1(input_data)
    print("æ–¹æ³• 1ï¼ˆå…ˆä½¿ç”¨ get_peft_modelï¼Œå†åŠ è½½ LoRAï¼‰è¾“å‡º:", output1.detach().numpy())

    # æ–¹æ³• 2ï¼šç›´æ¥åŠ è½½ LoRA
    output2 = model2(input_data)
    print("æ–¹æ³• 2ï¼ˆç›´æ¥åŠ è½½ LoRAï¼‰è¾“å‡º:", output2.detach().numpy())

    if torch.allclose(original_output, output1):
        print("\nåŸå§‹æ¨¡å‹å’Œæ–¹æ³• 1 è¾“å‡ºç›¸åŒã€‚")
    if torch.allclose(lora_output, output2):
        print("è®­ç»ƒåçš„ LoRA æ¨¡å‹å’Œæ–¹æ³• 2 è¾“å‡ºç›¸åŒã€‚\n")

# æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°
def compare_params(m1, m2):
    for (n1, p1), (n2, p2) in zip(m1.named_parameters(), m2.named_parameters()):
        if n1 != n2 or not torch.allclose(p1, p2):
            print(f"å‚æ•°ä¸åŒ¹é…: \n{n1}\n{n2}")
            return False
    return True

# æ¯”è¾ƒå››ä¸ªæ¨¡å‹çš„è¾“å‡º
compare_model_outputs(test_input)

# æ£€æŸ¥æ–¹æ³• 1 å’Œæ–¹æ³• 2 çš„å‚æ•°æ˜¯å¦ä¸€è‡´
if compare_params(model1, model2):
    print("æ–¹æ³• 1 å’Œæ–¹æ³• 2 çš„ LoRA æ¨¡å‹å‚æ•°ä¸€è‡´ï¼")
else:
    print("æ–¹æ³• 1 å’Œæ–¹æ³• 2 çš„ LoRA æ¨¡å‹å‚æ•°ä¸ä¸€è‡´ï¼")
```

è¾“å‡ºï¼š

```python
åŸå§‹æ¨¡å‹è¾“å‡º: [[-0.03600371]]
è®­ç»ƒåçš„ LoRA æ¨¡å‹è¾“å‡º: [[-0.03428639]]
æ–¹æ³• 1ï¼ˆå…ˆä½¿ç”¨ get_peft_modelï¼Œå†åŠ è½½ LoRAï¼‰è¾“å‡º: [[-0.03600371]]
æ–¹æ³• 2ï¼ˆç›´æ¥åŠ è½½ LoRAï¼‰è¾“å‡º: [[-0.03428639]]

åŸå§‹æ¨¡å‹å’Œæ–¹æ³• 1 è¾“å‡ºç›¸åŒã€‚
è®­ç»ƒåçš„ LoRA æ¨¡å‹å’Œæ–¹æ³• 2 è¾“å‡ºç›¸åŒã€‚

å‚æ•°ä¸åŒ¹é…: 
base_model.model.base_model.model.linear.base_layer.weight
base_model.model.linear.base_layer.weight
æ–¹æ³• 1 å’Œæ–¹æ³• 2 çš„ LoRA æ¨¡å‹å‚æ•°ä¸ä¸€è‡´ï¼
```

ä»è¾“å‡ºä¸­å¯ä»¥çœ‹åˆ°ï¼Œ**æ–¹æ³• 1**ï¼ˆåœ¨åŠ è½½ LoRA ä¹‹å‰ä½¿ç”¨ `get_peft_model()`ï¼‰ä¸åŸå§‹æ¨¡å‹çš„è¾“å‡º**å®Œå…¨ç›¸åŒ**ï¼Œè¿™æ„å‘³ç€ LoRA æ²¡æœ‰è¢«æœ‰æ•ˆåº”ç”¨ã€‚è€Œ**æ–¹æ³• 2**ï¼ˆç›´æ¥ä½¿ç”¨ `PeftModel.from_pretrained()` åŠ è½½ LoRA æƒé‡ï¼‰çš„è¾“å‡ºä¸è®­ç»ƒåçš„ LoRA æ¨¡å‹è¾“å‡º**ä¸€è‡´**ï¼Œè¯´æ˜è¢«æ­£ç¡®åŠ è½½ã€‚

å¦å¤–ï¼Œä½ è¿˜å¯ä»¥çœ‹åˆ°æ–¹æ³• 1 çš„æ¨¡å‹æ¶æ„ä¼šå¤šä¸€ä¸ª base_model.model åŒ…è£¹ï¼Œå¦‚æœä½ æ„Ÿå…´è¶£çš„è¯å¯ä»¥ä½¿ç”¨ `print(model1)` è¿›ä¸€æ­¥åœ°æŸ¥çœ‹ï¼Œè¿™è¯æ˜äº†åœ¨åŠ è½½ LoRA ä¹‹å‰ä½¿ç”¨ `get_peft_model()` ä¼šå¹²æ‰°æ¨¡å‹ç»“æ„ï¼Œå¯¼è‡´ LoRA åº”ç”¨å¤±æ•ˆã€‚æˆ‘å·²ç»å‘å®˜æ–¹æå‡ºäº† [issue#2115](https://github.com/huggingface/peft/issues/2115)ï¼Œå¹¶å¾—åˆ°äº†å¾ˆç§¯æçš„å›å¤ï¼Œè¿™äº›å¼€å‘äººå‘˜å¾ˆæ£’ã€‚é¢„è®¡åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¼šè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå½“å‰ Bug ä¼šå‡ºç°åœ¨ç‰ˆæœ¬ <=0.12.0ã€‚


## å‚è€ƒé“¾æ¥

[PEFT - Hugging Face](https://huggingface.co/docs/peft/index)
