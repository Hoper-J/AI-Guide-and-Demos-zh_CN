# å¦‚ä½•åŠ è½½ GGUF æ¨¡å‹ï¼ˆåˆ†ç‰‡ & Shared & Split & 00001-of-0000...çš„è§£å†³æ–¹æ³•ï¼‰

> å¯¹ Transformers æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°ç‰¹æ€§ï¼Œæˆ–è®¸æœ‰å¿…è¦æˆä¸ºä¸€ä¸ªæ‹“å±•æ–‡ç« è¿›è¡Œä»‹ç»ã€‚
>
> `llama-cpp-python` å’Œ `ollama` çš„æ–¹æ³•ä¹Ÿä¼šåœ¨æœ¬æ–‡ä¸­æåˆ°ï¼Œæˆ‘ä»¬åœ¨å…¶ä»–æ–‡ç« ä¸­é‡‡å–çš„ GGUF åŠ è½½æ–¹å¼ä¸º `llama-cpp-python`ã€‚	
>
> å³ä¾¿ä½ æ²¡æœ‰æ˜¾å¡ï¼Œä¾æ—§å¯ä»¥åŠ è½½å¹¶éƒ¨ç½²å¤§æ¨¡å‹ï¼Œè¿™å°±æ˜¯ GGUFã€‚

## ç›®å½•

- [ä½¿ç”¨ ğŸ¤— Transformers åŠ è½½ GGUF æ¨¡å‹](#ä½¿ç”¨--transformers-åŠ è½½-gguf-æ¨¡å‹)
    - [ç¡®ä¿å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ Transformers](#ç¡®ä¿å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„-transformers)
    - [åŠ è½½å•ä¸ª GGUF æ–‡ä»¶](#åŠ è½½å•ä¸ª-gguf-æ–‡ä»¶)
    - [åŠ è½½åˆ†ç‰‡ï¼ˆShared/Splitï¼‰çš„ GGUF æ–‡ä»¶](#åŠ è½½åˆ†ç‰‡sharedsplitçš„-gguf-æ–‡ä»¶)
       - [è§£å†³æ–¹æ³•ï¼šåˆå¹¶åˆ†ç‰‡æ–‡ä»¶](#è§£å†³æ–¹æ³•åˆå¹¶åˆ†ç‰‡æ–‡ä»¶)
    - [æŸ¥çœ‹æ¨ç†çš„å†…å­˜å ç”¨](#æŸ¥çœ‹æ¨ç†çš„å†…å­˜å ç”¨)
- [ä½¿ç”¨ llama-cpp-python åŠ è½½ GGUF æ¨¡å‹](#ä½¿ç”¨-llama-cpp-python-åŠ è½½-gguf-æ¨¡å‹)
  - [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
  - [åŠ è½½å•ä¸ª GGUF æ–‡ä»¶](#åŠ è½½å•ä¸ª-gguf-æ–‡ä»¶-1)
  - [åŠ è½½åˆ†ç‰‡ï¼ˆShared/Splitï¼‰çš„ GGUF æ–‡ä»¶](#åŠ è½½åˆ†ç‰‡sharedsplitçš„-gguf-æ–‡ä»¶-1)
  - [æŸ¥çœ‹æ¨ç†çš„å†…å­˜å ç”¨](#æŸ¥çœ‹æ¨ç†çš„å†…å­˜å ç”¨-1)
  - [å¸è½½åˆ° GPUï¼ˆoffloadï¼‰](#å¸è½½åˆ°-gpuoffload)
     - [å…¨éƒ¨å¸è½½](#å…¨éƒ¨å¸è½½)
     - [éƒ¨åˆ†å¸è½½](#éƒ¨åˆ†å¸è½½)
- [ä½¿ç”¨ Ollama åŠ è½½ GGUF æ¨¡å‹](#ä½¿ç”¨-ollama-åŠ è½½-gguf-æ¨¡å‹)
  - [å®‰è£… Ollama](#å®‰è£…-ollama)
  - [åˆ›å»º Modelfile æ–‡ä»¶](#åˆ›å»º-modelfile-æ–‡ä»¶)
  - [åˆ›å»ºæ¨¡å‹](#åˆ›å»ºæ¨¡å‹)
  - [è¿è¡Œæ¨¡å‹](#è¿è¡Œæ¨¡å‹)
- [ç›¸å…³æ–‡ç« é˜…è¯»](#ç›¸å…³æ–‡ç« é˜…è¯»)

## ä½¿ç”¨ ğŸ¤— Transformers åŠ è½½ GGUF æ¨¡å‹

> æ³¨æ„ï¼ŒTransformers éƒ¨åˆ†å½“å‰æ›´åƒæ˜¯ä¸€ä¸ªæ•…äº‹ï¼Œæš‚æ—¶è¿˜æ— æ³•éƒ¨ç½²åˆ°å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒçœŸæ­£è¿›è¡Œéƒ¨ç½²å¯ä»¥çœ‹ [llama-cpp-python](#ä½¿ç”¨-llama-cpp-python-åŠ è½½-gguf-æ¨¡å‹) éƒ¨åˆ†ã€‚

æˆ‘ä»¬å½“ç„¶å¯ä»¥ä½¿ç”¨ `llama.cpp` æˆ– `ollama` è¿›è¡ŒåŠ è½½ï¼Œåœ¨ 2024 å¹´ 5 æœˆ 17 æ—¥ä¹‹å‰ï¼ˆä¹Ÿå°±æ˜¯ Transformers v4.41.0 å‘å¸ƒä¹‹å‰ï¼‰ï¼Œå®ƒä»¬æ˜¯å¸¸ç”¨çš„æ–¹å¼ã€‚ä¸è¿‡ç°åœ¨ï¼ŒTransformer ä¹Ÿæ”¯æŒäº†è¿™ä¸ªç‰¹æ€§ã€‚

å¦‚æœä½ æŸ¥çœ‹Transformers çš„å®˜æ–¹æ–‡æ¡£ï¼Œä¼šå‘ç°å®ƒä» [v4.41.0](https://github.com/huggingface/transformers/releases/tag/v4.41.0) å¼€å§‹ï¼Œå°±å·²ç»æ”¯æŒäº†å‡ ç§æµè¡Œçš„æ¨¡å‹æ¶æ„ï¼š

- LLaMa
- Mistral

éšåï¼Œåœ¨ v4.42.4 ä¸­ï¼Œä»–ä»¬å¢åŠ äº†å¯¹ Qwen2 çš„æ”¯æŒï¼Œå¹¶ä¸”ä» v4.45.1 å¼€å§‹ï¼Œå¤§å¹…å¢åŠ äº†å¯¹é‡åŒ–ç±»å‹å’Œæ¨¡å‹æ¶æ„çš„æ”¯æŒã€‚ä½ å¯ä»¥è®¿é—®[æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/gguf#supported-model-architectures)æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ã€‚

### ç¡®ä¿å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ Transformers

åœ¨å°è¯•ä½¿ç”¨æ–°ç‰¹æ€§ä¹‹å‰ï¼Œç¡®ä¿ä½ å®‰è£…çš„æ˜¯æœ€æ–°ç‰ˆæœ¬çš„ Transformersã€‚å¦‚æœä½ åˆšéµå¾ªæ•™ç¨‹è®­ç»ƒå®Œ LoRA æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œé‚£ä¹ˆä½ çš„ç‰ˆæœ¬å¯èƒ½æ˜¯ v4.41.2ï¼Œè¿™ä¸ªç‰ˆæœ¬åœ¨å¯¼å…¥ Qwen çš„ GGUF æ–‡ä»¶æ—¶ä¼šæŠ¥é”™ï¼š`ValueError: Architecture qwen2 not supported`ã€‚

å‘½ä»¤è¡Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
pip install numpy==1.24.4
pip install gguf
pip install --upgrade transformers
```

### åŠ è½½å•ä¸ª GGUF æ–‡ä»¶

ä»¥æ¨¡å‹ [qwen2.5-7b-instruct-q3_k_m.gguf](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/blob/main/qwen2.5-7b-instruct-q3_k_m.gguf) ä¸ºä¾‹ï¼Œå¦‚æœæˆ‘ä»¬é€‰æ‹© Q3_K_Mï¼Œå…¶å¯¹åº”çš„æ–‡ä»¶åä¸º`qwen2.5-7b-instruct-q3_k_m.gguf`ï¼ŒåŠ è½½å‘½ä»¤å¦‚ä¸‹ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "Qwen/Qwen2.5-7B-Instruct-GGUF"
filename = "qwen2.5-7b-instruct-q3_k_m.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename, clean_up_tokenization_spaces=True)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```

**ä¼¼ä¹ä¸€åˆ‡éƒ½å¾ˆå¥½**ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨è¿™ç§æ–¹å¼é€‰æ‹©æƒ³è¦åŠ è½½çš„é‡åŒ–æ¨¡å‹å°±å¯ä»¥äº†ã€‚

> å³ä¾¿ä½ çš„ç”µè„‘æ˜¯ Macï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è¿™æ®µå‘½ä»¤è¿›è¡ŒåŠ è½½ï¼ˆå…ˆä¸è¦è¿›è¡Œæ¨ç†ï¼Œå¾€ä¸‹çœ‹ï¼‰ã€‚
>
> ![image-20241007105154505](./assets/image-20241007105154505.png)

### åŠ è½½åˆ†ç‰‡ï¼ˆShared/Splitï¼‰çš„ GGUF æ–‡ä»¶

ä½†å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šå¯¹äº**åˆ†ç‰‡**çš„ GGUF æ–‡ä»¶ï¼Œæ€ä¹ˆåŠ è½½å‘¢ï¼Ÿ

**åˆ†ç‰‡**æŒ‡çš„æ˜¯æ–‡ä»¶åæœ«å°¾ä¸º`<ShardNum>-of-<ShardTotal>`å½¢å¼çš„ GGUF æ–‡ä»¶ã€‚ä»¥ Qwen2.5 ä¸ºä¾‹ï¼Œå®ƒçš„ Q4_K_M é‡åŒ–æ¨¡å‹è¢«åˆ†ä¸ºä¸¤ä¸ªæ–‡ä»¶ï¼š

- [qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/blob/main/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf)
- [qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/blob/main/qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf)

**Transformers å¯ä»¥è‡ªåŠ¨åŠ è½½åˆ†ç‰‡æ¨¡å‹å—ï¼Ÿ**

é—æ†¾çš„æ˜¯ï¼Œç»è¿‡ç ”ç©¶å‘ç°ï¼ŒTransformers æš‚æ—¶ä¸æ”¯æŒç›´æ¥åŠ è½½åˆ†ç‰‡æ¨¡å‹ã€‚æœ‰äººåœ¨ GitHub ä¸Šæå‡ºäº†ç›¸å…³çš„ Issueï¼š[Support loading shard GGUF models #32266](https://github.com/huggingface/transformers/issues/32266)ã€‚

å¦‚æœç›´æ¥å°è¯•åŠ è½½ `qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf`ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "Qwen/Qwen2.5-7B-Instruct-GGUF"
filename = "qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename, clean_up_tokenization_spaces=True)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```

ä½ ä¼šå¾—åˆ°è¿™æ ·çš„è¾“å‡ºï¼š

```
Converting and de-quantizing GGUF tensors...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 280/280 [00:39<00:00,  7.05it/s]
Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct-GGUF and are newly initialized: ['model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 
...]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

å…¶ä¸­å‡ºç°å¤§é‡æƒé‡æœªè¢«åŠ è½½çš„è­¦å‘Šï¼Œè¿™æ˜¯å› ä¸ºç¬¬äºŒä¸ªåˆ†ç‰‡çš„æ•°æ®æ²¡æœ‰è¢«åŠ è½½ï¼Œå¯¼è‡´éƒ¨åˆ†æƒé‡è¢«éšæœºåˆå§‹åŒ–ã€‚è€Œè¿™äº›æœªåŠ è½½çš„æƒé‡æ­£æ˜¯åˆ†ç‰‡ 2 ä¸­çš„æ•°æ®ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![åˆ†ç‰‡ 2](./assets/image-20241007165652713.png)

é‚£ä¹ˆï¼Œå¯¹äºåˆ†ç‰‡æ–‡ä»¶ï¼Œå°±åªèƒ½æ‰‹åŠ¨åˆå¹¶æˆ–è€…ç”¨ `llama-cpp-python` äº†ï¼ŒçŒœæµ‹ Qwen é‡‡ç”¨çš„æ˜¯ `llama.cpp` ä¸­çš„ `llama-gguf-split` è¿›è¡Œæ‹†åˆ†ï¼Œæ‰€ä»¥ï¼Œè®©æˆ‘ä»¬ä¸‹è½½å®ƒè¿›è¡Œåˆå¹¶å°è¯•ã€‚

#### è§£å†³æ–¹æ³•ï¼šåˆå¹¶åˆ†ç‰‡æ–‡ä»¶

éµå¾ªæ­¥éª¤ï¼š

1. **ä¸‹è½½åˆ†ç‰‡æ–‡ä»¶**

   é¦–å…ˆï¼Œå®‰è£… `huggingface-hub`ï¼š

   ```bash
   pip install huggingface-hub
   ```

   ç„¶åï¼Œä¸‹è½½åˆ†ç‰‡æ–‡ä»¶ï¼š

   ```bash
   huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf --local-dir . --local-dir-use-symlinks False
   huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf --local-dir . --local-dir-use-symlinks False
   ```

2. **å…‹éš† `llama.cpp` ä»“åº“å¹¶ç¼–è¯‘**

   ```bash
   git clone https://github.com/ggerganov/llama.cpp
   cd llama.cpp
   make
   ```

3. **ä½¿ç”¨ `llama-gguf-split` å·¥å…·åˆå¹¶åˆ†ç‰‡æ–‡ä»¶**

   åœ¨ `llama.cpp` ç›®å½•ä¸‹ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆå¯ä»¥ä¿®æ”¹æ–‡ä»¶è·¯å¾„ï¼Œéµå¾ª `./llama-gguf-split --merge [åˆ†å—1çš„è·¯å¾„] [ç›®æ ‡æ–‡ä»¶å]` å³å¯ï¼‰ï¼š

   ```bash
   ./llama-gguf-split --merge ../qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf ../qwen2.5-7b-instruct-q4_k_m.gguf
   ```

   è¿™å°†æŠŠä¸¤ä¸ªåˆ†ç‰‡æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´çš„ `qwen-7b-instruct-q4_k_m.gguf` æ–‡ä»¶ï¼š

   ![åˆå¹¶æˆåŠŸ](./assets/image-20241007000104557.png)

   ä»è¾“å‡ºå¯ä»¥çœ‹åˆ°æ–‡ä»¶æˆåŠŸè¿›è¡Œäº†åˆå¹¶ã€‚

4. **åŠ è½½åˆå¹¶åçš„ GGUF æ–‡ä»¶**

   ```python
   from transformers import AutoTokenizer, AutoModelForCausalLM
   
   # æŒ‡å®šæ¨¡å‹æ–‡ä»¶æ‰€åœ¨çš„æœ¬åœ°ç›®å½•
   model_path = "./"  # å¦‚æœæ¨¡å‹æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹
   
   # åˆå¹¶åçš„ GGUF æ–‡ä»¶å
   gguf_file = "qwen2.5-7b-instruct-q4_k_m.gguf"
   
   # ä»æœ¬åœ°åŠ è½½æ¨¡å‹
   tokenizer = AutoTokenizer.from_pretrained(model_path, gguf_file=gguf_file, clean_up_tokenization_spaces=True)
   model = AutoModelForCausalLM.from_pretrained(model_path, gguf_file=gguf_file)
   ```

   **è¾“å‡º**ï¼š

   ```
   Converting and de-quantizing GGUF tensors...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 339/339 [00:57<00:00,  5.92it/s]
   ```

   æˆåŠŸåŠ è½½ğŸ‰ï¼Œæ¥éªŒè¯ä¸€ä¸‹ï¼š

    ```python
    input_text = "Hello, World!"
    inputs = tokenizer(input_text, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=50)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    ```

    **è¾“å‡º**ï¼š

    ```
    Hello, World! I'm a 21-year-old software engineer with a passion for learning and a love for technology. I'm currently working on a project that involves building a web application using React and Node.js. I'm also interested in machine learning and have been
    ```

    è¿˜æ˜¯ä¸€æ ·ï¼Œ**â€œä¼¼ä¹ä¸€åˆ‡éƒ½å¾ˆå¥½â€**ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å†…å­˜å ç”¨ã€‚

### æŸ¥çœ‹æ¨ç†çš„å†…å­˜å ç”¨

ä½¿ç”¨ `psutil` å’Œ `pynvml` åº“æ¥ç›‘æµ‹å†…å­˜å ç”¨æƒ…å†µï¼Œå…ˆè¿›è¡Œå®‰è£…ï¼š

```bash
pip install psutil
pip install pynvml
```

ç„¶åï¼Œåœ¨ä»£ç ä¸­æ·»åŠ å†…å­˜ç›‘æ§ï¼ˆä¸ç”¨å…³å¿ƒè¿™é‡Œçš„ä»£ç ç»†èŠ‚ï¼Œçœ‹ç»“æœï¼‰ï¼š

```python
import psutil
from transformers import AutoTokenizer, AutoModelForCausalLM
from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlShutdown

# åˆå§‹åŒ– NVML
nvmlInit()

# è·å– GPU å¥æŸ„ï¼ˆå‡è®¾ä½¿ç”¨çš„æ˜¯ GPU 0ï¼‰
gpu_index = 0
handle = nvmlDeviceGetHandleByIndex(gpu_index)

def get_gpu_memory():
    """
    è·å–æŒ‡å®š GPU çš„å·²ç”¨æ˜¾å­˜ï¼ˆå•ä½ï¼šMBï¼‰
    """
    info = nvmlDeviceGetMemoryInfo(handle)
    used = info.used / 1024 ** 2  # è½¬æ¢ä¸º MB
    return used

# è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä¿¡æ¯
process = psutil.Process()

# è·å–æ¨¡å‹åŠ è½½å‰çš„ CPU å’Œ GPU å†…å­˜
cpu_before = process.memory_info().rss / 1024 ** 2  # è½¬æ¢ä¸º MB
try:
    gpu_used_before = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_before = 0

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = "./"  # å¦‚æœæ¨¡å‹æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹

# åˆå¹¶åçš„ GGUF æ–‡ä»¶å
gguf_file = "qwen2.5-7b-instruct-q4_k_m.gguf"

# ä»æœ¬åœ°åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path, gguf_file=gguf_file, clean_up_tokenization_spaces=True)
model = AutoModelForCausalLM.from_pretrained(model_path, gguf_file=gguf_file)

# è·å–æ¨¡å‹åŠ è½½åçš„ CPU å’Œ GPU å†…å­˜
cpu_after_load = process.memory_info().rss / 1024 ** 2
try:
    gpu_used_after_load = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_after_load = 0

# è®¡ç®—åŠ è½½æ¨¡å‹åçš„å†…å­˜å˜åŒ–é‡
cpu_change_load = cpu_after_load - cpu_before
gpu_change_load = gpu_used_after_load - gpu_used_before

print(f"åŠ è½½æ¨¡å‹åCPUå†…å­˜å˜åŒ–é‡ï¼š{cpu_change_load:+.2f} MB")
print(f"åŠ è½½æ¨¡å‹åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š{gpu_change_load:+.2f} MB")

# åœ¨ç”Ÿæˆæ–‡æœ¬å‰çš„å†…å­˜çŠ¶æ€
input_text = "Hello, World!"
inputs = tokenizer(input_text, return_tensors="pt")
cpu_before_inference = process.memory_info().rss / 1024 ** 2
try:
    gpu_used_before_inference = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_before_inference = 0

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(**inputs, max_new_tokens=50)

# è·å–æ¨ç†åçš„ CPU å’Œ GPU å†…å­˜
cpu_after_inference = process.memory_info().rss / 1024 ** 2
try:
    gpu_used_after_inference = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_after_inference = 0

# è®¡ç®—æ¨ç†åçš„å†…å­˜å˜åŒ–é‡
cpu_change_inference = cpu_after_inference - cpu_before_inference
gpu_change_inference = gpu_used_after_inference - gpu_used_before_inference

print(f"æ¨ç†åCPUå†…å­˜å˜åŒ–é‡ï¼š{cpu_change_inference:+.2f} MB")
print(f"æ¨ç†åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š{gpu_change_inference:+.2f} MB")

# è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
print("\nç”Ÿæˆçš„æ–‡æœ¬ï¼š")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# å…³é—­ NVML
nvmlShutdown()

```

**è¾“å‡º**ï¼š

```
Converting and de-quantizing GGUF tensors...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 339/339 [00:39<00:00,  8.69it/s]
åŠ è½½æ¨¡å‹åCPUå†…å­˜å˜åŒ–é‡ï¼š+27826.30 MB
åŠ è½½æ¨¡å‹åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+0.00 MB
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
æ¨ç†åCPUå†…å­˜å˜åŒ–é‡ï¼š+344.52 MB
æ¨ç†åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+0.00 MB

ç”Ÿæˆçš„æ–‡æœ¬ï¼š
Hello, World! I'm a 21-year-old software engineer with a passion for learning and a love for technology. I'm currently working on a project that involves building a web application using React and Node.js. I'm also interested in machine learning and have been
```

**å¯ä»¥æ³¨æ„åˆ°ï¼ŒGGUF çš„ç¡®å¯ä»¥åªä½¿ç”¨ CPU è¿›è¡Œæ¨ç†ï¼ŒQwen2.5â€”7B çš„ Q4_K_M æ¨¡å‹éœ€è¦ 28GB çš„å†…å­˜ï¼Œè€Œæ˜¾å­˜ä¸º 0ã€‚**

æ€è€ƒä¸€ä¸‹ï¼Œè¿™å¯¹å—ï¼Ÿ**å ç”¨ 28GB å†…å­˜**ï¼Œæ¯”é¢„æœŸå¤šå¤ªå¤šäº†ï¼Œæ‰“å°ä¸€ä¸‹å‚æ•°çœ‹çœ‹ï¼š

```python
for name, param in model.named_parameters():
    print(f"å‚æ•°åç§°: {name}, æ•°æ®ç±»å‹: {param.dtype}")
```

**è¾“å‡º**ï¼š

```
å‚æ•°åç§°: model.embed_tokens.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.self_attn.q_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.self_attn.q_proj.bias, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.self_attn.k_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.self_attn.k_proj.bias, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.self_attn.v_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.self_attn.v_proj.bias, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.self_attn.o_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.mlp.gate_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.mlp.up_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.mlp.down_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.input_layernorm.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.0.post_attention_layernorm.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.1.self_attn.q_proj.weight, æ•°æ®ç±»å‹: torch.float32
å‚æ•°åç§°: model.layers.1.self_attn.q_proj.bias, æ•°æ®ç±»å‹: torch.float32
...
```

å¯ä»¥çœ‹åˆ°å…¨éƒ¨ä»¥ FP32 çš„å½¢å¼è¿›è¡ŒåŠ è½½ï¼ˆä¸çŸ¥é“ä¸ºä»€ä¹ˆè¿˜æ²¡æœ‰å®ç°é»˜è®¤çš„ç²¾åº¦è¯†åˆ«ï¼Œæˆ–è®¸æœ‰å¯¹åº”å‚æ•°ï¼Œä½†å®˜æ–¹æ–‡æ¡£å’Œæºç å¹¶æ²¡æœ‰ç›´è§‚å†™å‡ºï¼‰ã€‚è€Œå¦‚æœé‡‡å– BitsAndBytesConfigï¼Œè®¾ç½® int4 ç±»å‹è¿›è¡ŒåŠ è½½çš„è¯ï¼Œä¼šæŠ¥é”™ï¼š

```
ValueError: You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.
```

å½“ç„¶ï¼Œæ— è®ºæ˜¯å¦æŠ¥é”™ï¼Œç®€å•çš„è®¾ç½®ä¸º int4 éƒ½æ˜¯ä¸å¯¹çš„ï¼Œæ‰€ä»¥è¿™ä¸€ç‚¹æ²¡æœ‰é—®é¢˜ï¼Œä½†æˆ‘å¾ˆç–‘æƒ‘å®˜æ–¹æ–‡æ¡£çš„æ”¥å†™ä¸ºä»€ä¹ˆä¸ç›´è§‚çš„è¯´æ˜å®Œæ•´çš„ä½¿ç”¨æ–¹æ³•ï¼Œè€Œæ˜¯æˆ›ç„¶è€Œæ­¢ç»™äº†ä¸€ä¸ªåŠæˆå“ã€‚

å¦‚æœä½ æœ‰æ³¨æ„åˆ°ä¹‹å‰çš„è¾“å‡ºï¼š

```
Converting and de-quantizing GGUF tensors...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 339/339 [00:33<00:00, 10.00it/s]
```

ä½ å°±ä¼šå‘ç°ï¼Œå®é™…ä¸Šåœ¨åŠ è½½ GGUF æ–‡ä»¶æ—¶ï¼Œå®ƒå·²ç»è¢«**åé‡åŒ–**äº†ï¼Œæ‰€ä»¥ï¼Œè¿™ä¸ªç‰¹æ€§æ ¹æœ¬æ²¡æœ‰çœŸæ­£æ„ä¹‰ä¸Šçš„åŠ è½½ï¼Œè€Œæ˜¯å°† GGUF å½“æˆäº†â€œå‹ç¼©æ–‡ä»¶â€ã€‚

ä¸€æ—¶å…´èµ·ä¸ºå‘ç°æ–°ç‰¹æ€§è€Œæ”¥å†™çš„æ–‡ç« æœ‰ä¸€ç§è™å¤´è›‡å°¾çš„æ„Ÿè§‰ ï¼Œç­‰å¾…ç¤¾åŒºåç»­çš„ç»§ç»­å¼€å‘ï¼Œå¦‚æœè§£å†³äº†é—®é¢˜æˆ‘ä¼šå°†å…¶åŒæ­¥ã€‚

é‚£ï¼Œå¦‚æœæƒ³æ­£ç¡®åŠ è½½ GGUF æ–‡ä»¶ï¼Œæœ‰ä»€ä¹ˆè§£å†³æ–¹æ³•å—ï¼Ÿ

**ç­”ï¼šä½¿ç”¨ `llama-cpp-python` ã€‚**

## ä½¿ç”¨ llama-cpp-python åŠ è½½ GGUF æ¨¡å‹

ç›¸æ¯”ä¹‹ä¸‹ï¼Œ`llama-cpp-python` èƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒ GGUF æ ¼å¼çš„é‡åŒ–æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨åŠ è½½åˆ†ç‰‡æ¨¡å‹æ—¶ä¹Ÿæ›´ä¸ºæ–¹ä¾¿ã€‚

### ç¯å¢ƒé…ç½®

ä¸ºäº†ç¡®ä¿ä¹‹åçš„ "offload" æ­£å¸¸å·¥ä½œï¼Œéœ€è¦è¿›è¡Œä¸€äº›é¢å¤–çš„å·¥ä½œã€‚

é¦–å…ˆï¼Œæ‰¾åˆ° CUDA çš„å®‰è£…è·¯å¾„ï¼š

```bash
find /usr/local -name "cuda" -exec readlink -f {} \;
```

**å‚æ•°è§£é‡Š**ï¼š

- `-name "cuda"`ï¼šåœ¨ `/usr/local` ç›®å½•ä¸‹æœç´¢åä¸º "cuda" çš„æ–‡ä»¶æˆ–ç›®å½•ã€‚
- `-exec readlink -f {} \;`ï¼šå¯¹æ‰¾åˆ°çš„æ¯ä¸ªæ–‡ä»¶æˆ–ç›®å½•æ‰§è¡Œ `readlink -f`ï¼Œè·å–å…¶å®Œæ•´çš„ç»å¯¹è·¯å¾„ã€‚

å‡è®¾è¾“å‡ºå¦‚ä¸‹ï¼š

```
/usr/local/cuda-12.1
...
```

å¤åˆ¶è¿™ä¸ªè·¯å¾„ï¼Œè®¾ç½® `CUDA_HOME` ç¯å¢ƒå˜é‡ï¼š

```bash
export CUDA_HOME=/usr/local/cuda-12.1
```

æ¥ä¸‹æ¥ï¼Œå®‰è£… `llama-cpp-python`ï¼š

```bash
CMAKE_ARGS="-DGGML_CUDA=on \
            -DCUDA_PATH=${CUDA_HOME} \
            -DCUDAToolkit_ROOT=${CUDA_HOME} \
            -DCUDAToolkit_INCLUDE_DIR=${CUDA_HOME} \
            -DCUDAToolkit_LIBRARY_DIR=${CUDA_HOME}/lib64 \
            -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc" \
FORCE_CMAKE=1 \
pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir --verbose
```

### åŠ è½½å•ä¸ª GGUF æ–‡ä»¶

```python
from llama_cpp import Llama

repo_id = "Qwen/Qwen2.5-7B-Instruct-GGUF"
filename = "qwen2.5-7b-instruct-q3_k_m.gguf"

llm = Llama.from_pretrained(repo_id=repo_id, filename=filename)
```

å¦‚æœä½ ä¸‹è½½åˆ°äº†æŸä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ `model_path` **æŒ‡å®šè·¯å¾„**è¿›è¡ŒåŠ è½½ï¼š

```python
from llama_cpp import Llama

model_path = "./qwen2.5-7b-instruct-q3_k_m.gguf"

llm = Llama(model_path=model_path)
```

### åŠ è½½åˆ†ç‰‡ï¼ˆShared/Splitï¼‰çš„ GGUF æ–‡ä»¶

å¯¹äºåˆ†ç‰‡çš„æ¨¡å‹ï¼Œåªéœ€åœ¨ `additional_files` å‚æ•°ä¸­æŒ‡å®šå…¶ä»–åˆ†ç‰‡æ–‡ä»¶ï¼š

```python
from llama_cpp import Llama

repo_id = "Qwen/Qwen2.5-7B-Instruct-GGUF"
filename = "qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
additional_files = ["qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf"]

llm = Llama.from_pretrained(repo_id=repo_id, filename=filename, additional_files=additional_files)
```

`llama-cpp-python` ä¼šè‡ªåŠ¨åŠ è½½å¹¶åˆå¹¶ã€‚

**æŒ‡å®šè·¯å¾„**ï¼š

```python
from llama_cpp import Llama

model_path = "./qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
additional_files = ["./qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf"]

llm = Llama(model_path=model_path, additional_files=additional_files)
```

### æŸ¥çœ‹æ¨ç†çš„å†…å­˜å ç”¨

å®‰è£… `psutil` å’Œ `pynvml` åº“ï¼š

```bash
pip install psutil
pip install pynvml
```

ç°åœ¨ï¼Œæ¥çœ‹çœ‹æ­£ç¡®çš„å†…å­˜å ç”¨åº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæ‰§è¡Œï¼š

```python
import psutil
from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlShutdown
from llama_cpp import Llama

# åˆå§‹åŒ– NVML
nvmlInit()

# è·å– GPU å¥æŸ„ï¼ˆå‡è®¾ä½¿ç”¨çš„æ˜¯ GPU 0ï¼‰
gpu_index = 0
handle = nvmlDeviceGetHandleByIndex(gpu_index)

def get_gpu_memory():
    """
    è·å–æŒ‡å®š GPU çš„å·²ç”¨æ˜¾å­˜ï¼ˆå•ä½ï¼šMBï¼‰
    """
    info = nvmlDeviceGetMemoryInfo(handle)
    used = info.used / 1024 ** 2  # è½¬æ¢ä¸º MB
    return used

# è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä¿¡æ¯
process = psutil.Process()

# æ¨¡å‹åŠ è½½å‰çš„ CPU å’Œ GPU å†…å­˜
cpu_before = process.memory_info().rss / 1024 ** 2  # è½¬æ¢ä¸º MB
try:
    gpu_used_before = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_before = 0

# æ¨¡å‹åŠ è½½
model_path = "./qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
additional_files = ["./qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf"]

llm = Llama(model_path=model_path, additional_files=additional_files, n_gpu_layers=-1)

# æ¨¡å‹åŠ è½½åçš„ CPU å’Œ GPU å†…å­˜
cpu_after_load = process.memory_info().rss / 1024 ** 2
try:
    gpu_used_after_load = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_after_load = 0

# è®¡ç®—åŠ è½½æ¨¡å‹åçš„å†…å­˜å˜åŒ–é‡
cpu_change_load = cpu_after_load - cpu_before
gpu_change_load = gpu_used_after_load - gpu_used_before

print(f"åŠ è½½æ¨¡å‹åCPUå†…å­˜å˜åŒ–é‡ï¼š{cpu_change_load:+.2f} MB")
print(f"åŠ è½½æ¨¡å‹åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š{gpu_change_load:+.2f} MB")

# ç”Ÿæˆæ–‡æœ¬å‰çš„å†…å­˜çŠ¶æ€
input_text = "Hello, World!"
cpu_before_inference = process.memory_info().rss / 1024 ** 2
try:
    gpu_used_before_inference = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_before_inference = 0

# æ¨ç†é˜¶æ®µ
output = llm(input_text, max_tokens=50)

# æ¨ç†åçš„ CPU å’Œ GPU å†…å­˜
cpu_after_inference = process.memory_info().rss / 1024 ** 2
try:
    gpu_used_after_inference = get_gpu_memory()
except Exception as e:
    print(f"è·å–GPUæ˜¾å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    gpu_used_after_inference = 0

# è®¡ç®—æ¨ç†åçš„å†…å­˜å˜åŒ–é‡
cpu_change_inference = cpu_after_inference - cpu_before_inference
gpu_change_inference = gpu_used_after_inference - gpu_used_before_inference

print(f"æ¨ç†åCPUå†…å­˜å˜åŒ–é‡ï¼š{cpu_change_inference:+.2f} MB")
print(f"æ¨ç†åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š{gpu_change_inference:+.2f} MB")

# è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
print("\nç”Ÿæˆçš„æ–‡æœ¬ï¼š")
print(output['choices'][0]['text'])

# å…³é—­ NVML
nvmlShutdown()

```

**è¾“å‡º**ï¼š

```
åŠ è½½æ¨¡å‹åCPUå†…å­˜å˜åŒ–é‡ï¼š+4655.02 MB
åŠ è½½æ¨¡å‹åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+836.12 MB
llama_perf_context_print:        load time =     125.71 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =    4617.21 ms /    53 tokens
æ¨ç†åCPUå†…å­˜å˜åŒ–é‡ï¼š+0.06 MB
æ¨ç†åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+0.00 MB

ç”Ÿæˆçš„æ–‡æœ¬ï¼š
 : The Story of the First Computer Program
by Alice Aspnes
Illustrated by Elise Wilcox
Reviewed by Sarah W. (age 10)
Sarah W. is a student in Mrs. Karp's 4th Grade
```

ç°åœ¨ï¼Œæˆ‘ä»¬æˆåŠŸæ­£ç¡®çš„åŠ è½½äº† GGUF æ–‡ä»¶ã€‚

### å¸è½½åˆ° GPUï¼ˆoffloadï¼‰

æ˜¯çš„ï¼Œè¿™é‡Œæ˜¯å¸è½½è€ŒéåŠ è½½ã€‚ã€Œå†…å­˜ä¸å¤Ÿæ˜¾å­˜æ¥å‡‘ã€:)

#### å…¨éƒ¨å¸è½½

ä½¿ç”¨ `n_gpu_layers=-1` å¯ä»¥å°†æ‰€æœ‰å±‚å¸è½½åˆ° GPUï¼Œå‚è§[`llama_cpp.Llama`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#__codelineno-0-147)ï¼Œå³ï¼š

```python
llm = Llama(model_path=model_path, additional_files=additional_files, n_gpu_layers=-1)
```

å†æ¬¡æ‰§è¡Œä¹‹å‰çš„æµ‹é‡ä»£ç ï¼ˆåˆ«å¿˜äº†æ›¿æ¢å‚æ•°ï¼‰ï¼Œ**è¾“å‡º**ï¼š

```
åŠ è½½æ¨¡å‹åCPUå†…å­˜å˜åŒ–é‡ï¼š+449.30 MB
åŠ è½½æ¨¡å‹åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+4606.12 MB
llama_perf_context_print:        load time =      86.56 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =     861.72 ms /    53 tokens
æ¨ç†åCPUå†…å­˜å˜åŒ–é‡ï¼š+169.19 MB
æ¨ç†åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+88.00 MB

ç”Ÿæˆçš„æ–‡æœ¬ï¼š
 : The First 150 Years of the World's Most Famous Computer Programming Book
1978. A young programmer named Brian Kernighan walks into a room at Bell Laboratories. He's going to write a book with his colleague Dennis
```

å¦‚æœä½ çš„å†…å­˜å ç”¨å’Œä¹‹å‰ä¸€è‡´ï¼Œé‚£ä¹ˆè¯·æ ¹æ®æ–‡ç« ä¹‹å‰æä¾›çš„[æ–¹æ³•](#ç¯å¢ƒé…ç½®)é‡æ–°å®‰è£… `llama-cpp-python`ã€‚

#### éƒ¨åˆ†å¸è½½

ä½ ä¹Ÿå¯ä»¥æŒ‡å®šå¸è½½çš„å±‚æ•°ï¼Œä¿®æ”¹å‚æ•° `n_gpu_layers`ï¼Œæ¯”å¦‚è®¾ä¸º 8ï¼Œè¾“å‡ºï¼š

```
åŠ è½½æ¨¡å‹åCPUå†…å­˜å˜åŒ–é‡ï¼š+3890.05 MB
åŠ è½½æ¨¡å‹åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+1948.12 MB
llama_perf_context_print:        load time =     194.94 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =    3720.65 ms /    53 tokens
æ¨ç†åCPUå†…å­˜å˜åŒ–é‡ï¼š+162.46 MB
æ¨ç†åGPUæ˜¾å­˜å˜åŒ–é‡ï¼š+38.00 MB

ç”Ÿæˆçš„æ–‡æœ¬ï¼š
 : The Art of Computer Programming in the Age of the Internet
1981, when the first edition of Donald Knuth's "The Art of Computer Programming" was published, was a time when computer programming was still in its infancy. Computers
```

## ä½¿ç”¨ Ollama åŠ è½½ GGUF æ¨¡å‹

### å®‰è£… Ollama

æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://github.com/ollama/ollama?tab=readme-ov-file)å®‰è£…`Ollama`ã€‚ä»¥ Linux ä¸ºä¾‹ï¼š

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

![image-20241006232503510](./assets/image-20241006232503510.png)

### åˆ›å»º Modelfile æ–‡ä»¶

ç®€å•æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨åˆšåˆšåˆå¹¶çš„ Q4_K_Mï¼Œå›åˆ°æ‰€åœ¨ç›®å½•æŸ¥çœ‹æ–‡ä»¶åã€‚

```bash
# ç”¨å‘½ä»¤å›åˆ° Q4_K_M æ‰€åœ¨çš„ç›®å½•ä¸‹
cd ..
ls | grep qw
```

![image-20241007095500211](./assets/image-20241007095500211.png)

å¯ä»¥çœ‹åˆ° `qwen-7b-instruct-q4_k_m.gguf`ï¼Œåœ¨æ¨¡å‹æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º `ModelFile` æ–‡ä»¶ï¼Œå½“å‰æ¨¡ç‰ˆæ¥æºäº [Qwen å®˜æ–¹æ–‡æ¡£](https://qwen.readthedocs.io/zh-cn/latest/run_locally/ollama.html#run-ollama-with-your-gguf-files)ã€‚æ‰§è¡Œ `vim ModelFile` å‘½ä»¤ï¼š

```
FROM qwen2.5-7b-instruct-q4_k_m.gguf

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER repeat_penalty 1.05
PARAMETER top_k 20

TEMPLATE """{{ if .Messages }}
{{- if or .System .Tools }}<|im_start|>system
{{ .System }}
{{- if .Tools }}

# Tools

You are provided with function signatures within <tools></tools> XML tags:
<tools>{{- range .Tools }}
{"type": "function", "function": {{ .Function }}}{{- end }}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call>
{{- end }}<|im_end|>
{{ end }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 -}}
{{- if eq .Role "user" }}<|im_start|>user
{{ .Content }}<|im_end|>
{{ else if eq .Role "assistant" }}<|im_start|>assistant
{{ if .Content }}{{ .Content }}
{{- else if .ToolCalls }}<tool_call>
{{ range .ToolCalls }}{"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}}
{{ end }}</tool_call>
{{- end }}{{ if not $last }}<|im_end|>
{{ end }}
{{- else if eq .Role "tool" }}<|im_start|>user
<tool_response>
{{ .Content }}
</tool_response><|im_end|>
{{ end }}
{{- if and (ne .Role "assistant") $last }}<|im_start|>assistant
{{ end }}
{{- end }}
{{- else }}
{{- if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ end }}{{ .Response }}{{ if .Response }}<|im_end|>{{ end }}"""

# set the system message
SYSTEM """You are Qwen, created by Alibaba Cloud. You are a helpful assistant."""
```

å¤åˆ¶é»è´´åä½¿ç”¨ `esc` + `:wq` è¿›è¡Œä¿å­˜å¹¶é€€å‡ºã€‚

å¦‚æœä½ å¯¹å…¶ä¸­çš„å‚æ•°æ„Ÿå…´è¶£ï¼ŒæŸ¥é˜… [modelfile - docs](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)ï¼Œæœ‰æ—¶é—´çš„è¯æˆ‘ä¼šå†™ä¸€ä¸ªæ‹“å±•é˜…è¯»è¯¦ç»†è§£é‡Šã€‚

### åˆ›å»ºæ¨¡å‹

è¿è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºæ¨¡å‹ï¼š

```bash
ollama create qwen2.5_7b_Q4 -f Modelfile
```

ä½ åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼äºä¸‹é¢çš„ä¿¡æ¯ï¼š

![åˆ›å»ºæ¨¡å‹](./assets/image-20241007103722008.png)

å‘½ä»¤è¡Œè¾“å…¥ï¼š

```bash
ollama list
```

å¯ä»¥çœ‹åˆ°çš„ç¡®åˆ›å»ºæˆåŠŸäº†ã€‚

![æ¨¡å‹åˆ—è¡¨](./assets/image-20241007103902997.png)

### è¿è¡Œæ¨¡å‹

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ï¼š

```bash
ollama run qwen2.5_7b_Q4
```

æ˜¯çš„ï¼Œä¸€è¡Œå‘½ä»¤å°±å¯ä»¥ç›´æ¥è¿›è¡Œäº¤äº’ï¼š
![äº¤äº’](./assets/image-20241007104047095.png)

> åœ¨é¡¹ç›®çš„æ›´åæœŸï¼Œæ‰ä¼šè€ƒè™‘ `ollama` çš„è¯¦ç»†æ•™ç¨‹ï¼Œç°åœ¨ä¸“æ³¨äºä»£ç å±‚é¢ã€‚

## ç›¸å…³æ–‡ç« é˜…è¯»

[18. æ¨¡å‹é‡åŒ–æŠ€æœ¯æ¦‚è¿°åŠ GGUF & GGML æ–‡ä»¶æ ¼å¼è§£æ](./18.%20æ¨¡å‹é‡åŒ–æŠ€æœ¯æ¦‚è¿°åŠ%20GGUF%20%26%20GGML%20æ–‡ä»¶æ ¼å¼è§£æ.md#ç›´è§‚æ„Ÿå—-gguf-æ–‡ä»¶çš„æƒé‡)
