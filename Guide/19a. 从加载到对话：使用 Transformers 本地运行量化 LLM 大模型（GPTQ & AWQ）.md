# a. **ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨ Transformers æœ¬åœ°è¿è¡Œé‡åŒ– LLM å¤§æ¨¡å‹ï¼ˆGPTQ & AWQï¼‰**

> æ¥ç”¨ç‚¹ä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚
>
> LLM çš„åŠ è½½ã€å¾®è°ƒå’Œåº”ç”¨æ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œä»Šå¤©æˆ‘ä»¬å…ˆèšç„¦äº**åŠ è½½**ï¼Œæœ¬æ–‡çš„éš¾ç‚¹ä»…åœ¨äºæ­£ç¡®å®‰è£…å’ŒçŸ¥æ™“æ¨¡å‹é‡åŒ–çš„æ¦‚å¿µ :)ï¼Œæ‰€ä»¥ä¸ç”¨æ‹…å¿ƒï¼Œè¿™æ˜¯ä¸€ç¯‡è½»æ¾çš„æ–‡ç« ã€‚
>
> ä½ å¯èƒ½å·²ç»å¬è¯´è¿‡ä¸€äº›å·¥å…·ï¼Œæ¯”å¦‚ Ollama å’Œ GPT4ALLï¼Œè¿™äº›éƒ½æ˜¯å³å¼€å³ç”¨çš„ä¼˜ç§€å¼€æºé¡¹ç›®ã€‚ç„¶è€Œï¼Œä»…ä»…ä½¿ç”¨è¿™äº›å·¥å…·å¹¶ä¸èƒ½çœŸæ­£ç†è§£å®ƒä»¬æ˜¯æ€ä¹ˆåšçš„ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬æ›´åŠ æ·±å…¥ä¸€ç‚¹ï¼Œäº²è‡ªé€šè¿‡ä»£ç æ¥å®Œæˆ LLM éƒ¨ç½²å’Œç®€å•çš„å¯¹è¯äº¤äº’ã€‚
>
> åœ¨ä¹‹å‰çš„æ–‡ç« [ã€Š06. å¼€å§‹å®è·µï¼šéƒ¨ç½²ä½ çš„ç¬¬ä¸€ä¸ªLLMå¤§è¯­è¨€æ¨¡å‹ã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/06.%20å¼€å§‹å®è·µï¼šéƒ¨ç½²ä½ çš„ç¬¬ä¸€ä¸ªLLMå¤§è¯­è¨€æ¨¡å‹.md)ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„é¢„è®­ç»ƒæ¨¡å‹æ¥éƒ¨ç½²å¯¹è¯åº”ç”¨ï¼Œæ—¨åœ¨ä¸ºåˆå­¦è€…æä¾›ä¸€ä¸ªç®€å•çš„å…¥é—¨æ•™ç¨‹ã€‚ä½†å¦‚æœä½ æƒ³åŸºäºé‚£ç¯‡æ–‡ç« ï¼ŒçœŸæ­£éƒ¨ç½²æ›´æµè¡Œçš„å¤§æ¨¡å‹ï¼Œå¯èƒ½ä¼šé‡åˆ°ï¼š
>
> - **ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹**ï¼šOut of Memoryã€‚
> - **å¤§æ¨¡å‹ 4-bit é‡åŒ–å¯¼å…¥**ï¼šç¡®å®å¯ä»¥ï¼Œä½†ä¸‹è½½ä¸€ä¸ª 7B çš„å¤§æ¨¡å‹å†è½¬ 4-bitï¼Œéœ€è¦å…ˆåƒ 30GB çš„ç£ç›˜ç©ºé—´ï¼Œä¸å¤ªé€‚åˆéƒ¨ç½²å»â€œç©â€ã€‚
> - **æƒ³ä½¿ç”¨ GGUF æ–‡ä»¶**ï¼šæŸ¥é˜…æ–‡æ¡£å’Œæ”»ç•¥åï¼Œå°è¯•ç”¨ Transformers éƒ¨ç½²ï¼Œä½†å‘ç°æ¨ç†é€Ÿåº¦ææ…¢ï¼ˆè½¬æ¢æˆäº† FP32 å¹¶ä¸”è¿è¡Œåœ¨ CPU ä¸Šï¼‰ã€‚
> - ...
>
> è¿™ç¯‡æ–‡ç« ä¼šè§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ Transformers å’Œ Llama-cpp-python åˆ†åˆ«è¿›è¡Œæ¼”ç¤ºï¼Œå¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£å’ŒæŒæ¡ï¼Œæœ¬æ–‡å¯¹åº”äº Transformersï¼Œ[19b](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19b.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Llama-cpp-python%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.md) å¯¹åº”äº Llama-cpp-pythonã€‚
>
> æ³¨æ„ï¼šå½“å‰æ–‡ç« å¯¹æ˜¾å¡æ²¡æœ‰å¼ºåˆ¶è¦æ±‚ï¼Œæ— æ˜¾å¡ä¸€æ ·å¯ä»¥å®ç°ï¼ˆæ¨ç†ä½¿ç”¨ CPU ï¼‰ã€‚
>
> ä¸ä¹‹å‰çš„æ–‡ç« ä¸åŒæ˜¯ï¼Œä¸ºäº†ç›´è§‚æ„Ÿå—ï¼Œæœ¬æ–‡æ²¡æœ‰å¯¹ä»£ç è¿›è¡Œå‡½æ•°å°è£…ï¼Œè€Œæ˜¯åœ¨éœ€è¦æ—¶ç›´æ¥å¤ç”¨ã€‚
>
> [ä»£ç æ–‡ä»¶ä¸‹è½½ - Transformers](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/16a.%20ä½¿ç”¨%20Transformers%20åŠ è½½é‡åŒ–åçš„%20LLM%20å¤§æ¨¡å‹ï¼ˆGPTQ%20%26%20AWQï¼‰.ipynb)

## ç›®å½•

   - [å‰è¨€](#å‰è¨€)
   - [æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆæ¨èï¼‰](#æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ¨è)
     - [GPTQ](#gptq)
     - [AWQ](#awq)
     - [GGUF](#gguf)
     - [æ¨¡å‹æ‰€åœ¨åœ°](#æ¨¡å‹æ‰€åœ¨åœ°)
   - [ä½¿ç”¨ Transformers å¼€å§‹åŠ è½½](#ä½¿ç”¨-transformers-å¼€å§‹åŠ è½½)
     - [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
     - [GPTQ](#gptq-1)
       - [å®‰è£…](#å®‰è£…)
       - [å¯¼å…¥åº“](#å¯¼å…¥åº“)
       - [æœ¬åœ°å¯¼å…¥æ¨¡å‹](#æœ¬åœ°å¯¼å…¥æ¨¡å‹)
       - [è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹](#è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹)
       - [æ¨ç†æµ‹è¯•](#æ¨ç†æµ‹è¯•)
     - [AWQ](#awq-1)
       - [å®‰è£…](#å®‰è£…-1)
       - [å¯¼å…¥åº“](#å¯¼å…¥åº“-1)
       - [æœ¬åœ°å¯¼å…¥æ¨¡å‹](#æœ¬åœ°å¯¼å…¥æ¨¡å‹-1)
       - [è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹](#è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹-1)
       - [æ¨ç†æµ‹è¯•](#æ¨ç†æµ‹è¯•-1)
     - [äº†è§£æç¤ºè¯æ¨¡ç‰ˆï¼ˆPrompt Templateï¼‰](#äº†è§£æç¤ºè¯æ¨¡ç‰ˆprompt-template)
     - [æµå¼è¾“å‡º](#æµå¼è¾“å‡º)
     - [å•è½®å¯¹è¯](#å•è½®å¯¹è¯)
     - [å¤šè½®å¯¹è¯](#å¤šè½®å¯¹è¯)

## å‰è¨€

è®¿é—® [Hugging Face](https://huggingface.co/models)ï¼Œè®©æˆ‘ä»¬å…ˆé€‰æ‹©ä¸€ä¸ª7Bå·¦å³çš„è¯­è¨€æ¨¡å‹ï¼š

1. [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)
2. [Qwen/Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
3. [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)

> [!NOTE]
>
> ä½ å¯ä»¥éšæ„æ›´æ¢ä½ å–œæ¬¢çš„æ¨¡å‹ï¼Œä¸Šé¢åªæ˜¯ç®€å•åˆ—ä¸¾ã€‚

åœä¸€ä¸‹ï¼Œè¿˜è®°å¾— FP32 ä¸‹ 7B æ¨¡å‹çš„å‚æ•°æœ‰å¤šå¤§å—ï¼Ÿ

â€œä¸ä¸¥è°¨çš„è¯´ï¼Œå¥½åƒæ˜¯ 28 GBï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ç”¨æ¨¡å‹é‡åŒ–æ¥å¯¼å…¥æ¨¡å‹ï¼Œå°±æ˜¯å¤ªå¤§äº†ï¼Œå¯èƒ½è¦ä¸‹è½½æ¯”è¾ƒä¹…çš„æ—¶é—´:(â€

æ˜¯çš„ï¼Œè¿™ä¸ªè¯´æ³•æ²¡æœ‰é—®é¢˜ï¼Œä¸è¿‡ä¸Šé¢åˆ—å‡ºçš„æ¨¡å‹é‡‡ç”¨çš„æ˜¯ BF16ï¼Œæ‰€ä»¥è¿˜ä¼šæ›´å°ç‚¹ã€‚

â€œé‚£å¤§æ¦‚ 14 GBï¼Œæˆ‘æ™šç‚¹å†å¼€å§‹æ­£å¼å­¦ä¹ ï¼Œè¿˜æ˜¯è¦ä¸‹è½½å¾ˆä¹…â€

è¯¶ï¼Œé‚£ä½ æœ‰æ²¡æœ‰æƒ³è¿‡ï¼Œæ—¢ç„¶è¿™äº›æ¨¡å‹ä¸‹è½½ä¸‹æ¥éœ€è¦é‡åŒ–ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥å»ä¸‹ä¸€ä¸ªé‡åŒ–ç‰ˆçš„æ¨¡å‹ï¼Ÿ

æ˜¯çš„ï¼Œä»¥ä¸Šæ‰€åˆ—çš„ä¸‰ä¸ªæ¨¡å‹ï¼Œåœ¨ Hugging Face ä¸­éƒ½æœ‰ç€é‡åŒ–ç‰ˆæœ¬ï¼š

| GPTQ                                                         | AWQ                                                          | GGUF                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit](https://huggingface.co/neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit) | [solidrust/Mistral-7B-Instruct-v0.3-AWQ](https://huggingface.co/solidrust/Mistral-7B-Instruct-v0.3-AWQ) | [bartowski/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF) |
| [Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4) | [Qwen/Qwen2.5-7B-Instruct-AWQ](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-AWQ) | [Qwen/Qwen2.5-7B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF) |
| [hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4](https://huggingface.co/hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4) | [hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4](hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4) | [bartowski/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF) |

> [!note]
>
> æ³¨æ„ï¼Œè¡¨æ ¼ä¸­ GPTQ å’Œ AWQ çš„è·³è½¬é“¾æ¥å‡ä¸º 4-bit é‡åŒ–ã€‚
>
> **Qï¼šä¸ºä»€ä¹ˆ AWQ ä¸æ ‡æ³¨é‡åŒ–ç±»å‹ï¼Ÿ**
>
> Aï¼šå› ä¸º 3-bit æ²¡ä»€ä¹ˆéœ€æ±‚ï¼Œæ›´é«˜çš„ bit å®˜æ–¹ç°åœ¨è¿˜ä¸æ”¯æŒï¼ˆè§ [Issue #172](https://github.com/mit-han-lab/llm-awq/issues/172)ï¼‰ï¼Œæ‰€ä»¥åˆ†äº«çš„ AWQ æ–‡ä»¶åŸºæœ¬é»˜è®¤æ˜¯ 4-bitã€‚
>
> **Qï¼šGPTQï¼ŒAWQï¼ŒGGUF æ˜¯ä»€ä¹ˆï¼Ÿ**
>
> Aï¼šç®€å•äº†è§£è§ [18. æ¨¡å‹é‡åŒ–æŠ€æœ¯æ¦‚è¿°åŠ GGUF:GGML æ–‡ä»¶æ ¼å¼è§£æ](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/18.%20æ¨¡å‹é‡åŒ–æŠ€æœ¯æ¦‚è¿°åŠ%20GGUF%3AGGML%20æ–‡ä»¶æ ¼å¼è§£æ.md)ã€‚
>
> **Qï¼šæ€ä¹ˆå»æ‰¾å…¶ä»–æ¨¡å‹å¯¹åº”çš„é‡åŒ–ç‰ˆæœ¬ï¼Ÿ**
>
> Aï¼šå‡è®¾ä½ è¦æ‰¾çš„æ˜¯ 4 bit é‡åŒ–ï¼Œæœç´¢ `[æ¨¡å‹åç§°]-[GPTQ]/[AWQ]/[GGUF]` æˆ– `[æ¨¡å‹åç§°]-[4bit/INT4]`ã€‚

ä¸‰ç§é‡åŒ–æ¨¡å‹ï¼Œè¯¥é€‰å“ªä¸ªè¿›è¡Œæ¼”ç¤ºå‘¢ï¼Ÿé€‰æ‹©å›°éš¾ç—‡çŠ¯äº† :)

**ç´¢æ€§ä¸é€‰äº†ï¼Œæ¥ä¸‹æ¥å°†é€ä¸€ä»‹ç» GPTQï¼ŒAWQ ä»¥åŠ GGUF çš„åŠ è½½æ–¹å¼ï¼Œè¯»è€…é€‰æ‹©ä¸€ç§è¿›è¡Œå³å¯ã€‚**

> [!important]
>
> æ–‡ç« æ¼”ç¤ºçš„æ¨¡å‹ä¸º **Mistral-7B-Instruct-v0.3**ï¼Œå¯¹åº”çš„åŠ è½½æ–¹æ³•ï¼š
>
> a. **Transformers**ï¼šGPTQï¼ŒAWQ
>
> b. **LLama-cpp-python**ï¼šGGUF
>
> Llama-cpp-python çš„ä½¿ç”¨å°†ä½äºæ–‡ç«  [19b](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19b.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Llama-cpp-python%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.md)ï¼Œå»ºè®®åœ¨é˜…è¯»å®Œæ¨¡å‹ä¸‹è½½åå†è¿›è¡Œè·³è½¬ã€‚
>
> é»˜è®¤è¯»è€…åªé€‰å–ä¸€ç§æ–¹å¼è¿›è¡Œé˜…è¯»ï¼Œæ‰€ä»¥ä¸ºäº†é˜²æ­¢ç¼ºæ¼ï¼Œæ¯ä¸ªç±»å‹ä¼šé‡å¤æ¨¡å‹ä¸‹è½½çš„æŒ‡å¼•ï¼Œå½“ç„¶ï¼Œä½ å®Œå…¨å¯ä»¥é€‰æ‹©å…¨çœ‹ã€‚

## æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆæ¨èï¼‰

æ¥è¯•è¯•å¤šçº¿ç¨‹æŒ‡å®šæ–‡ä»¶ä¸‹è½½ï¼Œå¯¹äº Linuxï¼Œè¿™é‡Œç»™å‡ºé…ç½®å‘½ä»¤ï¼Œå…¶ä½™ç³»ç»Ÿå¯ä»¥å‚ç…§[ã€Ša. ä½¿ç”¨ HFD åŠ å¿« Hugging Face æ¨¡å‹å’Œæ•°æ®é›†çš„ä¸‹è½½ã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/a.%20ä½¿ç”¨%20HFD%20åŠ å¿«%20Hugging%20Face%20æ¨¡å‹å’Œæ•°æ®é›†çš„ä¸‹è½½.md)å…ˆè¿›è¡Œç¯å¢ƒé…ç½®ã€‚ä½ ä¹Ÿå¯ä»¥è·³è¿‡è¿™éƒ¨åˆ†ï¼Œåé¢ä¼šä»‹ç»è‡ªåŠ¨ä¸‹è½½ã€‚

```bash
sudo apt-get update
sudo apt-get install git git-lfs wget aria2
git lfs install
```

ä¸‹è½½å¹¶é…ç½® HFD è„šæœ¬ï¼š

```bash
wget https://huggingface.co/hfd/hfd.sh
chmod a+x hfd.sh
export HF_ENDPOINT=https://hf-mirror.com
```

ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½æŒ‡å®šæ¨¡å‹ã€‚

### GPTQ

å‘½ä»¤éµå¾ª `./hfd.sh <model_path> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š

```bash
./hfd.sh neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit --tool aria2c -x 16
```

### AWQ

å‘½ä»¤éµå¾ª `./hfd.sh <model_path> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š

```bash
./hfd.sh solidrust/Mistral-7B-Instruct-v0.3-AWQ --tool aria2c -x 16
```

### GGUF

ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½æŒ‡å®šæ¨¡å‹ï¼Œå‘½ä»¤éµå¾ª `./hfd.sh <model_path> --include <file_name> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š

```bash
./hfd.sh bartowski/Mistral-7B-Instruct-v0.3-GGUF --include Mistral-7B-Instruct-v0.3-Q4_K_M.gguf --tool aria2c -x 16
```

ä¸‹è½½å®Œæˆä½ åº”è¯¥å¯ä»¥çœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```
Download Results:
gid   |stat|avg speed  |path/URI
======+====+===========+=======================================================
145eba|OK  |   6.8MiB/s|./Mistral-7B-Instruct-v0.3-Q4_K_M.gguf

Status Legend:
(OK):download completed.
Downloaded https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf successfully.
Download completed successfully.
```

### æ¨¡å‹æ‰€åœ¨åœ°

é»˜è®¤åœ¨å½“å‰ç›®å½•ä¸‹çš„ `<model_name>` æ–‡ä»¶å¤¹ä¸­ï¼ˆ`<model_path>` ç”± `id` + `/` + `model_name` ç»„æˆï¼‰

```
3.9G    ./Mistral-7B-Instruct-v0.3-AWQ
3.9G    ./Mistral-7B-Instruct-v0.3-GPTQ-4bit
4.1G    ./Mistral-7B-Instruct-v0.3-GGUF
```

## ä½¿ç”¨ Transformers å¼€å§‹åŠ è½½

### ç¯å¢ƒé…ç½®

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ç¯å¢ƒå·²æ­£ç¡®é…ç½®ã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€éœ€çš„åº“ï¼š

```python
pip install numpy==1.24.4
pip install --upgrade transformers
```

### GPTQ

#### å®‰è£…

ä½ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœå®‰è£…ä¸æ­£ç¡®ï¼ŒGPTQ å°†æ— æ³•æ­£ç¡®ä½¿ç”¨ GPU è¿›è¡Œæ¨ç†ï¼Œä¹Ÿå°±æ˜¯è¯´æ— æ³•è¿›è¡ŒåŠ é€Ÿï¼Œå³ä¾¿ print(model.device) æ˜¾ç¤ºä¸º "cuda"ã€‚ç±»ä¼¼çš„é—®é¢˜è§ [Is This Inference Speed Slow?  #130](https://github.com/AutoGPTQ/AutoGPTQ/issues/130) å’Œ [CUDA extension not installed #694](https://github.com/AutoGPTQ/AutoGPTQ/issues/694)ã€‚

è¿™ä¸ªé—®é¢˜æ˜¯æ™®éå­˜åœ¨çš„ï¼Œå½“ä½ ç›´æ¥ä½¿ç”¨ `pip install auto-gptq` è¿›è¡Œå®‰è£…æ—¶ï¼Œå¯èƒ½å°±ä¼šå‡ºç°ã€‚

ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥å·²å®‰è£…çš„ç‰ˆæœ¬ï¼š

```bash
pip list | grep auto-gptq
```

å¦‚æœå‘ç°ä¹‹å‰å®‰è£…çš„ç‰ˆæœ¬ä¸å¸¦ cuda æ ‡è¯†ï¼Œå¸è½½å®ƒï¼Œä»æºç é‡æ–°è¿›è¡Œå®‰è£…ï¼ˆæ¨ç†é€Ÿåº¦å°†æå‡ä¸ºåŸæ¥çš„ 15 å€ä»¥ä¸Šï¼‰ã€‚

```bash
pip uninstall auto-gptq
git clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ
```

```bash
# ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä»»é€‰ä¸€ç§è¿›è¡Œå®‰è£…å³å¯ï¼Œç»æµ‹è¯•å‡æœ‰æ•ˆ
pip install -vvv --no-build-isolation -e .
# >> Successfully installed auto-gptq-0.8.0.dev0+cu121

python setup.py install
# >> Finished processing dependencies for auto-gptq==0.8.0.dev0+cu121
```

> [!CAUTION]
>
> è¯·ç¡®ä¿ç³»ç»Ÿå·²å®‰è£… CUDAï¼Œå¯ä»¥é€šè¿‡ `nvcc --version` æ£€æŸ¥ã€‚

#### å¯¼å…¥åº“

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
```

æ¥ä¸‹æ¥ä»‹ç»ä¸¤ç§å¯¼å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œæ ¹æ®éœ€è¦é€‰æ‹©æœ¬åœ°å¯¼å…¥æˆ–è‡ªåŠ¨ä¸‹è½½å¯¼å…¥ã€‚

#### æœ¬åœ°å¯¼å…¥æ¨¡å‹

å¦‚æœå·²ç»åœ¨æœ¬åœ°ä¸‹è½½äº†æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ¨¡å‹è·¯å¾„æ¥åŠ è½½æ¨¡å‹ã€‚ä»¥ä¸‹ç¤ºä¾‹å‡è®¾æ¨¡å‹ä½äºå½“å‰ç›®å½•çš„ `Mistral-7B-Instruct-v0.3-GPTQ-4bit` æ–‡ä»¶å¤¹ä¸‹ï¼š

```python
# æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„
model_path = "./Mistral-7B-Instruct-v0.3-GPTQ-4bit"

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_path)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype="auto",  # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹
    device_map="auto"    # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„è®¾å¤‡ï¼ˆCPU/GPUï¼‰
)
```

#### è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹

å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ `from_pretrained` æ–¹æ³•ä» Hugging Face Hub è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼š

```python
# æŒ‡å®šæ¨¡å‹çš„åç§°
model_name = "neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit"

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",  # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹
    device_map="auto"    # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„è®¾å¤‡ï¼ˆCPU/GPUï¼‰
)
```

#### æ¨ç†æµ‹è¯•

```python
# è¾“å…¥æ–‡æœ¬
input_text = "Hello, World!"

# å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼
input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

# ç”Ÿæˆè¾“å‡º
with torch.no_grad():
    output_ids = model.generate(
        input_ids=input_ids,
        max_length=50,
    )

# è§£ç ç”Ÿæˆçš„è¾“å‡º
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
print(output_text)
```

### AWQ

#### å®‰è£…

```bash
pip install autoawq autoawq-kernels
```

#### å¯¼å…¥åº“

```python
import torch
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer
```

åŒæ ·åœ°ï¼Œä¸‹é¢ä»‹ç»ä¸¤ç§å¯¼å…¥ AWQ æ¨¡å‹çš„æ–¹æ³•ï¼Œæ ¹æ®éœ€è¦é€‰æ‹©æœ¬åœ°å¯¼å…¥æˆ–è‡ªåŠ¨ä¸‹è½½å¯¼å…¥ã€‚

#### æœ¬åœ°å¯¼å…¥æ¨¡å‹

å¦‚æœå·²ç»ä¸‹è½½äº† AWQ æ ¼å¼çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½ï¼š

```python
# æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„
model_path = "./Mistral-7B-Instruct-v0.3-AWQ"

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

# åŠ è½½æ¨¡å‹
model = AutoAWQForCausalLM.from_quantized(
    model_path,
    fuse_layers=True  # èåˆéƒ¨åˆ†æ¨¡å‹å±‚ä»¥æé«˜æ¨ç†é€Ÿåº¦
)
```

#### è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹

å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå¯ä»¥ä» Hugging Face Hub è‡ªåŠ¨ä¸‹è½½ï¼š

```python
# æŒ‡å®šæ¨¡å‹çš„åç§°
model_name = "solidrust/Mistral-7B-Instruct-v0.3-AWQ"

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
model = AutoAWQForCausalLM.from_quantized(
    model_name,
    fuse_layers=True  # èåˆéƒ¨åˆ†æ¨¡å‹å±‚ä»¥æé«˜æ¨ç†é€Ÿåº¦
)
```

#### æ¨ç†æµ‹è¯•

```python
# è®¾ç½®è®¾å¤‡
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# è¾“å…¥æ–‡æœ¬
input_text = "Hello, World!"

# å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼
input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

# ç”Ÿæˆè¾“å‡º
with torch.no_grad():
    output_ids = model.generate(
        input_ids=input_ids,
        max_length=50,
        pad_token_id=tokenizer.eos_token_id
    )

# è§£ç ç”Ÿæˆçš„è¾“å‡º
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
print(output_text)
```

### äº†è§£æç¤ºè¯æ¨¡ç‰ˆï¼ˆPrompt Templateï¼‰

å…¶å®éå¸¸ç®€å•ï¼Œå°±æ˜¯æ›¾ç»æåˆ°çš„å ä½ç¬¦ï¼ˆä¸‹å›¾å¯¹äº `{{question}}` çš„åº”ç”¨ï¼‰ã€‚

![å ä½ç¬¦](./assets/%E5%8D%A0%E4%BD%8D%E7%AC%A6-6055722.png)

ä¸¾ä¸ªç›´è§‚çš„ä¾‹å­ï¼š

```python
# å®šä¹‰ Prompt Template
prompt_template = "é—®ï¼š{question}\nç­”ï¼š"

# å®šä¹‰é—®é¢˜
question = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ"

# ä½¿ç”¨ Prompt Template ç”Ÿæˆå®Œæ•´çš„æç¤º
prompt = prompt_template.format(question=question)
print(prompt)
```

**è¾“å‡º**ï¼š

```
é—®ï¼šäººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ
ç­”ï¼š
```

> [!tip]
>
> å¦‚æœä½ å¯¹ `"é—®ï¼š{question}\nç­”ï¼š".format()` çš„å½¢å¼ä¸å¤ªç†è§£ï¼Œå¯ä»¥å°†å…¶ç†è§£ä¸º `f"é—®ï¼š{question}\nç­”ï¼š"`ï¼š
>
> ```python
> print(f"é—®ï¼š{question}\nç­”ï¼š")
> ```
>
> **è¾“å‡º**ï¼š
>
> ```
> é—®ï¼šäººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ
> ç­”ï¼š
> ```
>
> åœ¨[ã€Š02. ç®€å•å…¥é—¨ï¼šé€šè¿‡APIä¸Gradioæ„å»ºAIåº”ç”¨ã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/02.%20ç®€å•å…¥é—¨ï¼šé€šè¿‡APIä¸Gradioæ„å»ºAIåº”ç”¨.md#ç¬¬ä¸€éƒ¨åˆ†æ–‡ç« æ‘˜è¦å•è½®å¯¹è¯åº”ç”¨)çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œä½ åº”è¯¥è§è¿‡å®ƒçš„ä½¿ç”¨ã€‚
>
> **Q: åº”è¯¥é€‰æ‹©å“ªç§å½¢å¼çš„ Prompt Template å‘¢ï¼Ÿ**
>
> A: æŒ‰ç…§è‡ªå·±çš„å–œå¥½è¿›è¡Œä½¿ç”¨ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ f-string åœ¨ Python 3.6 ç‰ˆæœ¬æ‰å¼•å…¥ã€‚
>
> **Q: æç¤ºè¯æ¨¡ç‰ˆçš„åº”ç”¨åœ¨å“ªé‡Œï¼Œä¸ºä»€ä¹ˆèƒ½å¾—åˆ°å‘å±•ï¼Ÿ**
>
> Aï¼šå·æ‡’ã€‚
>
> è®²ä¸ªå°æ•…äº‹ã€‚å‡è®¾å›¢é˜Ÿæ­£åœ¨è¿›è¡Œå¤§é‡çš„æ–‡çŒ®ç¿»è¯‘ä»»åŠ¡ã€‚æ—©åœ¨ GPT-3.5 åˆšå‘å¸ƒæ—¶ï¼Œæˆ‘ä»¬å°±å¼€å§‹ä½¿ç”¨ ChatGPT æ¥å¸®å¿™ã€Œç¿»è¯‘ã€æ–‡ç« ï¼Œä¸ºè‡ªå·±çš„æ‘¸é±¼å¤§ä¸šæ·»ç –åŠ ç“¦ã€‚è™½ç„¶æ¯æ¬¡éƒ½éœ€è¦è¾“å…¥å¤§é‡çš„æ–‡æœ¬æ‰èƒ½è®©å®ƒè¿›è¡Œåœ°é“çš„ç¿»è¯‘ï¼Œä½†è¿™æ€»æ¯”è‡ªå·±é€å¥ç¿»è¯‘çœåŠ›å¾—å¤šã€‚ã€Œå·¥èµ„ä¸å˜ï¼Œååé‡å¢åŠ ï¼ŒåŠ é‡ä¸åŠ ä»·ï¼Œç›´å‘¼å¥½å‘˜å·¥ã€
>
> ç»è¿‡ä¸€æ®µæ—¶é—´ï¼Œæˆ‘ä»¬æ€»ç»“å‡ºäº†ä¸€å¥—ã€Œæ¨¡ç‰ˆã€ï¼Œæ¯æ¬¡åªéœ€è¦å¤åˆ¶ç²˜è´´ï¼šå°†ã€Œæ¨¡ç‰ˆã€+ã€Œæ–‡æœ¬ã€ä¸¢ç»™ChatGPT å°±èƒ½å¾—åˆ°ç»“æœï¼ˆæˆ–è®¸å°ç»„å·²ç»æœ‰äººåœ¨å·å·ç”¨ API å†™æç¤ºè¯æ¨¡ç‰ˆğŸ¤£ï¼‰ã€‚ã€Œè™½ç„¶é‡å¤å•è°ƒæ— è¶£ï¼Œä½†æ˜¯è½»æ¾è¿˜é«˜æ•ˆã€
>
> çŸœçŸœä¸šä¸šçš„å½“äº†ä¸€å¹´ CV æœºå™¨äººï¼ˆ`Ctrl`+`Cã€Œæ¨¡ç‰ˆã€` `Vã€Œæ¨¡ç‰ˆã€` + `Ctrl`+`Cã€Œæ–‡æœ¬ã€` `Vã€Œæ–‡æœ¬ã€` ï¼‰ï¼ŒOpenAI å‘å¸ƒäº† GPTsï¼Œè¿™å¯å¤ªå¥½äº†ï¼æˆ‘ä»¬å¯ä»¥å®šåˆ¶ GPT æ¥åšä»»åŠ¡ï¼Œå°†åŸæ¥æ€»ç»“å‡ºçš„ã€Œæ¨¡ç‰ˆã€ç›´æ¥ä¸¢è¿›å»å°±è¡Œï¼šä¸€ä¸ª GPT è´Ÿè´£ã€Œåˆ†æ®µã€ï¼Œå¦ä¸€ä¸ªè´Ÿè´£ã€Œç¿»è¯‘ã€ï¼Œå†åŠ ä¸€ä¸ªè´Ÿè´£ã€Œæ¶¦è‰²ã€ï¼ã€Œå†ä¹Ÿä¸ç”¨å¤åˆ¶é‡å¤çš„ã€Œæ¨¡ç‰ˆã€äº†ã€
>
> **å›å½’æ­£é¢˜**ï¼Œåœ¨è§’è‰²æ‰®æ¼”æˆ–å…¶ä»– AI åº”ç”¨ä¸­ï¼Œä½¿ç”¨ Prompt è¿›è¡Œé¢„è®¾æ˜¯å¾ˆå¸¸è§çš„æ–¹æ³•ï¼ˆå› ä¸º In-context Learning æ˜¯æœ‰æ•ˆçš„ï¼‰ã€‚è€Œç”¨å¾—å¤šäº†ï¼Œè‡ªç„¶éœ€è¦ä¸€ä¸ªåˆé€‚çš„åç§°æ¥æŒ‡ä»£å®ƒï¼ŒPrompt Template ç¡®å®å¾ˆè´´åˆ‡ã€‚

### æµå¼è¾“å‡º

åœ¨é¡¹ç›®åˆæœŸè®¤è¯† API çš„æ—¶å€™ï¼Œæ–‡ç« [ã€Š01. åˆè¯†LLM APIï¼šç¯å¢ƒé…ç½®ä¸å¤šè½®å¯¹è¯æ¼”ç¤ºã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/01.%20åˆè¯†LLM%20APIï¼šç¯å¢ƒé…ç½®ä¸å¤šè½®å¯¹è¯æ¼”ç¤º.md#æµå¼è¾“å‡º)æœ‰æåˆ°è¿‡æµå¼è¾“å‡ºï¼Œè¿™æ˜¯æˆ‘ä»¬ä¸€ç›´ä»¥æ¥è§åˆ°çš„å¤§æ¨¡å‹è¾“å‡ºæ–¹å¼ï¼šé€å­—ï¼ˆtokenï¼‰æ‰“å°è€Œéç­‰å…¨éƒ¨ç”Ÿæˆå®Œå†æ‰“å°ã€‚

æ‰§è¡Œä¸‹é¢çš„ä»£ç è¯•è¯•ï¼ˆæ— è®ºä¹‹å‰å¯¼å…¥çš„æ˜¯å“ªç§æ¨¡å‹ï¼Œéƒ½å¯ä»¥ç»§ç»­ï¼‰ï¼š

```python
from transformers import TextStreamer

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# åˆ›å»º TextStreamer å®ä¾‹
streamer = TextStreamer(
    tokenizer, 
    skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬
    skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰
)

# å°†æç¤ºç¼–ç ä¸ºæ¨¡å‹è¾“å…¥
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

# è®¾ç½®ç”Ÿæˆå‚æ•°
generation_kwargs = {
    "input_ids": input_ids,  # æ¨¡å‹çš„è¾“å…¥ IDï¼Œæ³¨æ„ï¼Œè¿™ä¸æ˜¯ Embedding
    "max_length": 200,       # ç”Ÿæˆçš„æœ€å¤§ token æ•°
    "streamer": streamer,    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬
    "pad_token_id": tokenizer.eos_token_id  # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤è­¦å‘Š
}

# å¼€å§‹ç”Ÿæˆæ–‡æœ¬
with torch.no_grad():
    model.generate(**generation_kwargs)
```

> [!note]
>
> `**` æ˜¯ Python ä¸­çš„è§£åŒ…æ“ä½œç¬¦ï¼Œå®ƒå°†å­—å…¸ä¸­çš„é”®å€¼å¯¹è§£åŒ…ä¸ºå‡½æ•°çš„å…³é”®å­—å‚æ•°ã€‚åœ¨è¿™é‡Œï¼Œ`**generation_kwargs` å°†å­—å…¸ä¸­çš„å‚æ•°é€ä¸€ä¼ é€’ç»™ `model.generate()` æ–¹æ³•ï¼Œç­‰ä»·äºç›´æ¥å†™å‡ºæ‰€æœ‰å‚æ•°ï¼š
>
> ```python
> model.generate(
>  input_ids=input_ids, 
>  max_length=200, 
>  ...
> )
> ```
>
> ä½ éœ€è¦æ³¨æ„åˆ°ï¼Œè¿™å’Œä¹‹å‰é‡‡ç”¨äº†ä¸åŒçš„ä¼ å‚æ–¹å¼ï¼Œä½†æœ¬è´¨æ˜¯ä¸€æ ·çš„ã€‚ä¸ºäº†åˆè§çš„ç›´è§‚ï¼Œåœ¨åç»­çš„æ•™ç¨‹ä¸­ï¼Œä¼šè¾ƒå°‘åœ°ä½¿ç”¨è¿™ç§æ–¹å¼è¿›è¡Œä¼ å‚ã€‚
>
> **Qï¼šä¸ºä»€ä¹ˆè®¾ç½® `pad_token_id=tokenizer.eos_token_id`ï¼Ÿ**
>
> Aï¼šå¦‚æœä¸è¿›è¡Œè®¾ç½®ï¼Œä½ å°†åœ¨æ¯æ¬¡ç”Ÿæˆæ—¶çœ‹åˆ°è¿™æ ·çš„ä¸€è¡Œè­¦å‘Šï¼š
>
> ```
> Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.
> ```
>
> è®©æˆ‘ä»¬çœ‹çœ‹[æºç ](https://github.com/huggingface/transformers/blob/b880508440f43f80e35a78ccd2a32f3bde91cb23/src/transformers/generation_utils.py#L410-L414)ï¼š
>
> ```python
> def _get_pad_token_id(self, pad_token_id: int = None, eos_token_id: int = None) -> int:
>     if pad_token_id is None and eos_token_id is not None:
>         logger.warning(f"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.")
>         pad_token_id = eos_token_id
>     return pad_token_id
> ```
>
> å½“ `pad_token_id` ä¸º None è€Œ `eos_token_id` ä¸ä¸º None æ—¶ï¼Œå¼¹å‡ºè¿™ä¸ªè­¦å‘Šï¼Œå¹¶ä¸”è®¾ç½® pad_token_id = eos_token_idã€‚
>
> è¿™é‡Œæå‰è¿›è¡Œè®¾ç½®åªæ˜¯ä¸ºäº†æŠ‘åˆ¶è¿™ä¸ªè­¦å‘Šï¼Œå®é™…æ•ˆæœå’Œä¸è®¾ç½®çš„é»˜è®¤è¡Œä¸ºä¸€æ ·ã€‚
>
> åŒæ ·çš„é—®é¢˜è§ï¼šhttps://stackoverflow.com/a/71397707/20445396

**è¾“å‡º**ï¼š

![](./assets/Video_24-10-09_18-23-27-ezgif.com-video-to-gif-converter.gif)

ç°åœ¨ï¼Œä½ å¯ä»¥æ ¹æ®ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å†³å®šæ˜¯å¦ç»ˆæ­¢è¾“å‡ºã€‚

å¦å¤–ï¼Œå½“å‰è¾“å‡ºä¸­ä¸åŒ…å«æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ Prompt æ¨¡æ¿ã€‚å¦‚æœå¸Œæœ›åŒ…å«å®ƒï¼Œå°† `streamer` å‚æ•°ä¸­çš„ `skip_prompt` è®¾ç½®ä¸º `False` å³å¯ã€‚

> [!tip]
>
> æœ€è¿‘ç¤ºå‡ºçš„ GPT-4o canvas åœ¨å¯¹åŸå›ç­”è¿›è¡Œä¿®æ”¹æ—¶å±•ç¤ºå‡ºçš„æŒ‰è¡Œåˆ·æ–°åŸæ–‡æ¯”é€ Token æ›´èµå¿ƒæ‚¦ç›®ï¼Œè®¾è®¡çš„å¾ˆå¥½ï¼ˆä¸è¿‡ä¸€ç•ªä½“éªŒä¸‹æ¥ï¼Œç°åœ¨è¿˜å¤„äºå¥½ç©æœ‰æ„æ€ï¼Œä½†ä¸å¤Ÿç”¨çš„é˜¶æ®µï¼‰ã€‚
>
> ![image-20241009180422603](./assets/image-20241009180422603.png)

### å•è½®å¯¹è¯

è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹è‡ªèº«å¯¹åº”çš„èŠå¤©æ¨¡ç‰ˆã€‚

```python
# æ‰“å° chat_template ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
if hasattr(tokenizer, 'chat_template'):
    print(tokenizer.chat_template)
else:
    print("Tokenizer æ²¡æœ‰ 'chat_template' å±æ€§ã€‚")
```

**è¾“å‡º**ï¼š

```
{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}
```

ä½ åº”è¯¥èƒ½å‘ç°ï¼Œè¿™å…¶ä¸­æœ‰å¾ˆå¤šéå¸¸ç†Ÿæ‚‰çš„è¯ï¼š`messages`ã€`role`ã€`user` å’Œ `assistant`ï¼ˆåœ¨[æœ€æ—©](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/02.%20ç®€å•å…¥é—¨ï¼šé€šè¿‡APIä¸Gradioæ„å»ºAIåº”ç”¨.md#ç¬¬ä¸€éƒ¨åˆ†æ–‡ç« æ‘˜è¦å•è½®å¯¹è¯åº”ç”¨)è°ƒç”¨ API æ—¶å…¶å®è§åˆ°è¿‡ï¼‰ï¼Œä½†ä»Šå¤©ä¸å»äº†è§£å…¶ä¸­çš„ç»†èŠ‚ï¼ˆä¹‹åå°†è¯¦ç»†è®²è§£ï¼‰ï¼Œè®©æˆ‘ä»¬ç›´æ¥æ¥è®¾è®¡ `messages` å®ç°åˆšåˆšçš„æµå¼è¾“å‡ºï¼š

```python
prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ"

# å®šä¹‰æ¶ˆæ¯åˆ—è¡¨
messages = [
    {"role": "user", "content": prompt}
]

# ä½¿ç”¨ tokenizer.apply_chat_template() ç”Ÿæˆæ¨¡å‹è¾“å…¥
input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)

# ========== ä»¥ä¸‹ä»£ç ä¸ä¹‹å‰ä¸€è‡´ ==============
# åˆ›å»º TextStreamer å®ä¾‹
streamer = TextStreamer(
    tokenizer, 
    skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬
    skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰
)

# è®¾ç½®ç”Ÿæˆå‚æ•°
generation_kwargs = {
    "input_ids": input_ids,          # æ¨¡å‹çš„è¾“å…¥ ID
    "max_length": 200,               # ç”Ÿæˆçš„æœ€å¤§ token æ•°
    "streamer": streamer,            # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬
    "pad_token_id": tokenizer.eos_token_id  # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤è­¦å‘Š
}

# å¼€å§‹ç”Ÿæˆæ–‡æœ¬
with torch.no_grad():
    model.generate(**generation_kwargs)
```

å¦‚æœæƒ³è¿›è¡Œå•è½®å¯¹è¯ï¼Œé‚£ä¹ˆä¿®æ”¹ `prompt` å³å¯ï¼š

```python
prompt = input("User: ")
```

### å¤šè½®å¯¹è¯

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬é‡ç°æ›¾ç»è§è¿‡çš„å¤šè½®å¯¹è¯ï¼š

```python
from transformers import TextStreamer

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# åˆå§‹åŒ–å¯¹è¯å†å²
messages = []

# å¼€å§‹å¤šè½®å¯¹è¯
while True:
    # è·å–è¾“å…¥
    prompt = input("User: ")
    
    # é€€å‡ºå¯¹è¯æ¡ä»¶ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ç»ˆæ­¢ä»£ç å—ï¼‰
    if prompt.lower() in ["exit", "quit", "bye"]:
        print("Goodbye!")
        break
    
    # å°†è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²
    messages.append({"role": "user", "content": prompt})
    
    # ä½¿ç”¨ tokenizer.apply_chat_template() ç”Ÿæˆæ¨¡å‹è¾“å…¥
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)
    
    # åˆ›å»º TextStreamer å®ä¾‹
    streamer = TextStreamer(
        tokenizer, 
        skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬
        skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰
    )
    
    # è®¾ç½®ç”Ÿæˆå‚æ•°
    generation_kwargs = {
        "input_ids": input_ids,                  # æ¨¡å‹çš„è¾“å…¥ ID
        "max_length": input_ids.shape[1] + 500,  # ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼Œinput_ids.shape[1] å³è¾“å…¥å¯¹åº”çš„ tokens æ•°é‡
        "streamer": streamer,                    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬
        "pad_token_id": tokenizer.eos_token_id   # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤è­¦å‘Š
    }
    
    # å¼€å§‹ç”Ÿæˆå›å¤
    with torch.no_grad():
        output_ids = model.generate(**generation_kwargs)
    
    # è·å–ç”Ÿæˆçš„å›å¤æ–‡æœ¬
    assistant_reply = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    # å°†æ¨¡å‹çš„å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²
    messages.append({"role": "assistant", "content": assistant_reply})
```

**è¾“å‡º**ï¼š

```
User:  å¦‚æœä½ æ˜¯å¤§æ¨¡å‹é¢è¯•å®˜ï¼Œä½ ä¼šæ€ä¹ˆå‡ºé¢è¯•é¢˜
å¦‚æœæˆ‘æ˜¯å¤§æ¨¡å‹é¢è¯•å®˜ï¼Œæˆ‘ä¼šå‡ºé¢è¯•é¢˜å¦‚ä¸‹ï¼š

1. ä½ èƒ½è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ
2. ä½ èƒ½ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Ÿ
3. ä½ èƒ½è§£é‡Šä»€ä¹ˆæ˜¯å Propagation ç®—æ³•ï¼Ÿ
4. ä½ èƒ½ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä»€ä¹ˆæ˜¯è‡ªç¼–ç å™¨ï¼ˆAutoencoderï¼‰ï¼Ÿ
5. ä½ èƒ½è§£é‡Šä»€ä¹ˆæ˜¯ Transfer Learningï¼Ÿ
6. ä½ èƒ½ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä»€ä¹ˆæ˜¯ Generative Adversarial Networksï¼ˆGANsï¼‰ï¼Ÿ
7. ä½ èƒ½è§£é‡Šä»€ä¹ˆæ˜¯ Reinforcement Learningï¼Ÿ
8. ä½ èƒ½ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä»€ä¹ˆæ˜¯ Neural Turing Machinesï¼ˆNTMsï¼‰ï¼Ÿ
9. ä½ èƒ½è§£é‡Šä»€ä¹ˆæ˜¯ One-Shot Learningï¼Ÿ
10. ä½ èƒ½ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä»€ä¹ˆæ˜¯ Siamese Networksï¼Ÿ
User:  å¯¹äºç¬¬åä¸ªé—®é¢˜èƒ½å¦ç»™æˆ‘ç­”æ¡ˆ
å¯¹äºç¬¬åä¸ªé—®é¢˜ï¼šç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä»€ä¹ˆæ˜¯ Siamese Networksï¼Ÿ

Siamese Networks æ˜¯ä¸€ç§åŒå‘çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºå­¦ä¹ ä¸¤ä¸ªç›¸ä¼¼çš„è¾“å…¥çš„ç›¸ä¼¼æ€§ã€‚

ä¾‹å¦‚ï¼Œåœ¨äººè„¸è¯†åˆ«ä¸­ï¼ŒSiamese Networks å¯ä»¥ç”¨æ¥å­¦ä¹ ä¸¤ä¸ªäººè„¸çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œå®ç°äººè„¸è¯†åˆ«çš„ç›®çš„ã€‚
```

å¯ä»¥çœ‹åˆ°æ¨¡å‹æ‹¥æœ‰ä¹‹å‰çš„å¯¹è¯â€œè®°å¿†â€ã€‚

å¦‚æœä½ å¯¹ GGUF æ–‡ä»¶çš„åŠ è½½æˆ–è€… Llama-cpp-python çš„ä½¿ç”¨æ„Ÿå…´è¶£ï¼Œç»§ç»­é˜…è¯» [19b](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19b.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Llama-cpp-python%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.md)ã€‚
