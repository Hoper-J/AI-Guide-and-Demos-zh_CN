# Inseq 特征归因：可视化解释 LLM 的输出

> **Feature Attribution（特征归因）**：你可以将其当做对模型输出的解释，就像在图像分类中可视化模型关注的区域一样。
>
> 本文将介绍 **Inseq**，这是一个用于解释和可视化序列生成模型输出的工具。我们将通过翻译任务（关注整个序列）和文本生成任务（关注前面的词）来演示如何使用 Inseq 来了解输入文本的哪些部分对模型生成下一个单词的影响最大。
>
> 这篇文章也为[生成式人工智能导论](https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php)课程中 [HW7: Understand what Generative AI is thinking](https://colab.research.google.com/drive/1Xnz0GHC0yWO2Do0aAYBCq9zL45lbiRjM?usp=sharing#scrollTo=UFOUfh2k1jFN) 提供中文引导。
>
> [代码文件下载](../Demos/10.%20Inseq：可视化解释%20LLM%20的输出.ipynb) | [作业PDF](../GenAI_PDF/HW7.pdf)
>
> 在线链接：[Kaggle](https://www.kaggle.com/code/aidemos/10-inseq-llm) | [Colab](https://colab.research.google.com/drive/1bWqGtRaG3aO7Vo149wIPHaz_XKnbJqlE?usp=sharing)

## 目录

- [安装和导入一些必要的库](#安装和导入一些必要的库)
- [可视化翻译任务](#可视化翻译任务)
  - [下载中译英模型](#下载中译英模型)
  - [进行特征归因](#进行特征归因)
- [可视化文本生成任务](#可视化文本生成任务)
  - [下载 GPT-2 XL 模型](#下载-gpt-2-xl-模型)
  - [加载模型](#加载模型)
  - [进行特征归因](#进行特征归因)
- [参考资料](#参考资料)

## 安装和导入一些必要的库

```bash
uv add inseq
uv add transformers
uv add bitsandbytes
uv add accelerate
uv add sacremoses
```

## 修复 Bug：cannot import name 'display' from 'IPython.core.display'

当前版本的 inseq 库存在一个导入路径问题。在 `inseq/data/viz.py` 文件中，库尝试从 `IPython.core.display` 导入 `display`，但在新版本的 IPython 中，应该从 `IPython.display` 导入。

执行以下命令修复：

```bash
import sys
import IPython.display
import IPython.core
IPython.core.display = IPython.display
sys.modules['IPython.core.display'] = IPython.display
```

## 可视化翻译任务

### 下载中译英模型

为了加快模型下载速度，我们可以使用多线程下载。如果直接运行以下命令报错，可以参考[使用 HFD 加快 Hugging Face 模型和数据集的下载](../Guide/a.%20使用%20HFD%20加快%20Hugging%20Face%20模型和数据集的下载.md)进行前置安装。

首先，下载 `hfd.sh` 脚本并赋予执行权限：

```bash
wget https://hf-mirror.com/hfd/hfd.sh
chmod a+x hfd.sh
```

然后，多线程下载模型：

```bash
export HF_ENDPOINT=https://hf-mirror.com
./hfd.sh 'Helsinki-NLP/opus-mt-zh-en' --tool aria2c -x 8
```

![image-20240919190843315]( ./assets/image-20240919190843315.png)

### 进行特征归因

```python
import inseq
import torch

print("使用 Helsinki-NLP/opus-mt-zh-en 模型")

# 定义要使用的归因方法列表
attribution_methods = ['saliency', 'attention']

for method in attribution_methods:
    print(f"\n======= 归因方法: {method} =======")

    # 直接用 inseq 加载模型
    inseq_model = inseq.load_model(
        "Helsinki-NLP/opus-mt-zh-en",
        attribution_method=method,
        model_kwargs={
            "attn_implementation": "eager" if method == "attention" else None
        }
    )

    # 准备输入文本
    input_text = "我喜欢机器学习和人工智能。"

    # 进行归因分析
    attribution_result = inseq_model.attribute(
        input_texts=input_text,
        show_progress=True
    )

    # 清理 tokenizer 中的特殊字符（可选）
    for attr in attribution_result.sequence_attributions:
        for item in attr.source:
            item.token = item.token.replace('▁', '')
        for item in attr.target:
            item.token = item.token.replace('▁', '')

    # 显示归因结果
    attribution_result.show()

    # 打印生成的翻译
    if attribution_result.sequence_attributions:
        # 获取生成的 tokens
        generated_tokens = attribution_result.sequence_attributions[0].target
        generated_text = " ".join([token.token for token in generated_tokens])
        print(f"翻译结果: {generated_text}")

    # 清理内存
    del inseq_model
    torch.cuda.empty_cache()
```

**输出：**

1. **Saliency** 方法
   通过计算输入对输出的梯度，衡量输入 token 对生成输出的影响程度。你可以看到生成的每个输出 token 是如何受到输入 token 的影响的，比如： `I` `like` 受 `我喜欢` 的影响最大。
   ![image-20240919191245260]( ./assets/image-20240919191245260.png)

2. **Attention** 方法
   利用模型的注意力机制来进行解释，展示输入和输出 token 之间的注意力权重。

   ![image-20240919191411509]( ./assets/image-20240919191411509.png)

## 可视化文本生成任务

### 下载 Qwen3-0.6B 模型

同样地，可以使用多线程方法下载 Qwen3-0.6B（也可以换成Qwen3-4B）：

```bash
export HF_ENDPOINT=https://hf-mirror.com
./hfd.sh 'Qwen/Qwen3-0.6B' --tool aria2c -x 8
```

### 进行特征归因

```python
import inseq

# 定义要使用的归因方法列表
attribution_methods = ['saliency', 'attention']

for method in attribution_methods:
    print(f"======= 归因方法: {method} =======")

    # 直接用 inseq 加载模型
    inseq_model = inseq.load_model(
        "Qwen/Qwen3-0.6B",
        attribution_method=method,
        model_kwargs={
            "device_map": {"": 0}
        }
    )

    # 设置 padding token
    if inseq_model.tokenizer.pad_token is None:
        inseq_model.tokenizer.pad_token = inseq_model.tokenizer.eos_token
        inseq_model.tokenizer.pad_token_id = inseq_model.tokenizer.eos_token_id

    
    # 对输入文本进行归因分析
    attribution_result = inseq_model.attribute(
        input_texts="Hello world",
        show_progress=True
    )

    # 清理 tokenizer 中的特殊字符（可选）
    for attr in attribution_result.sequence_attributions:
        for item in attr.source:
            item.token = item.token.replace('Ġ', '')
        for item in attr.target:
            item.token = item.token.replace('Ġ', '')

    # 显示归因结果
    attribution_result.show()

    # 清理内存
    del inseq_model
    torch.cuda.empty_cache()
```

**输出：**

1. **Saliency** 方法
   ![image-20240919194633424]( ./assets/image-20240919194633424.png)

2. **Attention** 方法
   ![image-20240919194730663]( ./assets/image-20240919194730663.png)

## 参考资料

- [Inseq 官方文档](https://inseq.readthedocs.io/)