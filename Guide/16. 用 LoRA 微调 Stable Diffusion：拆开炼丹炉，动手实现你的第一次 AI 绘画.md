# ç”¨ LoRA å¾®è°ƒ Stable Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡ AI ç»˜ç”»

> æ€»å¾—æ‹†å¼€ç‚¼ä¸¹ç‚‰çœ‹çœ‹æ˜¯ä»€ä¹ˆæ ·çš„ã€‚è¿™ç¯‡æ–‡ç« å°†å¸¦ä½ ä»ä»£ç å±‚é¢ä¸€æ­¥æ­¥å®ç° AI æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆText-to-Imageï¼‰ä¸­çš„ LoRA å¾®è°ƒè¿‡ç¨‹ï¼Œä½ å°†ï¼š
>
> - äº†è§£ **Trigger Words**ï¼ˆè§¦å‘è¯ï¼‰åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å½±å“ç”Ÿæˆç»“æœã€‚
> - æŒæ¡ LoRA å¾®è°ƒçš„åŸºæœ¬åŸç†ã€‚
> - å­¦ä¹ æ•°æ®é›†çš„å‡†å¤‡ä¸ç»“æ„ï¼Œå¹¶çŸ¥é“å¦‚ä½•æ ¹æ®éœ€æ±‚å®šåˆ¶è‡ªå·±çš„æ•°æ®é›†ã€‚
> - ç†è§£ Stable Diffusion æ¨¡å‹çš„å¾®è°ƒæ­¥éª¤ã€‚
> - æ˜ç™½åœ¨ç”»å›¾ç•Œé¢ï¼ˆUIï¼‰ä¸‹åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆã€‚
> - ä½¿ç”¨ä»£ç å®ç° AI ç»˜ç”»ã€‚
>
> å¦‚æœä½ æƒ³åˆ¶ä½œå±äºè‡ªå·±çš„æ•°æ®é›†ï¼Œæœ€å¥½éµå¾ªä»¥ä¸‹å»ºè®®ï¼š
>
> 1. **è‡³å°‘å‡†å¤‡ 20 å¼ å›¾ç‰‡**ï¼šæƒ³å­¦åˆ°çš„æ¦‚å¿µè¶Šå¤æ‚å°±éœ€è¦è¶Šå¤šçš„å›¾ç‰‡ã€‚ä½ å¯ä»¥å°è¯•å°†æ ·ä¾‹æ•°æ®é›†çš„å›¾ç‰‡æ•°é‡å‡å°‘åˆ° 20 å¼ ï¼Œçœ‹çœ‹æ•ˆæœä¼šæœ‰ä»€ä¹ˆå˜åŒ–ã€‚
> 2. **è£å‰ªå›¾ç‰‡**ï¼šå»ºè®®å¯¹å›¾ç‰‡è¿›è¡Œè£å‰ªï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥ä¸è£å‰ªï¼Œå¦‚æœä½ ä¸è¿½æ±‚æ•ˆæœçš„è¯ã€‚è¿™é‡Œä¼šè‡ªåŠ¨ resize åˆ°è‡ªå®šä¹‰çš„åˆ†è¾¨ç‡ã€‚
>
> **ä¸å…¶èŠ±è´¹å¤§é‡æ—¶é—´å»è°ƒå‚ï¼Œæ›´ä¼˜çš„é€‰æ‹©æ˜¯å¤„ç†å¥½ä½ çš„æ•°æ®é›†å’Œ Promptsã€‚å½“ç„¶ï¼Œè¿™ä¸¤ä»¶äº‹æƒ…å¯ä»¥åŒæ­¥è¿›è¡Œã€‚**
>
> æ³¨æ„ï¼Œå½“å‰æ–‡ç« ä½¿ç”¨çš„æ˜¯è‡ªç„¶è¯­è¨€æ ‡æ³¨ï¼ˆè€Œé Tagï¼‰ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ Tagï¼Œè¿™ä¸¤ç§æ–¹å¼æœ¬è´¨ä¸Šæ˜¯ä¸€è‡´çš„ã€‚
>
> åŒæ—¶ï¼Œå¦‚æœä½ å¯¹æ·±åº¦å­¦ä¹ æœ‰æ‰€äº†è§£ï¼Œé‚£ä¹ˆä»£ç ä¸­çš„ä¸€åˆ‡ï¼Œéƒ½å°†æ˜¯ä½ æ›¾ç»è§è¿‡çš„å†…å®¹ç¿»ç‰ˆï¼Œæ²¡æœ‰ä»€ä¹ˆæ–°çš„ï¼Œé™¤äº† LoRAã€‚å¦å¤–ï¼Œè¿™ç¯‡æ–‡ç« ä¹Ÿä¸º[ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¼è®º](https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php)è¯¾ç¨‹ä¸­ [HW10: Stable Diffusion Fine-tuning](https://colab.research.google.com/drive/1dI_-HVggxyIwDVoreymviwg6ZOvEHiLS?usp=sharing#scrollTo=CnJtiRaRuTFX) æä¾›ä¸­æ–‡å¼•å¯¼ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å°†åŒæ­¥ä½¿ç”¨æ¼”å‘˜ [Brad Pittï¼ˆå¸ƒæ‹‰å¾·Â·çš®ç‰¹ï¼‰](https://baike.baidu.com/item/å¸ƒæ‹‰å¾·Â·çš®ç‰¹/81288)çš„å›¾ç‰‡ä½œä¸ºè®­ç»ƒé›†ï¼Œå…±è®¡ä¸€ç™¾å¼ ã€‚
>
> ä»£ç æ–‡ä»¶ä¸‹è½½ï¼š[é•œåƒäº¤äº’ç‰ˆ](../Demos/14a.%20å°è¯•ä½¿ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusion%20æ¨¡å‹.ipynb) |  [ç²¾ç®€å­¦ä¹ ç‰ˆ](../Demos/14b.%20å°è¯•ä½¿ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusion%20æ¨¡å‹%20-%20ç²¾ç®€ç‰ˆ.ipynb) | [ğŸ¡ SD LoRA è„šæœ¬](../CodePlayground/sd_lora.py)
>
> åœ¨çº¿é“¾æ¥ï¼ˆç²¾ç®€ç‰ˆï¼‰ï¼š[Kaggle](https://www.kaggle.com/code/aidemos/14b-lora-stable-diffusion) | [Colab](https://colab.research.google.com/drive/1idmnaQZwRhjUPw7ToEXlVo82Mihfl_aA?usp=sharing)

## ç›®å½•


- [å‰è¨€](#å‰è¨€)
- [å¼€å§‹åŠ¨æ‰‹](#å¼€å§‹åŠ¨æ‰‹)
- [å®‰è£…å¿…è¦çš„åº“](#å®‰è£…å¿…è¦çš„åº“)
- [å¯¼å…¥](#å¯¼å…¥)
- [å‡†å¤‡æ•°æ®](#å‡†å¤‡æ•°æ®)
- [è®¾ç½®é¡¹ç›®è·¯å¾„](#è®¾ç½®é¡¹ç›®è·¯å¾„)
- [å¯¼å…¥æ•°æ®](#å¯¼å…¥æ•°æ®)
  - [æ€ä¹ˆæ‰©å……æ•°æ®é›†ï¼Ÿ](#æ€ä¹ˆæ‰©å……æ•°æ®é›†)
  - [æ€ä¹ˆè®©æ¨¡å‹ç†è§£æ–‡æœ¬ï¼Ÿ](#æ€ä¹ˆè®©æ¨¡å‹ç†è§£æ–‡æœ¬)
  - [è‡ªå®šä¹‰æ•°æ®é›†](#è‡ªå®šä¹‰æ•°æ®é›†)
- [å®šä¹‰å¾®è°ƒç›¸å…³çš„å‡½æ•°](#å®šä¹‰å¾®è°ƒç›¸å…³çš„å‡½æ•°)
  - [åŠ è½½ LoRA](#åŠ è½½-lora)
  - [å‡†å¤‡ä¼˜åŒ–å™¨](#å‡†å¤‡ä¼˜åŒ–å™¨)
  - [å®šä¹‰ collate_fn å‡½æ•°](#å®šä¹‰-collate_fn-å‡½æ•°)
- [è®¾ç½®ç›¸å…³å‚æ•°](#è®¾ç½®ç›¸å…³å‚æ•°)
  - [è®¾å¤‡é…ç½®](#è®¾å¤‡é…ç½®)
  - [æ¨¡å‹ä¸è®­ç»ƒå‚æ•°é…ç½®](#æ¨¡å‹ä¸è®­ç»ƒå‚æ•°é…ç½®)
- [å¾®è°ƒå‰çš„å‡†å¤‡](#å¾®è°ƒå‰çš„å‡†å¤‡)
  - [å‡†å¤‡æ•°æ®é›†](#å‡†å¤‡æ•°æ®é›†)
  - [å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨](#å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨)
- [å¼€å§‹å¾®è°ƒ](#å¼€å§‹å¾®è°ƒ)
- [ç”Ÿæˆå›¾åƒå’Œè¯„ä¼°](#ç”Ÿæˆå›¾åƒå’Œè¯„ä¼°)
  - [ä»€ä¹ˆæ˜¯ pipelineï¼Ÿ](#ä»€ä¹ˆæ˜¯-pipeline)
  - [æ¨ç†ç›¸å…³çš„å‚æ•°](#æ¨ç†ç›¸å…³çš„å‚æ•°)
  - [åŠ è½½ç”¨äºéªŒè¯çš„ prompts](#åŠ è½½ç”¨äºéªŒè¯çš„-prompts)
  - [å®šä¹‰ç”Ÿæˆå›¾åƒçš„å‡½æ•°](#å®šä¹‰ç”Ÿæˆå›¾åƒçš„å‡½æ•°)
  - [å®šä¹‰è¯„ä¼°å‡½æ•°](#å®šä¹‰è¯„ä¼°å‡½æ•°)
- [æ‹“å±•ä½œä¸š](#æ‹“å±•ä½œä¸š)
- [ç”¨ ğŸ¡ è„šæœ¬å¾®è°ƒ SDï¼ˆå¯é€‰ï¼‰](#ç”¨--è„šæœ¬å¾®è°ƒ-sdå¯é€‰)

  - [å…‹éš†ä»“åº“](#å…‹éš†ä»“åº“)
  - [æ‰§è¡Œè„šæœ¬](#æ‰§è¡Œè„šæœ¬)

- [å‚è€ƒé“¾æ¥](#å‚è€ƒé“¾æ¥)

## å‰è¨€

ä¸‹é¢æ˜¯ä½¿ç”¨ promptï¼š`"A man in a graphic tee and sport coat."`ï¼Œåœ¨é»˜è®¤è®¾ç½®ä¸‹è®­ç»ƒ 2000 ä¸ªæ­¥éª¤åæ¨¡å‹ç”Ÿæˆçš„å›¾åƒï¼Œè®­ç»ƒæ—¶é•¿çº¦ä¸º 18 åˆ†é’Ÿã€‚ä¹ä¸€çœ‹ï¼Œæ˜¯ä¸æ˜¯è¿˜æŒºä¸é”™çš„ï¼Ÿ

![valid_image](./assets/valid_image_12.png)

ä½ å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œæˆ‘ä»¬çš„ prompt ä¸­å¹¶æ²¡æœ‰æåˆ° Brad Pittï¼ˆå¸ƒæ‹‰å¾·Â·çš®ç‰¹ï¼‰è¿™ä¸ªæ¼”å‘˜ï¼ˆå°½ç®¡æˆ‘ä»¬çš„æ•°æ®é›†å®Œå…¨æ¥è‡ªäºä»–ï¼‰ï¼Œä½†æ¨¡å‹å´èƒ½å¤Ÿç»˜åˆ¶é•¿å¾—åƒ Brad Pitt çš„äººã€‚

è¿™æ˜¯å› ä¸ºï¼Œå¦‚æœæˆ‘ä»¬åœ¨ prompt ä¸­ç›´æ¥æŒ‡å®š "Brad Pitt"ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•å®Œå…¨å­¦ä¹ åˆ°ä»–çš„ç‰¹å¾é£æ ¼ã€‚ä¸¾ä¸ªä¾‹å­ï¼š

- "A man in a graphic tee and sport coat. Brad Pitt."
- "A man in a graphic tee and sport coat."

ç¬¬ä¸€æ¡ prompt æ˜¾ç„¶æ›´ç²¾å‡†ï¼Œä½†ç²¾å‡†å¹¶ä¸æ„å‘³ç€æ¨¡å‹è®­ç»ƒå¾—æ›´å¥½ã€‚å¦‚æœä½ ç”¨ä¸€ç³»åˆ—åŒ…å« "Brad Pitt" çš„ prompt æ¥è®­ç»ƒï¼Œæ¨¡å‹æ›´æœ‰å¯èƒ½å­¦åˆ°çš„æ˜¯ï¼šåªæœ‰åœ¨åŠ ä¸Š "Brad Pitt" æ—¶æ‰è¿›è¡Œé£æ ¼è½¬å˜ã€‚ä½ å¯èƒ½ä¼šè¯´ï¼šâ€œæˆ‘å°±æ˜¯æƒ³è¦è¿™ä¸ªæ•ˆæœâ€ï¼Œé‚£ä¹ˆå¾ˆå¥½ï¼Œ"Brad Pitt" å°±æ˜¯ä½ æ¨¡å‹çš„ **Trigger Word**ï¼ˆè§¦å‘è¯ï¼‰ã€‚ä½†æœ‰å¯èƒ½è¿˜æœ‰åŒå­¦ï¼šâ€œæˆ‘å¸Œæœ›æ¨¡å‹åªä¸º Brad Pitt æœåŠ¡ï¼Œæˆ‘è¦æŠŠæ‰€æœ‰çš„ 'man' éƒ½å˜æˆ Brad Pittâ€ï¼Œé‚£ä¹ˆåœ¨è®­ç»ƒæ—¶å°±ä¸è¦åœ¨ prompt ä¸­å¢åŠ  "Brad Pitt"ã€‚ç®€è€Œè¨€ä¹‹ï¼š**åç€æ¥**ã€‚

è¿™å®é™…ä¸Šå¹¶æ²¡æœ‰åç›´è§‰ï¼Œè·³å‡ºæ¥æƒ³ä¸€æƒ³ï¼š

1. æƒ³è±¡ä¸€ä¸‹ä½ æ˜¯ä¸€ä½ç”»å®¶ï¼Œç”Ÿæ´»åœ¨ä¸€ä¸ªä»ä¸å˜æš—çš„ä¸–ç•Œé‡Œï¼Œæ•´ä¸ªä¸–ç•Œæ°¸è¿œæ˜¯ç™½å¤©ï¼Œä½ å·²ç»ä¹ æƒ¯ç”»å‡ºç™½å¤©èƒŒæ™¯ä¸‹çš„å„ç§æ™¯è±¡ï¼Œä½†ä½ ä¸çŸ¥é“ç™½å¤©æ˜¯ä»€ä¹ˆï¼Œè¿™å°±æ˜¯ä½ æ‰€ç†ŸçŸ¥çš„ã€Œæ—¥å¸¸ã€ã€‚

2. æœ‰ä¸€å¤©ï¼Œæœ‰äººç»™ä½ çœ‹äº†ä¸€äº›ç…§ç‰‡ï¼Œè¯´ï¼šâ€œHeyï¼Œå®é™…ä¸Šä¸–ç•Œå¯ä»¥æ˜¯é»‘çš„ï¼Œå«åšå¤œæ™šâ€ï¼Œè¿™æ—¶å€™ä½ å°±ä¼šç†è§£åˆ°ï¼Œæ—¥å¸¸æ˜¯æœ‰å¦ä¸€ç§çŠ¶æ€çš„ï¼Œå«åšå¤œæ™šï¼Œå³ä¾¿ä½ ä»¥å‰ä»æ¥æ²¡æœ‰è¿‡æ¦‚å¿µï¼Œä½†ç°åœ¨ï¼Œä½ å°†è®¤çŸ¥åˆ°å®ƒï¼Œä½ å°†è¿™éƒ¨åˆ†æ–°çš„æ¦‚å¿µèšç„¦åˆ°äº†ã€Œå¤œæ™šã€ã€‚äºæ˜¯ï¼Œä»æ­¤ä»¥åï¼Œä½ çš„ç”»ä½œè¢«åˆ†ä¸ºäº†ã€Œæ—¥å¸¸ã€å’Œã€Œæ—¥å¸¸ï¼Œå¤œæ™šã€ã€‚

3. åŒæ—¶ï¼Œåœ¨å¦ä¸€ä¸ªå¹³è¡Œä¸–ç•Œï¼Œæœ‰äººå‘Šè¯‰ä½ ï¼šâ€œä½ çœ¼ä¸­çœ‹åˆ°çš„ä¸–ç•Œæ˜¯ä¸å¯¹çš„â€ï¼Œä»–ä»¬â€œæ²»â€å¥½äº†ä½ çš„çœ¼ç›ï¼Œå‘ä½ å±•ç¤ºäº†ä¸€ä¸ªå®Œå…¨é™Œç”Ÿçš„æ¼†é»‘ä¸–ç•Œï¼Œå¹¶æ‰¿è¯ºåªè¦ä½ å­¦ä¼šç”»å‡ºè¿™ç§é£æ ¼çš„ç”»ä½œï¼Œå°†ä¼šè·å¾—ä¸°åšçš„å›æŠ¥ï¼Œå¦åˆ™å°†æ— äººé—®æ´¥ä½ çš„ç”»æ‘Šã€‚äºæ˜¯ä½ å¼€å§‹ç”»â€œå¤œæ™šâ€é£æ ¼çš„ã€Œæ—¥å¸¸ã€ã€‚

è¿™æ˜¯æœæ”¥çš„ä¸‰ä¸ªå°ç‰‡æ®µï¼Œå¸Œæœ›ä½ å–œæ¬¢ã€‚

ä½ å¯ä»¥åˆ†åˆ«å°†å®ƒç†è§£ä¸ºï¼š

1. **åŸå§‹æ¨¡å‹**ï¼šæ´»åœ¨è‡ªå·±ä¸–ç•Œçš„ç”»å®¶ã€‚
2. **LoRA å¾®è°ƒ**ï¼šå½“æ–°æ ‡ç­¾ï¼ˆTagï¼‰â€œå¤œæ™šâ€è¢«å¼•å…¥ï¼Œç”»å®¶å­¦ä¼šäº†å¤œæ™šçš„æ¦‚å¿µã€‚Promptï¼šå¤œæ™šï¼Œæ—¥å¸¸ã€‚
3. **å¦ä¸€ä¸ª LoRA å¾®è°ƒ**ï¼šè¿ç§»é£æ ¼ï¼Œç”»å®¶å°†â€œå¤œæ™šâ€è§†ä¸ºçœŸæ­£çš„æ—¥å¸¸é£æ ¼ã€‚Promptï¼šæ—¥å¸¸ã€‚

å› æ­¤ï¼Œè®­ç»ƒæ¨¡å‹å°±åƒæ•™å°æœ‹å‹è®¤çŸ¥ä¸–ç•Œã€‚å¦‚æœä½ å°†ä¸–ç•Œåˆ†è§£ä¸ºä¸åŒçš„æ¦‚å¿µå¹¶é€ä¸€ä¼ æˆï¼Œå­©å­ä¼šå­¦åˆ°ä¸åŒçš„çŸ¥è¯†ã€‚è¿™å°±ç±»ä¼¼äºæ¨¡å‹å­¦ä¹ ä¸åŒçš„æ ‡ç­¾å’Œé£æ ¼ã€‚å¦‚æœä½ ä¸æ˜ç¡®åŒºåˆ†æ¦‚å¿µï¼Œå¹¶å°†æ–°æ¦‚å¿µæ··æ‚åœ¨å·²æœ‰çš„è®¤çŸ¥ä¸­ï¼Œå­©å­çš„è®¤çŸ¥ä¼šè¢«é‡å¡‘ï¼Œæˆ–è®¸ä¼šå°†é¹¿â€œè¯¯â€è®¤ä¸ºé©¬ã€‚è¿™æ˜¯åˆç†çš„ï¼Œæ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå–å†³äºä½ å¦‚ä½•æ•™å¯¼ï¼ˆpromptï¼‰å®ƒã€‚

**Prompt å°æŠ€å·§ï¼š**

- **æ˜ç¡®ä½ çš„ç›®æ ‡**ï¼šåœ¨è®­ç»ƒå‰ï¼Œæ€è€ƒä½ æ˜¯å¸Œæœ›æ¨¡å‹å­¦ä¹ ç‰¹å®šçš„é£æ ¼ã€ç‰¹å®šçš„äººç‰©ï¼Œè¿˜æ˜¯å¸Œæœ›æ¨¡å‹åœ¨ç‰¹å®šçš„åœºæ™¯ä¸‹æ‰ç”Ÿæˆç‰¹å®šçš„æ•ˆæœã€‚åˆ°åº•æ˜¯å¸Œæœ›æ‰€æœ‰çš„ man éƒ½æ˜¯ Brad Pittï¼Œè¿˜æ˜¯å¸Œæœ›æ¨¡å‹çŸ¥é“ Brad Pitt æ˜¯ä¸€ä¸ª manã€‚
- **ä¿æŒä¸€è‡´æ€§**ï¼šå¦‚æœä½ å¸Œæœ›å°†æŸä¸ªæ¦‚å¿µæ‹†åˆ†å‡ºæ¥ï¼Œåº”è¯¥ä¸ºå®ƒåˆ›å»ºä¸€ä¸ªç‰¹å®šçš„æ ‡ç­¾ï¼ˆtagï¼‰ï¼Œå¹¶åº”ç”¨äºå…·æœ‰ç›¸åŒæ¦‚å¿µçš„å›¾åƒä¸Šã€‚

å¤§æ¨¡å‹å¾ˆèªæ˜ï¼Œå®ƒä¼šè‡ªåŠ¨å°†å›¾åƒä¸­çš„å…±æ€§å½’å› äºå…±ç”¨çš„æ ‡ç­¾ä¸Šã€‚å› æ­¤ï¼Œå¦‚æœä¸ç»™å®ƒæ–°çš„æ ‡ç­¾ï¼Œå®ƒä¼šå°†æ–°å­¦åˆ°çš„å†…å®¹èå…¥åˆ°å·²æœ‰çš„æ ‡ç­¾ä¸­ã€‚

è¿™äº›æ˜¯å…³äº AI ç»˜ç”» Prompt + å¾®è°ƒèƒŒåé€»è¾‘çš„å¤§ç™½è¯ã€‚æ‰¯è¿œäº†ï¼Œè®©æˆ‘ä»¬å›åˆ°ä»£ç éƒ¨åˆ† :)

## å¼€å§‹åŠ¨æ‰‹

ä¸‹é¢ï¼Œæˆ‘å°†å¸¦ä½ ä»ä»£ç å±‚é¢ä¸€æ­¥æ­¥å®ç° LoRA å¾®è°ƒ Stable Diffusion æ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™é‡Œçš„çŸ¥è¯†æ˜¯é€šç”¨çš„ï¼Œä½ å®Œå…¨å¯ä»¥æ¨å¹¿è‡³ä»»ä½•éœ€è¦ LoRA å¾®è°ƒçš„é¢†åŸŸã€‚

## å®‰è£…å¿…è¦çš„åº“

é¦–å…ˆï¼Œç¡®ä¿å®‰è£…ä»¥ä¸‹å¿…è¦çš„ Python åº“ï¼š

```bash
pip install timm
pip install fairscale
pip install transformers
pip install requests
pip install accelerate
pip install diffusers
pip install einop
pip install safetensors
pip install voluptuous
pip install jax
pip install jaxlib
pip install peft
pip install deepface==0.0.92
pip install tensorflow==2.9.0  # ä¸ºäº†é¿å…æœ€åè¯„ä¼°é˜¶æ®µä½¿ç”¨deepfaceæ—¶çš„é”™è¯¯ï¼Œè¿™é‡Œé€‰æ‹©é™çº§ç‰ˆæœ¬
pip install keras
pip install opencv-python
```

## å¯¼å…¥

```python
# ========== æ ‡å‡†åº“æ¨¡å— ==========
import os
import math
import glob
import shutil
import subprocess

# ========== ç¬¬ä¸‰æ–¹åº“ ==========
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from tqdm.auto import tqdm

# ========== æ·±åº¦å­¦ä¹ ç›¸å…³åº“ ==========
from torchvision import transforms

# Transformers (Hugging Face)
from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPProcessor

# Diffusers (Hugging Face)
from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    UNet2DConditionModel,
    DiffusionPipeline
)
from diffusers.optimization import get_scheduler
from diffusers.training_utils import compute_snr

# ========== LoRA æ¨¡å‹åº“ ==========
from peft import LoraConfig, get_peft_model, PeftModel

# ========== é¢éƒ¨æ£€æµ‹åº“ ==========
from deepface import DeepFace

import cv2
```

## å‡†å¤‡æ•°æ®

å½“å‰æ¼”ç¤ºä½¿ç”¨çš„æ˜¯ Brad Pittï¼ˆå¸ƒæ‹‰å¾·Â·çš®ç‰¹ï¼‰ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹ç»˜åˆ¶çš„ man æ˜¯ Brad Pittï¼Œç²—ç•¥åœ°æ¢ä¸ªè¡¨è¿°ï¼šAI æ¢è„¸ã€‚

é‚£æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„æè¿°ï¼Œæ ‡æ³¨åº”è¯¥é•¿ä»€ä¹ˆæ ·å‘¢ï¼Ÿ

**ç­”**ï¼šéƒ½å¸¦ â€œmanâ€ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å½“å‰æ•°æ®é›†çš„æ ‡æ³¨ç¤ºä¾‹ï¼š

1. a man with a beard and a suit jacket
2. a man in a suit and tie standing in front of a crowd
3. a man with long hair and a tie
4. ...

ç›¸ä¿¡ä½ å‘ç°äº†ï¼Œæ‰€æœ‰çš„æ ‡æ³¨ï¼Œéƒ½ä¸ä¼šå«æœ‰ â€œBrad Pittâ€ï¼Œé‚£è¿™ç¯‡æ–‡ç« è®­ç»ƒå‡ºçš„ LoRA æ¨¡å‹çš„ **Trigger Words**ï¼ˆè§¦å‘è¯ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”**ï¼šâ€œa manâ€ã€‚

æ˜¯ä¸æ˜¯å¾ˆæœ‰è¶£ï¼Œçœ‹ä¼¼ç®€å•çš„ Prompt ä¸­ä¹Ÿæœ‰ä¸€äº›çœŸå®æœ‰ç”¨çš„å°æŠ€å·§å’Œé€»è¾‘ã€‚åˆ«æ€¥ç€å»ç‚¼ä¸¹ï¼Œæˆ‘ä»¬ç»§ç»­å¾€ä¸‹çœ‹ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ Brad Pitt çš„ 100 å¼ å›¾ç‰‡è¿›è¡Œæ¼”ç¤ºï¼Œæ•°æ®é›†å·²ç»ä¸Šä¼ åˆ°äº†[Demos/data/14](../Demos/data/14/Datasets.zip)ï¼Œä½ å¯ä»¥ä¸‹è½½åæ”¾åˆ°å½“å‰ç›®å½•ä¸‹çš„ `./data/14` ä¸‹ã€‚è¿™ä¸ªè·¯å¾„æ²¡æœ‰ä»€ä¹ˆè¯´æ³•ï¼Œå•çº¯æ˜¯ä¸ºäº†å¯¹é½ç¤ºä¾‹ä»£ç ï¼Œä½ ä¹Ÿå¯ä»¥ä¿®æ”¹ä»£ç å…³äºæ•°æ®çš„è·¯å¾„ï¼Œè¿™é‡Œä¸ä¼šæœ‰é™åˆ¶ï¼Œä½ ç”šè‡³å¯ä»¥ç›´æ¥ç”¨å…¶ä»–çš„æ•°æ®é›†ï¼Œåªè¦å®ƒçš„æ–‡ä»¶ç»„ç»‡å¦‚ä¸‹ï¼š

```
-- å›¾ç‰‡1
-- å›¾ç‰‡1.txt
-- å›¾ç‰‡2
-- å›¾ç‰‡2.txt
...
```

> **æ³¨æ„**ï¼šå›¾ç‰‡å’Œå¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨éœ€è¦åŒåï¼Œä¸”ä½äºåŒä¸€æ–‡ä»¶å¤¹ä¸­ã€‚

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæ ·ä¾‹æ•°æ®é›†çš„è£å‰ªå¤§å°å’Œæ¯”ä¾‹éƒ½æ˜¯ä¸ä¸€è‡´çš„ï¼Œåªæ˜¯æ¥è¿‘æ­£æ–¹å½¢ï¼Œä½†è¿™æ²¡æœ‰å¤ªå¤§çš„å…³ç³»ï¼Œå› ä¸ºåœ¨æ•°æ®é¢„å¤„ç†çš„æ—¶å€™ä¼šè‡ªåŠ¨æ”¾ç¼©ï¼ˆresizeï¼‰ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œä¸ç”¨æ‹…å¿ƒä½ çš„æ•°æ®é›†æ— æ³•è®­ç»ƒã€‚

## è®¾ç½®é¡¹ç›®è·¯å¾„

å¾ˆå¥½ï¼ç°åœ¨ä½ å·²ç»çŸ¥é“è¿™ç¯‡æ–‡ç« æ•°æ®é›†ç›¸å…³çš„æ‰€æœ‰å‰ç½®çŸ¥è¯†ï¼Œç›´æ¥å¤åˆ¶ä¸‹é¢çš„ä»£ç è¿è¡Œï¼Œä¸ç”¨åœ¨æ„å…¶ä¸­çš„ä»»ä½•ä»£ç ç»†èŠ‚ï¼Œä½ åªéœ€è¦çŸ¥é“ä¼šåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹`SD`ï¼Œä¹‹åçš„æ‰€æœ‰ç»“æœéƒ½ä¼šè¢«å­˜æ”¾åœ¨å…¶ä¸­ï¼š

```python
# é¡¹ç›®åç§°å’Œæ•°æ®é›†åç§°
project_name = "Brad"
dataset_name = "Brad"

# æ ¹ç›®å½•å’Œä¸»è¦ç›®å½•
root_dir = "./"  # å½“å‰ç›®å½•
main_dir = os.path.join(root_dir, "SD")  # ä¸»ç›®å½•

# é¡¹ç›®ç›®å½•
project_dir = os.path.join(main_dir, project_name)  # é¡¹ç›®ç›®å½•

# æ•°æ®é›†å’Œæ¨¡å‹è·¯å¾„
images_folder = os.path.join(main_dir, "Datasets", dataset_name)
prompts_folder = os.path.join(main_dir, "Datasets", "prompts")
captions_folder = images_folder  # ä¸åŸå§‹ä»£ç ä¸€è‡´
output_folder = os.path.join(project_dir, "logs")  # å­˜æ”¾ model checkpoints å’Œ validation çš„æ–‡ä»¶å¤¹

# prompt æ–‡ä»¶è·¯å¾„
validation_prompt_name = "validation_prompt.txt"
validation_prompt_path = os.path.join(prompts_folder, validation_prompt_name)

# æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
model_path = os.path.join(project_dir, "logs", "checkpoint-last")

# å…¶ä»–è·¯å¾„è®¾ç½®
zip_file = os.path.join("./", "data/14/Datasets.zip")
inference_path = os.path.join(project_dir, "inference")  # ä¿å­˜æ¨ç†ç»“æœçš„æ–‡ä»¶å¤¹

os.makedirs(images_folder, exist_ok=True)
os.makedirs(prompts_folder, exist_ok=True)
os.makedirs(output_folder, exist_ok=True)
os.makedirs(inference_path, exist_ok=True)

# æ£€æŸ¥å¹¶è§£å‹æ•°æ®é›†
print("ğŸ“‚ æ­£åœ¨æ£€æŸ¥å¹¶è§£å‹æ ·ä¾‹æ•°æ®é›†...")

if not os.path.exists(zip_file):
    print("âŒ æœªæ‰¾åˆ°æ•°æ®é›†å‹ç¼©æ–‡ä»¶ Datasets.zipï¼")
    print("è¯·ä¸‹è½½æ•°æ®é›†:\n../Demos/data/14/Datasets.zip\nå¹¶æ”¾åœ¨ ./data/14 æ–‡ä»¶å¤¹ä¸‹")
else:
    subprocess.run(f"unzip -q -o {zip_file} -d {main_dir}", shell=True)
    print(f"âœ… é¡¹ç›® {project_name} å·²å‡†å¤‡å¥½ï¼")
```

å¦‚æœä½ ç”¨çš„æ˜¯è‡ªå·±çš„æ•°æ®é›†ï¼Œä¿®æ”¹ `zip_file` å³å¯ï¼ˆå‹ç¼©ä¸º zip æ ¼å¼ï¼‰ï¼š

```python
zip_file = # æ”¹ä¸ºä½ è‡ªå·±çš„æ•°æ®é›†è·¯å¾„
```

## å¯¼å…¥æ•°æ®

ä¸‹é¢ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰ä¸€ä¸ª `Dataset` ç±»ï¼Œå®ƒçš„ä½œç”¨æ˜¯å‘Šè¯‰æ¨¡å‹å¦‚ä½•å¤„ç†ä½ çš„æ•°æ®é›†ï¼Œè¿™ä¸ªè‡ªå®šä¹‰çš„ç±»èƒ½å¤Ÿè¿”å›å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨åˆ†åˆ«ä½œä¸º `data` å’Œ `label`ã€‚æ¥ä¸‹æ¥çš„å†…å®¹ä¼šæœ‰ç‚¹â€œå¹²â€ï¼Œä½ ä¹Ÿå¯ä»¥å°†å…¶å…ˆå½“ä½œé»‘ç›’ï¼Œæˆ‘ä¼šåœ¨æ¯ä¸ªå‡½æ•°ä¹‹åæä¾›ä¸€ä¸ªç®€ç»ƒçš„è§£é‡Šå¸®ä½ ç†è§£ã€‚

### æ€ä¹ˆæ‰©å……æ•°æ®é›†ï¼Ÿ

> æ‹“å±•æ–‡ç« ï¼š[e. æ•°æ®å¢å¼ºï¼štorchvision.transforms å¸¸ç”¨æ–¹æ³•è§£æ](../Guide/e.%20æ•°æ®å¢å¼ºï¼štorchvision.transforms%20å¸¸ç”¨æ–¹æ³•è§£æ.md)

è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸ç†Ÿæ‚‰çš„è¯ï¼š`transform`ï¼Œä½†è¿™ä¸ªè·Ÿæˆ‘ä»¬è€³ç†Ÿèƒ½è¯¦çš„ `transformer` å¯ä¸åŒï¼Œ`transform` å°±æ˜¯å•çº¯çš„å¯¹å›¾åƒè¿›è¡Œæ“ä½œï¼Œæ¯”å¦‚è¯´è°ƒæ•´å¤§å°ï¼Œç¿»è½¬ï¼Œåˆæˆ–è€…éšæœºçš„è£å‰ªä¸€éƒ¨åˆ†åŒºåŸŸï¼Œè¿™äº›æ“ä½œç»Ÿç§°ä¸ºæ•°æ®å¢å¼ºã€‚

æ•°æ®å¢å¼ºå°±æ˜¯æ‰©å……æ•°æ®é›†çš„å¤–æŒ‚ï¼Œä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œå³ä¾¿è¿›è¡Œæ°´å¹³ç¿»è½¬+é¢œè‰²å˜åŒ–+ä¸­å¿ƒè£å‰ªï¼Œå®ƒä¹Ÿæ˜¯ä¸€åªä¼é¹…ã€‚

![Transform](./assets/1.png)

è¿™å¤§å¤§åœ°æ‰©å……äº†æ•°æ®é›†ã€‚

çŸ¥é“äº†æ¦‚å¿µåï¼Œç®€å•å®šä¹‰å½“å‰çš„æ•°æ®å¢å¼ºå¦‚ä¸‹ï¼š

```python
# è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡
resolution = 512

# æ•°æ®å¢å¼ºæ“ä½œ
train_transform = transforms.Compose(
    [
        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°
        transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ
        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
        transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
    ]
)
```

### æ€ä¹ˆè®©æ¨¡å‹ç†è§£æ–‡æœ¬ï¼Ÿ

ä½¿ç”¨ CLIPTokenizerï¼Œè¿™æ˜¯ Hugging Face transformers åº“ä¸­çš„ä¸€ä¸ªç±»ï¼Œä¸“é—¨ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œ**åˆ†è¯**ï¼ˆtokenizationï¼‰æ“ä½œã€‚CLIPï¼Œå…¨ç§° **Contrastive Language-Image Pretraining**ï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰ï¼ŒContrastive è¿™ä¸ªè¯è¯´é€äº†å®ƒçš„ç”±æ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ„æ€çš„è‡ªè®­ç»ƒæ€æƒ³ï¼šé€šè¿‡æœ€å¤§åŒ–å¯¹åº”æ–‡æœ¬-å›¾åƒå¯¹çš„ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–ä¸åŒæ–‡æœ¬-å›¾åƒå¯¹çš„ç›¸ä¼¼æ€§å®ç°è®­ç»ƒã€‚

> **å­¦ä¹ èµ„æ–™**
>
> è®ºæ–‡é“¾æ¥ï¼š[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
> å¯¹ç†è®ºæ„Ÿå…´è¶£çš„è¯å¯ä»¥è¿›ä¸€æ­¥æŸ¥çœ‹ä»¥ä¸‹å››ä¸ªéå¸¸æ£’çš„è§†é¢‘ï¼š
>
> 1. [å¯¹æ¯”å­¦ä¹ è®ºæ–‡ç»¼è¿°ã€è®ºæ–‡ç²¾è¯»ã€‘](https://www.bilibili.com/video/BV19S4y1M7hm/?share_source=copy_web&vd_source=e46571d631061853c8f9eead71bdb390)
> 2. [CLIP è®ºæ–‡é€æ®µç²¾è¯»ã€è®ºæ–‡ç²¾è¯»ã€‘](https://www.bilibili.com/video/BV1SL4y1s7LQ/?share_source=copy_web&vd_source=e46571d631061853c8f9eead71bdb390)
> 3. [CLIP æ”¹è¿›å·¥ä½œä¸²è®²ï¼ˆä¸Šï¼‰ã€è®ºæ–‡ç²¾è¯»Â·42ã€‘](https://www.bilibili.com/video/BV1FV4y1p7Lm/?share_source=copy_web&vd_source=e46571d631061853c8f9eead71bdb390)
> 4. [CLIP æ”¹è¿›å·¥ä½œä¸²è®²ï¼ˆä¸‹ï¼‰ã€è®ºæ–‡ç²¾è¯»Â·42ã€‘](https://www.bilibili.com/video/BV1gg411U7n4/?share_source=copy_web&vd_source=e46571d631061853c8f9eead71bdb390)
>
> ä½ å°†å‘ç°ä¸¤ä¸ªå®è— UP ä¸»ï¼Œæˆ‘æ— æ³•ç”¨è¯­è¨€è¡¨è¾¾å¯¹ä»–ä»¬çš„èµç¾ï¼Œåªèƒ½é“ä¸€å¥ï¼šâ€œå¯¼å¸ˆå¥½ï¼â€ã€‚

å…·ä½“æ¥è¯´ï¼Œ`CLIPTokenizer` å°†è¾“å…¥çš„ prompt æ‹†è§£ä¸º tokenï¼ˆå•è¯æˆ–å­è¯ï¼‰ï¼Œå¹¶å°†è¿™äº› token æ˜ å°„ä¸º`input_ids` ä¾› CLIP æ¨¡å‹çš„ `text_encoder` å¤„ç†ï¼Œä»è€Œç”Ÿæˆ prompt çš„åµŒå…¥å‘é‡ï¼Œä»¥è®©æ¨¡å‹ç†è§£ã€‚

å°±åƒä¸€åˆ‡æ•°æ®åˆ°äº†è®¡ç®—æœºä¸­éƒ½å˜æˆ 0ï¼Œ1 è®©å…¶å¤„ç†ï¼Œæ‰€ä»¥å‘ä¸ŠæŠ½è±¡ä¸€ä¸‹ï¼Œ`CLIP` å°±æ˜¯å°†äººç±»å¯ä»¥é˜…è¯»çš„æ–‡æœ¬æè¿°å˜æˆæ¨¡å‹èƒ½å¤Ÿç†è§£çš„å½¢å¼ã€‚

> **æ‹“å±•ï¼šçœ‹çœ‹ Tokenizer å®é™…ä¸Šåšäº†ä»€ä¹ˆ**
>
> ```python
> from transformers import CLIPTokenizer
> 
> # åˆå§‹åŒ– CLIPTokenizer
> tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
> 
> # ç¤ºä¾‹ prompt
> prompt_text = "A man in a graphic tee and sport coat."
> 
> # å…ˆä½¿ç”¨ tokenizer.tokenize æŸ¥çœ‹åˆ†è¯åçš„ token
> tokens = tokenizer.tokenize(prompt_text)
> print("Tokens:", tokens)
> 
> # å°†æ–‡æœ¬è½¬åŒ–ä¸º token
> inputs = tokenizer(
>     prompt_text,
>     padding="max_length",  # å¦‚æœè¾“å…¥é•¿åº¦ä¸è¶³æœ€å¤§é•¿åº¦ï¼Œè¿›è¡Œå¡«å……
>     truncation=True,       # å¦‚æœè¾“å…¥è¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­
>     return_tensors="pt"    # è¿”å› PyTorch å¼ é‡
> )
> 
> # æ‰“å°åˆ†è¯åçš„ç»“æœ
> print("Tokenized Input IDs:", inputs.input_ids)
> print("Attention Mask:", inputs.attention_mask)
> ```
> **è¾“å‡ºï¼š**
>
> ```python
> Tokens: ['a</w>', 'man</w>', 'in</w>', 'a</w>', 'graphic</w>', 'tee</w>', 'and</w>', 'sport</w>', 'coat</w>', '.</w>']
> Tokenized Input IDs: tensor([[49406,   320,   786,   530,   320,  4245,  3385,   537,  2364,  7356,
>            269, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
>          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
>          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
>          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
>          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
>          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
>          49407, 49407, 49407, 49407, 49407, 49407, 49407]])
> Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>          0, 0, 0, 0, 0]])
> ```
> é—®ï¼š49407 æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘ä»¬çš„ prompt ä¸­ä¼¼ä¹æ²¡æœ‰é‡å¤çš„è¯ã€‚
>
> ç­”ï¼šç»“æŸæ ‡è®°ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬è®¾ç½®äº† `padding="max_length"`ã€‚æ€è€ƒä¸€ä¸‹ï¼Œè®¾ç½®`padding=False`åè¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿå…ˆä¸è¦å¾€ä¸‹æ»‘ã€‚
>
> #### å…·ä½“è§£é‡Šï¼š
>
> - **Tokenized Input IDs**ï¼šè¿™ä¸ªå¼ é‡å±•ç¤ºäº†è¾“å…¥æ–‡æœ¬ `A man in a graphic tee and sport coat.` è¢«è½¬æ¢ä¸ºçš„æ•°å­— ID åºåˆ—ã€‚æ¯ä¸ªæ•°å­— ID å¯¹åº”äºè¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ª tokenï¼Œ`49406` æ˜¯èµ·å§‹æ ‡è®°ï¼Œ`49407` æ˜¯ç»“æŸæ ‡è®°ã€‚
> - **Attention Mask**ï¼šç”¨äºæ ‡è®°å“ªäº› token éœ€è¦æ¨¡å‹çš„å…³æ³¨ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œ0 è¡¨ç¤ºå¡«å……çš„æ— æ•ˆ tokenã€‚
>
> **`padding=False`æ—¶çš„è¾“å‡ºï¼š**
>
> ```python
> Tokens: ['a</w>', 'man</w>', 'in</w>', 'a</w>', 'graphic</w>', 'tee</w>', 'and</w>', 'sport</w>', 'coat</w>', '.</w>']
> Tokenized Input IDs: tensor([[49406,   320,   786,   530,   320,  4245,  3385,   537,  2364,  7356,
>            269, 49407]])
> Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
> ```
>
> æ˜¯ä¸æ˜¯å’Œé¢„æœŸä¸€è‡´å‘¢ï¼Ÿ
>
> æ¥ä¸‹æ¥ï¼Œ`input_ids` å°†è¢«ä¼ å…¥ `text_encoder`ï¼Œç”Ÿæˆæ–‡æœ¬çš„åµŒå…¥å‘é‡ã€‚

### è‡ªå®šä¹‰æ•°æ®é›†

åœ¨è®¤è¯† `transform` å’Œ `tokenizer` ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰è‡ªå·±çš„æ•°æ®é›†ã€‚è¿™ä¸ª `Text2ImageDataset` è´Ÿè´£å°†å›¾åƒå’Œæ–‡æœ¬é…å¯¹ï¼Œå¹¶è¿›è¡Œæ•°æ®çš„é¢„å¤„ç†ï¼Œä»¥ä¾¿è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚

```python
# è¯†åˆ«å›¾ç‰‡åç¼€
IMAGE_EXTENSIONS = [".png", ".jpg", ".jpeg", ".webp", ".bmp", ".PNG", ".JPG", ".JPEG", ".WEBP", ".BMP"]

class Text2ImageDataset(torch.utils.data.Dataset):
    """
    (1) ç›®æ ‡:
        - ç”¨äºæ„å»ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¾®è°ƒæ•°æ®é›†
    """
    def __init__(self, images_folder, captions_folder, transform, tokenizer):
        """
        (2) å‚æ•°:
            - images_folder: str, å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            - captions_folder: str, æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„
            - transform: function, å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º torch.Tensor
            - tokenizer: CLIPTokenizer, å°†æ–‡æœ¬æ ‡æ³¨è½¬ä¸º word ids
        """
        # åˆå§‹åŒ–å›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ‰©å±•åæ‰¾åˆ°æ‰€æœ‰å›¾åƒæ–‡ä»¶
        self.image_paths = []
        for ext in IMAGE_EXTENSIONS:
            self.image_paths.extend(glob.glob(os.path.join(images_folder, f"*{ext}")))
        self.image_paths = sorted(self.image_paths)

        # åŠ è½½å¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨ï¼Œä¾æ¬¡è¯»å–æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­çš„å†…å®¹
        caption_paths = sorted(glob.glob(os.path.join(captions_folder, "*.txt")))
        captions = []
        for p in caption_paths:
            with open(p, "r", encoding="utf-8") as f:
                captions.append(f.readline().strip())

        # ç¡®ä¿å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸€è‡´
        if len(captions) != len(self.image_paths):
            raise ValueError("å›¾åƒæ•°é‡ä¸æ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ã€‚")

        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º word ids
        inputs = tokenizer(
            captions, max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
        )
        self.input_ids = inputs.input_ids
        self.transform = transform

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        input_id = self.input_ids[idx]
        try:
            # åŠ è½½å›¾åƒå¹¶å°†å…¶è½¬æ¢ä¸º RGB æ¨¡å¼ï¼Œç„¶ååº”ç”¨æ•°æ®å¢å¼º
            image = Image.open(img_path).convert("RGB")
            tensor = self.transform(image)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒè·¯å¾„: {img_path}, é”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªå…¨é›¶çš„å¼ é‡å’Œç©ºçš„è¾“å…¥ ID ä»¥é¿å…å´©æºƒ
            tensor = torch.zeros((3, resolution, resolution))
            input_id = torch.zeros_like(input_id)
        
        return tensor, input_id  # è¿”å›å¤„ç†åçš„å›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æ ‡æ³¨

    def __len__(self):
        return len(self.image_paths)
```

**è§£é‡Š**ï¼š

- **`IMAGE_EXTENSIONS`**ï¼šå®šä¹‰å¯æ¥å—çš„å›¾åƒæ–‡ä»¶æ‰©å±•ååˆ—è¡¨ã€‚
- **`__init__` æ–¹æ³•**ï¼š
  - **å›¾åƒè·¯å¾„**ï¼šé€šè¿‡éå†æŒ‡å®šçš„å›¾åƒæ–‡ä»¶å¤¹ï¼Œè·å–æ‰€æœ‰ç¬¦åˆæ‰©å±•åçš„å›¾åƒæ–‡ä»¶è·¯å¾„ï¼Œå¹¶æ’åºã€‚
  - **æ–‡æœ¬æ ‡æ³¨**ï¼šåœ¨æ ‡æ³¨æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾æ‰€æœ‰ `.txt` æ–‡ä»¶ï¼Œè¯»å–å…¶å†…å®¹å¹¶å­˜å‚¨ä¸ºåˆ—è¡¨ã€‚
  - **ä¸€è‡´æ€§æ£€æŸ¥**ï¼šç¡®ä¿å›¾åƒæ•°é‡ä¸æ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸€è‡´ã€‚
  - **æ–‡æœ¬ç¼–ç **ï¼šä½¿ç”¨ `tokenizer` å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º token IDsã€‚
  - **æ•°æ®è½¬æ¢**ï¼šå­˜å‚¨å›¾åƒçš„é¢„å¤„ç†æ–¹æ³• `transform`ã€‚
- **`__getitem__` æ–¹æ³•**ï¼š
  - æ ¹æ®ç´¢å¼•è·å–å›¾åƒè·¯å¾„å’Œå¯¹åº”çš„æ–‡æœ¬ token IDã€‚
  - å°è¯•åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒï¼Œå¤±è´¥æ—¶è¿”å›å…¨é›¶å¼ é‡ã€‚
- **`__len__` æ–¹æ³•**ï¼šè¿”å›æ•°æ®é›†çš„é•¿åº¦ã€‚

## å®šä¹‰å¾®è°ƒç›¸å…³çš„å‡½æ•°

### åŠ è½½ LoRA

> å‰ç½®æ–‡ç« ï¼š
>
> - [04. è®¤è¯† LoRAï¼šä»çº¿æ€§å±‚åˆ°æ³¨æ„åŠ›æœºåˆ¶](../Guide/04.%20è®¤è¯†%20LoRAï¼šä»çº¿æ€§å±‚åˆ°æ³¨æ„åŠ›æœºåˆ¶.md)
> - [14. PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRA](../Guide/14.%20PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨%20LoRA.md)
>
> **LoRAï¼ˆLow-Rank Adaptationï¼‰** æ˜¯ä¸€ç§éå¸¸é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å®šå±‚æ·»åŠ å°çš„ä½ç§©çŸ©é˜µï¼ˆå¯ä»¥è”æƒ³çº¿æ€§ä»£æ•°ä¸­çš„å¥‡å¼‚å€¼åˆ†è§£ï¼‰ï¼Œæ¥å®ç°æ¨¡å‹çš„å¾®è°ƒï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç±» Adapterã€‚
>
> LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤§æ¨¡å‹ä¸­çš„æŸäº›æƒé‡çŸ©é˜µè¿‘ä¼¼ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µè¿›è¡Œæ›´æ–°ï¼Œä»è€Œå¤§å¹…å‡å°‘éœ€è¦å¾®è°ƒçš„å‚æ•°æ•°é‡ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’ŒèŠ‚çœå­˜å‚¨ç©ºé—´ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œæ¨¡å‹è¶Šå¤§ï¼Œå‡å°æ¯”ä¾‹è¶Šå¤¸å¼ ï¼Œå¯¹äº GPT-3ï¼ŒLoRA å¾®è°ƒçš„è®­ç»ƒå‚æ•°é‡ä¸ºåŸæ¥çš„ 1/10000ã€‚

é€šå¸¸ï¼Œåœ¨å¾®è°ƒæ—¶æˆ‘ä»¬åªå¯¹æ¨¡å‹çš„ç‰¹å®šéƒ¨åˆ†ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ Qã€Kã€V çŸ©é˜µï¼‰è¿›è¡Œ LoRA å¾®è°ƒï¼Œè€Œä¸æ˜¯å¾®è°ƒæ•´ä¸ªæ¨¡å‹ã€‚è¿™é‡Œé€‰æ‹©å¯¹ `unet` å’Œ `text_encoder` å¢åŠ  LoRAï¼Œå› ä¸ºè¿™ä¸¤ä¸ªæ¨¡å—ç›´æ¥è´Ÿè´£å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬å¼•å¯¼ä¸­çš„å…³é”®ä»»åŠ¡ï¼š`unet` å¤„ç†æ‰©æ•£è¿‡ç¨‹çš„é€†è¿ç®—ï¼Œ`text_encoder` å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºç‰¹å¾å‘é‡ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åº”ç”¨ LoRA æ¨¡å‹ã€‚

```python
def prepare_lora_model(lora_config, pretrained_model_name_or_path, model_path=None, resume=False, merge_lora=False):
    """
    (1) ç›®æ ‡:
        - åŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ LoRA å±‚ï¼Œå¹¶æ ¹æ®éœ€è¦åˆå¹¶ LoRA æƒé‡ã€‚è¿™åŒ…æ‹¬ Tokenizerã€å™ªå£°è°ƒåº¦å™¨ã€UNetã€VAE å’Œæ–‡æœ¬ç¼–ç å™¨ã€‚

    (2) å‚æ•°:
        - lora_config: LoraConfig, LoRA çš„é…ç½®å¯¹è±¡
        - pretrained_model_name_or_path: str, Hugging Face ä¸Šçš„æ¨¡å‹åç§°æˆ–è·¯å¾„
        - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤
        - merge_lora: bool, æ˜¯å¦åœ¨æ¨ç†æ—¶åˆå¹¶ LoRA æƒé‡

    (3) è¿”å›:
        - tokenizer: CLIPTokenizer
        - noise_scheduler: DDPMScheduler
        - unet: UNet2DConditionModel
        - vae: AutoencoderKL
        - text_encoder: CLIPTextModel
    """
    # åŠ è½½å™ªå£°è°ƒåº¦å™¨ï¼Œç”¨äºæ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å™ªå£°æ·»åŠ å’Œç§»é™¤è¿‡ç¨‹
    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder="scheduler")

    # åŠ è½½ Tokenizerï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º tokens
    tokenizer = CLIPTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="tokenizer"
    )

    # åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸ºç‰¹å¾å‘é‡
    text_encoder = CLIPTextModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="text_encoder"
    )

    # åŠ è½½ VAE æ¨¡å‹ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¤„ç†å›¾åƒçš„æ½œåœ¨è¡¨ç¤º
    vae = AutoencoderKL.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="vae"
    )

    # åŠ è½½ UNet æ¨¡å‹ï¼Œè´Ÿè´£å¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒç”Ÿæˆå’Œæ¨ç†è¿‡ç¨‹
    unet = UNet2DConditionModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="unet"
    )
    
    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡
    if resume:
        if model_path is None or not os.path.exists(model_path):
            raise ValueError("å½“ resume è®¾ç½®ä¸º True æ—¶ï¼Œå¿…é¡»æä¾›æœ‰æ•ˆçš„ model_path")
        # ä½¿ç”¨ PEFT çš„ from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹
        text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, "text_encoder"))
        unet = PeftModel.from_pretrained(unet, os.path.join(model_path, "unet"))

        # ç¡®ä¿ UNet çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True
        for param in unet.parameters():
            if param.requires_grad is False:
                param.requires_grad = True
        
        # ç¡®ä¿æ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True
        for param in text_encoder.parameters():
            if param.requires_grad is False:
                param.requires_grad = True
                
        print(f"âœ… å·²ä» {model_path} æ¢å¤æ¨¡å‹æƒé‡")

    else:
        # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet
        text_encoder = get_peft_model(text_encoder, lora_config)
        unet = get_peft_model(unet, lora_config)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        print("ğŸ“Š Text Encoder å¯è®­ç»ƒå‚æ•°:")
        text_encoder.print_trainable_parameters()
        print("ğŸ“Š UNet å¯è®­ç»ƒå‚æ•°:")
        unet.print_trainable_parameters()
    
    if merge_lora:
        # åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼Œä»…åœ¨æ¨ç†æ—¶è°ƒç”¨
        text_encoder = text_encoder.merge_and_unload()
        unet = unet.merge_and_unload()

        # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
        text_encoder.eval()
        unet.eval()

    # å†»ç»“ VAE å‚æ•°
    vae.requires_grad_(False)

    # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹
    unet.to(DEVICE, dtype=weight_dtype)
    vae.to(DEVICE, dtype=weight_dtype)
    text_encoder.to(DEVICE, dtype=weight_dtype)
    
    return tokenizer, noise_scheduler, unet, vae, text_encoder
```

**è§£é‡Šï¼š**

- **åŠ è½½æ¨¡å‹ç»„ä»¶ï¼š** ä¾æ¬¡åŠ è½½äº†å™ªå£°è°ƒåº¦å™¨ã€Tokenizerã€æ–‡æœ¬ç¼–ç å™¨ï¼ˆ`text_encoder`ï¼‰ã€VAE å’Œ UNet æ¨¡å‹ã€‚
- **åº”ç”¨ LoRAï¼š** ä½¿ç”¨ `get_peft_model` å‡½æ•°å°† LoRA é…ç½®åº”ç”¨åˆ° `text_encoder` å’Œ `unet` æ¨¡å‹ä¸­ã€‚è¿™ä¼šåœ¨æ¨¡å‹ä¸­æ’å…¥å¯è®­ç»ƒçš„ LoRA å±‚ã€‚
- **æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼š** è°ƒç”¨ `print_trainable_parameters()` æ¥æŸ¥çœ‹ LoRA æ·»åŠ äº†å¤šå°‘å¯è®­ç»ƒå‚æ•°ã€‚
- **æ¢å¤è®­ç»ƒï¼š** å¦‚æœè®¾ç½®äº† `resume=True`ï¼Œåˆ™ä»æŒ‡å®šçš„ `model_path` åŠ è½½ä¹‹å‰ä¿å­˜çš„æ¨¡å‹æƒé‡ã€‚
- **åˆå¹¶ LoRA æƒé‡ï¼š** å¦‚æœ `merge_lora=True`ï¼Œåˆ™å°† LoRA çš„æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿åœ¨æ¨ç†æ—¶ä½¿ç”¨ï¼Œæ„Ÿå…´è¶£çš„è¯é˜…è¯»ï¼š[14. PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRA](../Guide/14.%20PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨%20LoRA.md)ã€‚
- **å†»ç»“ VAE å‚æ•°ï¼š** è°ƒç”¨ `vae.requires_grad_(False)` æ¥å†»ç»“ VAE çš„å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒä¸­ä¸æ›´æ–°ã€‚
- **ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡ï¼š** å°†æ‰€æœ‰æ¨¡å‹ç»„ä»¶ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ï¼Œå¹¶è®¾ç½®æ•°æ®ç±»å‹ã€‚

**ä¸ºä»€ä¹ˆåªå¾®è°ƒ `unet` å’Œ `text_encoder` æœ€ç»ˆå´è¿”å›è¿™ä¹ˆå¤šæ¨¡å—ï¼Ÿ**

å› ä¸ºåœ¨åé¢çš„å¾®è°ƒä¸­ï¼Œæˆ‘ä»¬å°†ä»æ–‡æœ¬å¼€å§‹å¤„ç†è€Œéå°†å…¶å½“ä½œåˆä¸€ä¸ªé»‘ç›’ã€‚

### å‡†å¤‡ä¼˜åŒ–å™¨

æ¥ä¸‹æ¥ï¼Œéœ€è¦å¯¹äºåº”ç”¨äº† LoRA çš„ UNet å’Œæ–‡æœ¬ç¼–ç å™¨ï¼ˆ`text_encoder`ï¼‰åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼Œè¿™ä¹Ÿæ˜¯ç‚¼ä¸¹ç‚‰ UI ä¸­å¸¸éœ€è¦è°ƒèŠ‚çš„é€‰é¡¹ã€‚

```python
def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):
    """
    (1) ç›®æ ‡:
        - ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°åˆ†åˆ«è®¾ç½®ä¼˜åŒ–å™¨ï¼Œå¹¶æŒ‡å®šä¸åŒçš„å­¦ä¹ ç‡ã€‚

    (2) å‚æ•°:
        - unet: UNet2DConditionModel, Hugging Face çš„ UNet æ¨¡å‹
        - text_encoder: CLIPTextModel, Hugging Face çš„æ–‡æœ¬ç¼–ç å™¨
        - unet_learning_rate: float, UNet çš„å­¦ä¹ ç‡
        - text_encoder_learning_rate: float, æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡

    (3) è¿”å›:
        - è¾“å‡º: ä¼˜åŒ–å™¨ Optimizer
    """
    # ç­›é€‰å‡º UNet ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°
    unet_lora_layers = [p for p in unet.parameters() if p.requires_grad]
    
    # ç­›é€‰å‡ºæ–‡æœ¬ç¼–ç å™¨ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°
    text_encoder_lora_layers = [p for p in text_encoder.parameters() if p.requires_grad]
    
    # å°†éœ€è¦è®­ç»ƒçš„å‚æ•°åˆ†ç»„å¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    trainable_params = [
        {"params": unet_lora_layers, "lr": unet_learning_rate},
        {"params": text_encoder_lora_layers, "lr": text_encoder_learning_rate}
    ]
    
    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(trainable_params)
    
    return optimizer
```

### å®šä¹‰ `collate_fn` å‡½æ•°

åœ¨å¤§å¤šæ•°å¸¸è§çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚å›¾åƒåˆ†ç±»æˆ–å›å½’ï¼‰ï¼Œæ•°æ®é›†é€šå¸¸æ˜¯ç®€å•çš„ `(data, label)` ç»“æ„ï¼ŒPyTorch çš„ `DataLoader` é»˜è®¤èƒ½å¤Ÿå¤„ç†è¿™æ ·çš„ç®€å•æ•°æ®ç»“æ„ï¼Œå°†æ ·æœ¬æ‰“åŒ…æˆæ‰¹æ¬¡ï¼ˆbatchï¼‰ã€‚åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬ä¹Ÿæ˜¯ä¸€ä¸ªåŒ…å«å›¾åƒå¼ é‡å’Œæ–‡æœ¬ç¼–ç çš„å…ƒç»„ `(tensor, input_id)`ã€‚é»˜è®¤çš„ `collate_fn` å¯ä»¥å°†è¿™äº›æ ·æœ¬æ‰“åŒ…æˆæ‰¹æ¬¡ï¼Œè®¿é—®æ—¶éœ€è¦ä½¿ç”¨ç´¢å¼•ï¼Œä¾‹å¦‚ `batch[0]` å’Œ `batch[1]`ã€‚

ä¸ºäº†ä½¿ä»£ç æ›´å…·å¯è¯»æ€§ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ª `collate_fn` å‡½æ•°ï¼Œå°†æ‰¹æ¬¡æ•°æ®ç»„ç»‡æˆå­—å…¸çš„å½¢å¼ï¼Œæ–¹ä¾¿é€šè¿‡é”®åç›´æ¥è®¿é—®ï¼Œä¾‹å¦‚ `batch["pixel_values"]` å’Œ `batch["input_ids"]`ã€‚è‡ªå®šä¹‰çš„ `collate_fn` å®šä¹‰å¦‚ä¸‹ï¼š

```python
def collate_fn(examples):
    pixel_values = []
    input_ids = []
    
    for tensor, input_id in examples:
        pixel_values.append(tensor)
        input_ids.append(input_id)
    
    pixel_values = torch.stack(pixel_values, dim=0).float()
    input_ids = torch.stack(input_ids, dim=0)
    
    # å¦‚æœä½ å–œæ¬¢åˆ—è¡¨æ¨å¯¼å¼çš„è¯ï¼Œä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•
    #pixel_values = torch.stack([example[0] for example in examples], dim=0).float()
    #input_ids = torch.stack([example[1] for example in examples], dim=0)
    return {"pixel_values": pixel_values, "input_ids": input_ids}
```

**è§£é‡Šï¼š**

- **`examples` æ˜¯ä»€ä¹ˆï¼Ÿ**
  - `examples` æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†ä¸€ä¸ªæ‰¹æ¬¡ä¸­çš„å¤šä¸ªæ ·æœ¬ã€‚
  - å…¶ä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ä»æˆ‘ä»¬è‡ªå®šä¹‰çš„ `Text2ImageDataset` æ•°æ®é›†ä¸­è·å–çš„ï¼Œå½¢å¼ä¸º `(tensor, input_id)`ã€‚
    - `tensor`ï¼šç»è¿‡é¢„å¤„ç†çš„å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º `(C, H, W)`ï¼Œå³é€šé“æ•°ï¼ˆChannelï¼‰å’Œå›¾åƒçš„é«˜åº¦ï¼ˆHeightï¼‰ã€å®½åº¦ï¼ˆWeightï¼‰ã€‚
    - `input_id`ï¼šå¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨ç»è¿‡ `tokenizer` ç¼–ç åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º `(sequence_length,)`ã€‚

**è¡¥å……**ï¼šPyTorch çš„ `torch.stack()` å‡½æ•°ä¼šå°†å¤šä¸ªå¼ é‡æ²¿æ–°ç»´åº¦æ‹¼æ¥åœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œå°†ä¸€æ‰¹å›¾åƒå¼ é‡æ‹¼æ¥æˆ `(batch_size, C, H, W)` çš„å½¢å¼ï¼Œç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡æ•°æ®çš„ç»„ç»‡ç»“æ„ä¸€è‡´ã€‚

> **æ‹“å±•ï¼šè‡ªå®šä¹‰å’Œé»˜è®¤ `collate_fn` çš„å¯¹æ¯”**
>
> ä¸‹é¢æä¾›äº†ä¸€ä¸ªå¯¹æ¯”å‡½æ•°ï¼Œæ¥å±•ç¤ºè‡ªå®šä¹‰ `collate_fn` å’Œé»˜è®¤ `collate_fn` åœ¨å¤„ç†å½“å‰æ•°æ®æ—¶çš„ä¸åŒã€‚ä½ å¯ä»¥é€šè¿‡è¿è¡Œä»£ç æ¥è§‚å¯Ÿè‡ªå®šä¹‰å’Œé»˜è®¤æ–¹å¼çš„ä½¿ç”¨å·®å¼‚ã€‚
>
> ```python
> import torch
> from torch.utils.data import DataLoader, Dataset
> 
> def compare_dataloaders(dataset, batch_size):
>     # ç¬¬ä¸€ç§æƒ…å†µï¼šä½¿ç”¨è‡ªå®šä¹‰çš„ collate_fn
>     train_dataloader_custom = DataLoader(
>        dataset,
>        shuffle=True,
>        collate_fn=collate_fn,  # ä½¿ç”¨è‡ªå®šä¹‰çš„ collate_fn
>        batch_size=batch_size,
>     )
>     
>     # ç¬¬äºŒç§æƒ…å†µï¼šä¸ä½¿ç”¨è‡ªå®šä¹‰çš„ collate_fnï¼ˆé»˜è®¤æ–¹å¼ï¼‰
>     train_dataloader_default = DataLoader(
>        dataset,
>        shuffle=True,
>        batch_size=batch_size,
>     )
>     
>     # ä»æ¯ä¸ªæ•°æ®åŠ è½½å™¨ä¸­å–ä¸€ä¸ªæ‰¹æ¬¡è¿›è¡Œå¯¹æ¯”
>     custom_batch = next(iter(train_dataloader_custom))
>     default_batch = next(iter(train_dataloader_default))
>     
>     # æ‰“å°è‡ªå®šä¹‰ collate_fn çš„è¾“å‡ºç»“æœ
>     print("ä½¿ç”¨è‡ªå®šä¹‰ collate_fn:")
>     print("æ‰¹æ¬¡çš„ç±»å‹:", type(custom_batch))
>     print("æ‰¹æ¬¡ pixel_values çš„å½¢çŠ¶:", custom_batch["pixel_values"].shape)
>     print("æ‰¹æ¬¡ input_ids çš„å½¢çŠ¶:", custom_batch["input_ids"].shape)
>     
>     # æ‰“å°é»˜è®¤ DataLoader çš„è¾“å‡ºç»“æœ
>     print("\nä½¿ç”¨é»˜è®¤ collate_fn:")
>     print("æ‰¹æ¬¡çš„ç±»å‹:", type(default_batch))
>     
>     pixel_values, input_ids = default_batch
>     print("æ‰¹æ¬¡ pixel_values çš„å½¢çŠ¶:", pixel_values.shape)
>     print("æ‰¹æ¬¡ input_ids çš„å½¢çŠ¶:", input_ids.shape)
>     
>     return custom_batch, default_batch
>        
> # å¯¹æ¯”
> custom_batch, default_batch = compare_dataloaders(dataset, batch_size=2)
> ```
>
> 
>
> **è¾“å‡º**ï¼š
>
> ```python
> ä½¿ç”¨è‡ªå®šä¹‰ collate_fn:
> æ‰¹æ¬¡çš„ç±»å‹: <class 'dict'>
> æ‰¹æ¬¡ pixel_values çš„å½¢çŠ¶: torch.Size([2, 3, 224, 224])
> æ‰¹æ¬¡ input_ids çš„å½¢çŠ¶: torch.Size([2, 16])
> 
> ä½¿ç”¨é»˜è®¤ collate_fn:
> æ‰¹æ¬¡çš„ç±»å‹: <class 'list'>
> æ‰¹æ¬¡ pixel_values çš„å½¢çŠ¶: torch.Size([2, 3, 224, 224])
> æ‰¹æ¬¡ input_ids çš„å½¢çŠ¶: torch.Size([2, 16])
> ```
> å…·ä½“é€‰æ‹©å“ªä¸€ç§ç”±ä½ å†³å®šï¼Œé»˜è®¤çš„æ–¹æ³•å®é™…ä¸Šæ›´æ™®éã€‚

## è®¾ç½®ç›¸å…³å‚æ•°

### è®¾å¤‡é…ç½®

å½“å‰çš„å¾®è°ƒæ¯«æ— ç–‘é—®éœ€è¦ç”¨åˆ°æ˜¾å¡ï¼ˆGPUï¼‰ï¼Œå¯¹äº Apple èŠ¯ç‰‡çš„ Mac æ¥è¯´ï¼ŒæŠŠ "cuda" æ”¹ä¸º "mps"ï¼Œä¹Ÿå°±æ˜¯ä½¿ç”¨ç¬¬äºŒè¡Œä»£ç ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºPyTorchç‰ˆæœ¬è¿‡ä½çš„ç¯å¢ƒï¼Œ `torch.backends.mps.is_available()` ä¼šæŠ¥é”™ï¼Œæ‰€ä»¥è¿™é‡Œé€‰æ‹©æ³¨é‡Šã€‚

```python
# è®¾å¤‡é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# For Mac M1, M2...
# DEVICE = torch.device("mps" if torch.backends.mps.is_available() else ("cuda" if torch.cuda.is_available() else "cpu"))

print(f"ğŸ–¥ å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")
```

### æ¨¡å‹ä¸è®­ç»ƒå‚æ•°é…ç½®

è¿™é‡Œçš„å‚æ•°å¤§å¤šä¸ä¹‹å‰çš„å‡½æ•°ç›¸å…³ï¼Œä¸‹é¢æ˜¯ä½ å¯ä»¥è°ƒèŠ‚çš„å†…å®¹ï¼š

- **è®­ç»ƒå‚æ•°**ï¼šè®¾ç½®æ‰¹æ¬¡å¤§å°ã€æ•°æ®ç±»å‹ã€éšæœºç§å­ç­‰ã€‚
  - `train_batch_size = 2` æ—¶ï¼Œå¾®è°ƒæ˜¾å­˜è¦æ±‚ä¸º 5Gï¼Œåœ¨å‘½ä»¤è¡Œè¾“å…¥ `nvidia-smi` å¯ä»¥æŸ¥çœ‹å½“å‰æ˜¾å­˜å ç”¨ã€‚
- **ä¼˜åŒ–å™¨å‚æ•°**ï¼šä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨åˆ†åˆ«è®¾ç½®å­¦ä¹ ç‡ã€‚
- **å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼šé€‰æ‹© `cosine_with_restarts` è°ƒåº¦å™¨ï¼Œè¿™ä¸€ç‚¹ä¸€èˆ¬æ— å…³ç´§è¦ã€‚
- **é¢„è®­ç»ƒæ¨¡å‹**ï¼šæŒ‡å®šé¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ã€‚
- **LoRA é…ç½®**ï¼šè®¾ç½® LoRA çš„ç›¸å…³å‚æ•°ï¼Œå¦‚ç§© `r`ã€`lora_alpha`ã€åº”ç”¨æ¨¡å—ç­‰ã€‚

```python
# è®­ç»ƒç›¸å…³å‚æ•°
train_batch_size = 2  # è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œå³æ¯æ¬¡è®­ç»ƒä¸­å¤„ç†çš„æ ·æœ¬æ•°é‡
weight_dtype = torch.bfloat16  # æƒé‡æ•°æ®ç±»å‹ï¼Œä½¿ç”¨ bfloat16 ä»¥èŠ‚çœå†…å­˜å¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦
snr_gamma = 5  # SNR å‚æ•°ï¼Œç”¨äºä¿¡å™ªæ¯”åŠ æƒæŸå¤±çš„è°ƒèŠ‚ç³»æ•°

# è®¾ç½®éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
seed = 1126  # éšæœºæ•°ç§å­
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)
    
# Stable Diffusion LoRA çš„å¾®è°ƒå‚æ•°

# ä¼˜åŒ–å™¨å‚æ•°
unet_learning_rate = 1e-4  # UNet çš„å­¦ä¹ ç‡ï¼Œæ§åˆ¶ UNet å‚æ•°æ›´æ–°çš„æ­¥é•¿
text_encoder_learning_rate = 1e-4  # æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ–‡æœ¬åµŒå…¥å±‚çš„å‚æ•°æ›´æ–°æ­¥é•¿

# å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
lr_scheduler_name = "cosine_with_restarts"  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ä¸º Cosine annealing with restartsï¼Œé€æ¸å‡å°‘å­¦ä¹ ç‡å¹¶å®šæœŸé‡å¯
lr_warmup_steps = 100  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œåœ¨æœ€åˆçš„ 100 æ­¥ä¸­é€æ¸å¢åŠ å­¦ä¹ ç‡åˆ°æœ€å¤§å€¼
max_train_steps = 2000  # æ€»è®­ç»ƒæ­¥æ•°ï¼Œå†³å®šäº†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„è¿­ä»£æ¬¡æ•°
num_cycles = 3  # Cosine è°ƒåº¦å™¨çš„å‘¨æœŸæ•°é‡ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¼šé‡å¤ 3 æ¬¡å­¦ä¹ ç‡å‘¨æœŸæ€§é€’å‡å¹¶é‡å¯

# é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹è·¯å¾„ï¼Œç”¨äºåŠ è½½æ¨¡å‹è¿›è¡Œå¾®è°ƒ
pretrained_model_name_or_path = "stablediffusionapi/cyberrealistic-41"  

# LoRA é…ç½®
lora_config = LoraConfig(
    r=32,  # LoRA çš„ç§©ï¼Œå³ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œå†³å®šäº†å‚æ•°è°ƒæ•´çš„è‡ªç”±åº¦
    lora_alpha=16,  # ç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶ LoRA æƒé‡å¯¹æ¨¡å‹çš„å½±å“
    target_modules=[
        "q_proj", "v_proj", "k_proj", "out_proj",  # æŒ‡å®š Text encoder çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŠ•å½±çŸ©é˜µï¼‰
        "to_k", "to_q", "to_v", "to_out.0"  # æŒ‡å®š UNet çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´ UNet ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰
    ],
    lora_dropout=0  # LoRA dropout æ¦‚ç‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨ dropout
)
```

## å¾®è°ƒå‰çš„å‡†å¤‡

### å‡†å¤‡æ•°æ®é›†

```python
# åˆå§‹åŒ– tokenizer
tokenizer = CLIPTokenizer.from_pretrained(
    pretrained_model_name_or_path,
    subfolder="tokenizer"
)

# å‡†å¤‡æ•°æ®é›†
dataset = Text2ImageDataset(
    images_folder=images_folder,
    captions_folder=captions_folder,
    transform=train_transform,
    tokenizer=tokenizer,
)

train_dataloader = torch.utils.data.DataLoader(
    dataset,
    shuffle=True,
    collate_fn=collate_fn,  # ä¹‹å‰å®šä¹‰çš„collate_fn()
    batch_size=train_batch_size,
    num_workers=8,
)

print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
```

**è§£é‡Šï¼š**

- **åŠ è½½ Tokenizerï¼š** ä½¿ç”¨ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸åŒçš„ Tokenizerã€‚
- **åˆ›å»ºæ•°æ®é›†ï¼š** ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„ `Text2ImageDataset`ã€‚
- **åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼š** ä½¿ç”¨ PyTorch çš„ `DataLoader`ã€‚

### å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨

```python
# å‡†å¤‡æ¨¡å‹
tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(
    lora_config,
    pretrained_model_name_or_path,
    model_path,
    resume=False,
    merge_lora=False
)

# å‡†å¤‡ä¼˜åŒ–å™¨
optimizer = prepare_optimizer(
    unet, 
    text_encoder, 
    unet_learning_rate=unet_learning_rate, 
    text_encoder_learning_rate=text_encoder_learning_rate
)

# è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
lr_scheduler = get_scheduler(
    lr_scheduler_name,
    optimizer=optimizer,
    num_warmup_steps=lr_warmup_steps,
    num_training_steps=max_train_steps,
    num_cycles=num_cycles
)

print("âœ… æ¨¡å‹å’Œä¼˜åŒ–å™¨å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
```

**è§£é‡Šï¼š**

- **å‡†å¤‡æ¨¡å‹ï¼š** è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ `prepare_lora_model` å‡½æ•°ã€‚
- **å‡†å¤‡ä¼˜åŒ–å™¨ï¼š** è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ `prepare_optimizer` å‡½æ•°ã€‚
- **è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š** ä½¿ç”¨ Hugging Face çš„ `get_scheduler` å‡½æ•°ã€‚

## å¼€å§‹å¾®è°ƒ

ä¸»è¦æµç¨‹å’Œç»“æ„å¦‚ä¸‹ï¼š

- **è®­ç»ƒå¾ªç¯ï¼š** æˆ‘ä»¬åœ¨å¤šä¸ª `epoch` ä¸­è¿›è¡Œè®­ç»ƒï¼Œç›´åˆ°è¾¾åˆ° `max_train_steps`ã€‚æ¯ä¸ª `epoch` ä»£è¡¨ä¸€è½®æ•°æ®çš„å®Œæ•´è®­ç»ƒï¼Œåœ¨å¸¸è§çš„ UI ç•Œé¢ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ° `epoch` å’Œ `max_train_steps` çš„å‚æ•°ã€‚
- **ç¼–ç å›¾åƒï¼š** ä½¿ç”¨ VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰å°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatent spaceï¼‰ï¼Œä»¥ä¾¿åç»­åœ¨æ‰©æ•£æ¨¡å‹ä¸­æ·»åŠ å™ªå£°å¹¶è¿›è¡Œå¤„ç†ã€‚
- **æ·»åŠ å™ªå£°ï¼š** ä½¿ç”¨å™ªå£°è°ƒåº¦å™¨ï¼ˆ`noise_scheduler`ï¼‰ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ éšæœºå™ªå£°ï¼Œæ¨¡æ‹Ÿå›¾åƒä»æ¸…æ™°åˆ°å™ªå£°çš„é€€åŒ–è¿‡ç¨‹ã€‚è¿™æ˜¯æ‰©æ•£æ¨¡å‹çš„å…³é”®æ­¥éª¤ï¼Œè®­ç»ƒæ—¶æ¨¡å‹é€šè¿‡å­¦ä¹ å¦‚ä½•è¿˜åŸå™ªå£°ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡é€æ­¥å»å™ªç”Ÿæˆæ¸…æ™°çš„å›¾åƒã€‚
- **è·å–æ–‡æœ¬åµŒå…¥ï¼š** ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨ï¼ˆ`text_encoder`ï¼‰å°†è¾“å…¥çš„æ–‡æœ¬ prompt è½¬æ¢ä¸ºéšè—çŠ¶æ€ï¼ˆæˆ‘ä»¬è§è¿‡å¾ˆå¤šç±»ä¼¼çš„è¡¨è¾¾ï¼šéšè—å‘é‡/ç‰¹å¾å‘é‡/embedding/...ï¼‰ï¼Œä¸ºå›¾åƒç”Ÿæˆæä¾›æ–‡æœ¬å¼•å¯¼ä¿¡æ¯ã€‚
- **è®¡ç®—ç›®æ ‡å€¼ï¼š** æ ¹æ®æ‰©æ•£æ¨¡å‹çš„ç±»å‹ï¼ˆ`epsilon` æˆ– `v_prediction`ï¼‰ï¼Œç¡®å®šæ¨¡å‹çš„ç›®æ ‡è¾“å‡ºï¼ˆå™ªå£°æˆ–é€Ÿåº¦å‘é‡ï¼‰ã€‚
- **UNet é¢„æµ‹ï¼š** ä½¿ç”¨ UNet æ¨¡å‹å¯¹å¸¦å™ªå£°çš„æ½œåœ¨è¡¨ç¤ºè¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆçš„è¾“å‡ºç”¨äºè¿˜åŸå™ªå£°æˆ–é¢„æµ‹é€Ÿåº¦å‘é‡ã€‚
- **è®¡ç®—æŸå¤±ï¼š** é€šè¿‡åŠ æƒå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è®¡ç®—æ¨¡å‹æŸå¤±ï¼Œå¹¶è¿›è¡Œåå‘ä¼ æ’­ã€‚
- **ä¼˜åŒ–ä¸ä¿å­˜**ï¼šé€šè¿‡ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨é€‚å½“æ—¶ä¿å­˜æ£€æŸ¥ç‚¹ã€‚

```python
# ç¦ç”¨å¹¶è¡ŒåŒ–ï¼Œé¿å…è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åˆå§‹åŒ–
global_step = 0
best_face_score = float("inf")  # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§ï¼Œå­˜å‚¨æœ€ä½³é¢éƒ¨ç›¸ä¼¼åº¦åˆ†æ•°

# è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
progress_bar = tqdm(
    range(max_train_steps),  # æ ¹æ® num_training_steps è®¾ç½®
    desc="è®­ç»ƒæ­¥éª¤",
)

# è®­ç»ƒå¾ªç¯
for epoch in range(math.ceil(max_train_steps / len(train_dataloader))):
    # å¦‚æœä½ æƒ³åœ¨è®­ç»ƒä¸­å¢åŠ è¯„ä¼°ï¼Œé‚£åœ¨å¾ªç¯ä¸­å¢åŠ  train() æ˜¯æœ‰å¿…è¦çš„
    unet.train()
    text_encoder.train()
    
    for step, batch in enumerate(train_dataloader):
        if global_step >= max_train_steps:
            break
        
        # ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰
        latents = vae.encode(batch["pixel_values"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()
        latents = latents * vae.config.scaling_factor  # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨ç©ºé—´

        # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ
        noise = torch.randn_like(latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()
        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

        # è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º
        encoder_hidden_states = text_encoder(batch["input_ids"].to(DEVICE))[0]

        # è®¡ç®—ç›®æ ‡å€¼
        if noise_scheduler.config.prediction_type == "epsilon":
            target = noise  # é¢„æµ‹å™ªå£°
        elif noise_scheduler.config.prediction_type == "v_prediction":
            target = noise_scheduler.get_velocity(latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡

        # UNet æ¨¡å‹é¢„æµ‹
        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states)[0]

        # è®¡ç®—æŸå¤±
        if not snr_gamma:
            loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
        else:
            # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±
            snr = compute_snr(noise_scheduler, timesteps)
            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]
            if noise_scheduler.config.prediction_type == "epsilon":
                mse_loss_weights = mse_loss_weights / snr
            elif noise_scheduler.config.prediction_type == "v_prediction":
                mse_loss_weights = mse_loss_weights / (snr + 1)
            
            # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±
            loss = F.mse_loss(model_pred.float(), target.float(), reduction="none")
            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
            loss = loss.mean()

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
        global_step += 1

        # æ‰“å°è®­ç»ƒæŸå¤±
        if global_step % 100 == 0 or global_step == max_train_steps:
            print(f"ğŸ”¥ æ­¥éª¤ {global_step}, æŸå¤±: {loss.item()}")

        # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œå½“å‰ç®€å•è®¾ç½®ä¸ºæ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡
        if global_step % 500 == 0:
            save_path = os.path.join(output_folder, f"checkpoint-{global_step}")
            os.makedirs(save_path, exist_ok=True)

            # ä½¿ç”¨ save_pretrained ä¿å­˜ PeftModel
            unet.save_pretrained(os.path.join(save_path, "unet"))
            text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
            print(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´æ¨¡å‹åˆ° {save_path}")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° checkpoint-last
save_path = os.path.join(output_folder, "checkpoint-last")
os.makedirs(save_path, exist_ok=True)
unet.save_pretrained(os.path.join(save_path, "unet"))
text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
print(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {save_path}")

print("ğŸ‰ å¾®è°ƒå®Œæˆï¼")
```

è®­ç»ƒå®Œæˆåçš„ `checkpoint` ä¼šä¿å­˜åˆ° `./SD/Brad/logs/checkpoint-last` ä¸­ï¼Œä»¥ `max_train_steps=200` ä¸ºä¾‹ï¼Œæ¨¡å‹è¾“å‡ºå¦‚ä¸‹ï¼š

![image-20240929230118158](./assets/image-20240929230118158.png)

## ç”Ÿæˆå›¾åƒå’Œè¯„ä¼°

### ä»€ä¹ˆæ˜¯ `pipeline`ï¼Ÿ

`pipeline` æ˜¯ Hugging Face åº“ä¸­ä¸€ç§é«˜å±‚æ¬¡çš„å°è£…å·¥å…·ï¼Œé€šå¸¸ç”¨äºæ¨ç†ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`pipeline` ä»¥ **eval æ¨¡å¼**åŠ è½½æ¨¡å‹ï¼Œå› æ­¤é€‚åˆç”¨äºç”Ÿæˆæˆ–è¯„ä¼°åœºæ™¯ã€‚æˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ˜¯ `Diffusers.DiffusionPipeline`ï¼Œå®ƒå°†ä¹‹å‰æåˆ°çš„å¤šä¸ªæ¨¡å‹ç»„ä»¶ï¼ˆå¦‚ UNetã€VAEã€æ–‡æœ¬ç¼–ç å™¨ç­‰ï¼‰ç»„åˆåœ¨ä¸€èµ·ï¼Œå®ç°ä»æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆã€‚

`pipeline` çš„å·¥ä½œåŸç†ä¹Ÿè·Ÿä¹‹å‰å¾®è°ƒè¿‡ç¨‹ç±»ä¼¼ï¼š

1. **æ–‡æœ¬ç¼–ç **ï¼š`pipeline` ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ä¼šå°†è¾“å…¥çš„ `prompt` è½¬æ¢ä¸ºç‰¹å¾å‘é‡ã€‚
2. **å™ªå£°æ³¨å…¥**ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­ï¼Œæ¨¡å‹ä»éšæœºå™ªå£°å¼€å§‹ç”Ÿæˆå›¾åƒã€‚
3. **è¿­ä»£å»å™ª**ï¼šUNet ä½¿ç”¨ä»æ–‡æœ¬ç¼–ç å™¨å¾—åˆ°çš„ç‰¹å¾å‘é‡æŒ‡å¯¼å»å™ªè¿‡ç¨‹ï¼Œé€æ­¥å°†å™ªå£°è¿˜åŸä¸ºé«˜è´¨é‡å›¾åƒã€‚
4. **å›¾åƒè§£ç **ï¼šæœ€ç»ˆï¼ŒVAE å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå®é™…çš„å›¾åƒã€‚

### æ¨ç†ç›¸å…³çš„å‚æ•°

1. **ä»€ä¹ˆæ˜¯æ¨ç†æ­¥æ•°ï¼ˆ`num_inference_steps`ï¼‰ï¼Ÿ**

   - æ¨ç†æ­¥æ•°æ§åˆ¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒæ—¶çš„å»å™ªè¿­ä»£æ¬¡æ•°ã€‚æ­¥æ•°è¶Šå¤šï¼Œç”Ÿæˆçš„å›¾åƒè´¨é‡è¶Šé«˜ï¼Œä½†æ¨ç†æ—¶é—´ä¹Ÿç›¸åº”å¢åŠ ã€‚è¿™æ˜¯ä¸€ä¸ªéœ€è¦ä½ æ ¹æ®å›¾åƒè´¨é‡å’Œæ—¶é—´éœ€æ±‚å»æƒè¡¡çš„å‚æ•°ï¼Œé€šå¸¸åœ¨è‚‰çœ¼è§‰å¾—å¤Ÿå¥½çš„æ—¶å€™ï¼Œå°±å¯ä»¥äº†ã€‚
   
2. **å¦‚ä½•å†³å®š `prompt` çš„å½±å“ç¨‹åº¦ï¼ˆ`guidance_scale`ï¼‰ï¼Ÿ**

   - `guidance_scale` å†³å®šäº†æ–‡æœ¬æç¤ºå¯¹ç”Ÿæˆå›¾åƒçš„å½±å“ç¨‹åº¦ã€‚è¾ƒé«˜çš„ `guidance_scale` ä¼šè®©æ¨¡å‹æ›´ä¸¥æ ¼åœ°æŒ‰ç…§ `prompt` ç”Ÿæˆå›¾åƒï¼Œæ•°å€¼é€šå¸¸åœ¨ 7.5 åˆ° 10 ä¹‹é—´è°ƒæ•´ï¼Œè¿‡é«˜å¯èƒ½ä¼šå¯¼è‡´å›¾åƒå¤±çœŸï¼ŒåŒæ ·éœ€è¦ä½ å»æƒè¡¡ã€‚è¿™ä¸ªå‚æ•°ä¸æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„ `temperature` å‚æ•°ç±»ä¼¼ï¼Œé€‚ç”¨äºä¸åŒåœºæ™¯ã€‚
   
3. **æ€ä¹ˆç¡®ä¿ç›¸åŒ `prompt` ç”Ÿæˆç›¸åŒçš„å›¾åƒï¼Ÿ**

   - è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­ï¼ˆseedï¼‰ï¼Œå¯ä»¥ç¡®ä¿åŒæ ·çš„ `prompt` åœ¨æ¯æ¬¡è¿è¡Œæ—¶ç”Ÿæˆç›¸åŒçš„å›¾åƒã€‚å¯ä»¥é€šè¿‡ä½¿ç”¨ `torch.Generator` ç”Ÿæˆéšæœºæ•°å¹¶è®¾ç½®ç§å­ï¼ˆseedï¼‰ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

    ```python
    generator = torch.Generator().manual_seed(42)
    ```


### åŠ è½½ç”¨äºéªŒè¯çš„ prompts

è¿™æ˜¯ä¸€ç»„ç”¨äºç”Ÿå›¾çš„æ–‡æœ¬æç¤ºï¼ˆpromptsï¼‰ï¼Œæœ¬å®éªŒä¸­ä½äº`./SD/Datasets/prompts/validation_prompt.txt`ï¼Œä¸‹é¢æ‘˜å–å‡ è¡Œ prompt é¢„è§ˆï¼š

- A man in a black hoodie and khaki pants.
- A man sports a red polo and denim jacket.
- A man wears a blue shirt and brown blazer.
- ...

å®šä¹‰åŠ è½½ `prompts` çš„å‡½æ•°å¦‚ä¸‹ï¼š

```python
def load_validation_prompts(validation_prompt_path):
    """
    (1) ç›®æ ‡:
        - åŠ è½½éªŒè¯æç¤ºæ–‡æœ¬ã€‚

    (2) å‚æ•°:
        - validation_prompt_path: str, éªŒè¯æç¤ºæ–‡ä»¶çš„è·¯å¾„

    (3) è¿”å›:
        - validation_prompt: list, éªŒè¯æç¤ºçš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªprompt
    """
    with open(validation_prompt_path, "r", encoding="utf-8") as f:
        validation_prompt = [line.strip() for line in f.readlines()]
    return validation_prompt
```

### å®šä¹‰ç”Ÿæˆå›¾åƒçš„å‡½æ•°

ç»“åˆä¹‹å‰çš„è®¨è®ºï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç”Ÿæˆå›¾åƒçš„å‡½æ•°ï¼š

```python
def generate_images(pipeline, prompts, num_inference_steps=50, guidance_scale=7.5, output_folder="inference", generator=None):
    """
    (1) ç›®æ ‡:
        - ä½¿ç”¨ DiffusionPipeline ç”Ÿæˆå›¾åƒï¼Œä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹å¹¶è¿”å›ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ã€‚

    (2) å‚æ•°:
        - pipeline: DiffusionPipeline, å·²åŠ è½½å¹¶é…ç½®å¥½çš„ Pipeline
        - prompts: list, æ–‡æœ¬æç¤ºåˆ—è¡¨
        - num_inference_steps: int, æ¨ç†æ­¥éª¤æ•°ï¼Œè¶Šé«˜å›¾åƒè´¨é‡è¶Šå¥½ï¼Œä½†æ¨ç†æ—¶é—´ä¹Ÿä¼šå¢åŠ 
        - guidance_scale: float, å†³å®šæ–‡æœ¬æç¤ºå¯¹ç”Ÿæˆå›¾åƒçš„å½±å“ç¨‹åº¦
        - output_folder: str, ä¿å­˜ç”Ÿæˆå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„
        - generator: torch.Generator, æ§åˆ¶ç”Ÿæˆéšæœºæ•°çš„ç§å­ï¼Œç¡®ä¿å›¾åƒç”Ÿæˆçš„ä¸€è‡´æ€§ã€‚å¦‚æœä¸æä¾›ï¼Œç”Ÿæˆçš„å›¾åƒæ¯æ¬¡å¯èƒ½ä¸åŒ

    (3) è¿”å›:
        - ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ï¼ŒåŒæ—¶å›¾åƒä¹Ÿä¼šä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ã€‚
    """
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
    os.makedirs(output_folder, exist_ok=True)
    generated_images = []
    
    for i, prompt in enumerate(tqdm(prompts, desc="ç”Ÿæˆå›¾åƒä¸­")):
        # ä½¿ç”¨ pipeline ç”Ÿæˆå›¾åƒ
        image = pipeline(prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=generator).images[0]
        
        # ä¿å­˜å›¾åƒåˆ°æŒ‡å®šæ–‡ä»¶å¤¹
        save_file = os.path.join(output_folder, f"generated_{i+1}.png")
        image.save(save_file)
        
        # å°†å›¾åƒä¿å­˜åˆ°åˆ—è¡¨ä¸­ï¼Œç¨åè¿”å›
        generated_images.append(image)
    
    print(f"âœ… å·²ç”Ÿæˆå¹¶ä¿å­˜ {len(prompts)} å¼ å›¾åƒåˆ° {output_folder}")
    
    return generated_images
```

### å®šä¹‰è¯„ä¼°å‡½æ•°

è™½ç„¶å›¾åƒç”Ÿæˆçš„å¥½ä¸åç°åœ¨æ›´å¤šçš„ç”±äººå»åˆ¤æ–­ï¼Œä½†æœ€åŸºç¡€çš„æ¨¡å—è¿˜æ˜¯å¯ä»¥äº¤ç»™æœºå™¨ï¼Œä»¥å½“å‰å®éªŒä¸ºä¾‹ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯ â€œAI æ¢è„¸â€ï¼Œé‚£å°±å¯ä»¥æœ‰ä¸¤ä¸ªæ–°çš„åº¦é‡ï¼š

- **æ— è„¸å›¾åƒçš„æ•°é‡**

  - ä½¿ç”¨ `DeepFace` åº“æ£€æµ‹ç”Ÿæˆå›¾åƒä¸­çš„äººè„¸ã€‚å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œåˆ™è¯¥å›¾åƒè®¡ä¸ºæ— è„¸å›¾åƒï¼Œæ•°é‡åŠ  1ã€‚

- **é¢éƒ¨ç›¸ä¼¼æ€§**

  - åˆ©ç”¨ `DeepFace` åº“æå–ç”Ÿæˆå›¾åƒä¸­çš„äººè„¸ç‰¹å¾ï¼Œç„¶åä¸è®­ç»ƒé›†ä¸­äººè„¸çš„ç‰¹å¾è¿›è¡Œå¯¹æ¯”ã€‚é€šè¿‡è®¡ç®—æ¬§æ°è·ç¦»æ¥è¡¡é‡ç›¸ä¼¼åº¦ï¼Œè·ç¦»è¶Šå°ï¼Œè¡¨ç¤ºç”Ÿæˆçš„äººè„¸ä¸è®­ç»ƒé›†ä¸­äººè„¸çš„ç›¸ä¼¼åº¦è¶Šé«˜ã€‚

    > **æ‹“å±•ï¼šä»€ä¹ˆæ˜¯æ¬§å¼è·ç¦»ï¼Ÿ**
    >
    > å¬èµ·æ¥å¾ˆå¤æ‚ï¼Œå®é™…ä¸Šéå¸¸ç®€å•ï¼Œä»¥äºŒç»´ç©ºé—´ä¸ºä¾‹ï¼š
    >
    > å¦‚æœ $\mathbf{p} = (x_1, y_1)$ å’Œ $\mathbf{q} = (x_2, y_2)$ï¼Œå®ƒä»¬ä¹‹é—´çš„æ¬§å¼è·ç¦»å…¬å¼ä¸º:
    > 
    > $$d(\mathbf{p}, \mathbf{q}) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$$
    > 
    > æ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Ÿè¿™å°±æ˜¯æˆ‘ä»¬åœ¨å‡ ä½•å­¦ä¸­å­¦è¿‡çš„**ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»**å…¬å¼ã€‚
    >
    > å°†å…¶æ‹“å±•åˆ° $n$ ç»´ç©ºé—´ï¼Œå¯¹äºä¸¤ä¸ªç‚¹ $\mathbf{p} = (p_1, p_2, \dots, p_n)$ å’Œ $\mathbf{q} = (q_1, q_2, \dots, q_n)$ ï¼Œæ¬§å¼è·ç¦»çš„å…¬å¼ä¸º:
    > 
    > $$d(\mathbf{p}, \mathbf{q}) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \dots + (p_n - q_n)^2}$$
    > 
    > P.S. è™½ç„¶æ¬§å¼è·ç¦»é€šå¸¸é€‚ç”¨äºæ¬§å‡ é‡Œå¾—ç©ºé—´ï¼Œä½†æˆ‘ä»¬ä¸éœ€è¦ç‰¹åˆ«å…³æ³¨è¿™äº›æ•°å­¦é™åˆ¶ã€‚

é™¤äº†äººè„¸ç”Ÿæˆä¹‹å¤–ï¼ŒAI å›¾åƒç”Ÿæˆé¢†åŸŸè¿˜æœ‰å¾ˆå¤šå…¶ä»–åº”ç”¨åœºæ™¯ã€‚é‚£ä¹ˆï¼Œæœ‰æ²¡æœ‰é€šç”¨çš„è¯„ä¼°æ–¹æ³•æ¥è¡¡é‡ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„åŒ¹é…åº¦å‘¢ï¼Ÿ

æœ‰ï¼Œ**CLIP è¯„åˆ†**ã€‚

æ˜¯çš„ï¼ŒCLIP é™¤äº†å¯ä»¥å¤„ç†æ–‡æœ¬è¾“å…¥ï¼Œè¿˜å¯ä»¥è¯„ä¼°æœ€ç»ˆçš„æ¨¡å‹ï¼Œæ— è®ºç”Ÿæˆçš„æ˜¯äººè„¸ã€é£æ™¯è¿˜æ˜¯ç‰©ä½“ï¼Œå®ƒéƒ½å¯ä»¥å¸®åŠ©æˆ‘ä»¬åˆ¤æ–­ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„ç›¸å…³æ€§ã€‚

å¯¹äºå½“å‰å®éªŒï¼Œæˆ‘ä»¬é‡‡å–è¿™ä¸‰ç§æ–¹å¼å¯¹æ¨¡å‹è¿›è¡Œåº¦é‡ï¼Œå®Œæ•´æµç¨‹å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ `load_validation_prompts()` å‡½æ•°ä»æ–‡ä»¶ä¸­åŠ è½½ promptsã€‚
2. ä½¿ç”¨ `prepare_lora_model()` å‡½æ•°åŠ è½½å·²ç»ç»è¿‡ LoRA å¾®è°ƒçš„ UNet å’Œæ–‡æœ¬ç¼–ç å™¨ï¼ˆ`text_encoder`ï¼‰ï¼Œå¹¶åˆå¹¶ LoRA æƒé‡ã€‚æ¨¡å‹ä¼šä»ä¸Šä¸€æ¬¡è®­ç»ƒä¿å­˜çš„æ–‡ä»¶ä¸­æ¢å¤æƒé‡ã€‚
3. ä½¿ç”¨å·²ç»å¾®è°ƒçš„ UNet å’Œæ–‡æœ¬ç¼–ç å™¨æ¥åˆ›å»º `DiffusionPipeline`ã€‚
4. åŠ è½½ CLIP æ¨¡å‹åç»­ç”¨äºè¯„ä¼°ã€‚
5. ä½¿ç”¨ `DeepFace` æå–è®­ç»ƒå›¾åƒçš„é¢éƒ¨åµŒå…¥ `train_emb` ä¸ç”Ÿæˆçš„å›¾åƒè¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—é¢éƒ¨ç›¸ä¼¼åº¦ã€‚
6. è¿›è¡Œè¯„ä¼°ï¼Œæœ€åæ‰“å°ç»“æœã€‚

```python
def evaluate(lora_config):
    """
    åŠ è½½æ¨¡å‹ã€ç”Ÿæˆå›¾åƒå¹¶è¯„ä¼°ã€‚
    
    ä¸»è¦æ­¥éª¤ï¼š
    1. åŠ è½½éªŒè¯æ–‡æœ¬æç¤ºï¼ˆpromptsï¼‰ç”¨äºç”Ÿæˆå›¾åƒã€‚
    2. åŠ è½½å’Œå‡†å¤‡ LoRA å¾®è°ƒåçš„æ¨¡å‹ã€‚
    3. ä½¿ç”¨ DiffusionPipeline ç”Ÿæˆå›¾åƒã€‚
    4. è¯„ä¼°ç”Ÿæˆå›¾åƒçš„äººè„¸ç›¸ä¼¼åº¦ã€CLIP è¯„åˆ†å’Œæ— é¢éƒ¨å›¾åƒæ•°é‡ã€‚
    5. æ‰“å°è¯„ä¼°ç»“æœã€‚
    """
    print("ğŸ“‚ åŠ è½½éªŒè¯æç¤º...")
    validation_prompts = load_validation_prompts(validation_prompt_path)

    print("ğŸ”§ å‡†å¤‡ LoRA æ¨¡å‹...")
    # å‡†å¤‡ LoRA æ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼Œåˆå¹¶æƒé‡ï¼‰
    tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(
        lora_config,
        pretrained_model_name_or_path,
        model_path=model_path,
        resume=True,  # ä»æ£€æŸ¥ç‚¹æ¢å¤
        merge_lora=True  # åˆå¹¶ LoRA æƒé‡
    )

    # åˆ›å»º DiffusionPipeline å¹¶æ›´æ–°å…¶ç»„ä»¶
    print("ğŸ”„ åˆ›å»º DiffusionPipeline...")
    pipeline = DiffusionPipeline.from_pretrained(
        pretrained_model_name_or_path,
        unet=unet,  # ä¼ é€’åŸºç¡€æ¨¡å‹
        text_encoder=text_encoder,  # ä¼ é€’åŸºç¡€æ¨¡å‹
        torch_dtype=weight_dtype,
        safety_checker=None,
    )
    pipeline = pipeline.to(DEVICE)

    # åŠ è½½ CLIP æ¨¡å‹å’Œå¤„ç†å™¨
    print("ğŸ¯ åŠ è½½ CLIP æ¨¡å‹...")
    clip_model_name = "openai/clip-vit-base-patch32"
    clip_model = CLIPModel.from_pretrained(clip_model_name).to(DEVICE)
    clip_processor = CLIPProcessor.from_pretrained(clip_model_name)

    # CLIP æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    clip_model.eval()

    # è®¾ç½®éšæœºæ•°ç§å­
    generator = torch.Generator(device=DEVICE)
    generator.manual_seed(seed)

    # åŠ è½½è®­ç»ƒå›¾åƒçš„é¢éƒ¨åµŒå…¥
    print("ğŸ“‚ åŠ è½½è®­ç»ƒå›¾åƒçš„é¢éƒ¨åµŒå…¥...")
    train_image_paths = sorted([
        p for p in glob.glob(os.path.join(images_folder, "*")) 
        if any(p.endswith(ext) for ext in IMAGE_EXTENSIONS)
    ])
    train_emb_list = []
    for img_path in tqdm(train_image_paths, desc="æå–è®­ç»ƒå›¾åƒé¢éƒ¨åµŒå…¥"):
        face_representation = DeepFace.represent(
            img_path, 
            detector_backend="ssd",
            model_name="GhostFaceNet",
            enforce_detection=False
        )
        if face_representation:
            embedding = face_representation[0]['embedding']
            train_emb_list.append(embedding)

    if len(train_emb_list) == 0:
        print("âš ï¸ æœªèƒ½æå–åˆ°ä»»ä½•è®­ç»ƒå›¾åƒçš„é¢éƒ¨åµŒå…¥ã€‚")
        train_emb = torch.tensor([]).to(DEVICE)
    else:
        train_emb = torch.tensor(train_emb_list).to(DEVICE)

    # ç”Ÿæˆå›¾åƒ
    generated_images = generate_images(
        pipeline=pipeline,
        prompts=validation_prompts,
        num_inference_steps=30,
        guidance_scale=7.5,
        output_folder=inference_path,
        generator=generator
    )

    # è¯„ä¼°ç”Ÿæˆçš„å›¾åƒï¼Œmisè®°å½•æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾åƒæ•°é‡
    face_score, clip_score, mis = 0, 0, 0  # åˆå§‹åŒ–è¯„ä¼°åˆ†æ•°å’Œè®¡æ•°
    valid_emb = []
    print("ğŸ“Š æ­£åœ¨è®¡ç®—è¯„ä¼°åˆ†æ•°...")

    for i, image in enumerate(tqdm(generated_images, desc="è¯„ä¼°å›¾åƒä¸­")):
        # ä½¿ç”¨ DeepFace æ£€æµ‹é¢éƒ¨ç‰¹å¾
        opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        emb = DeepFace.represent(
            opencvImage,
            detector_backend="ssd",
            model_name="GhostFaceNet",
            enforce_detection=False,
        )
        if not emb or emb[0].get('face_confidence', 0) == 0:
            mis += 1  # æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾åƒæ•°é‡
            continue

        # è®¡ç®— CLIP åˆ†æ•°
        current_prompt = validation_prompts[i]
        inputs = clip_processor(text=current_prompt, images=image, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            outputs = clip_model(**inputs)
        sim = outputs.logits_per_image
        clip_score += sim.item()

        # æ”¶é›†æœ‰æ•ˆçš„é¢éƒ¨åµŒå…¥
        valid_emb.append(emb[0]['embedding'])

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„é¢éƒ¨åµŒå…¥ï¼Œåˆ™è¿”å›é»˜è®¤åˆ†æ•°
    if len(valid_emb) == 0:
        print("âš ï¸ æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨åµŒå…¥ï¼")
        return 0, 0, mis

    # è®¡ç®—é¢éƒ¨ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä½¿ç”¨æ¬§æ°è·ç¦»ï¼‰
    valid_emb = torch.tensor(valid_emb).to(DEVICE)
    valid_emb = valid_emb / valid_emb.norm(p=2, dim=-1, keepdim=True)
    train_emb = train_emb / train_emb.norm(p=2, dim=-1, keepdim=True)
    face_distance = torch.cdist(valid_emb, train_emb, p=2).mean().item()
    face_score = face_distance  # å¹³å‡æ¬§æ°è·ç¦»ä½œä¸ºé¢éƒ¨ç›¸ä¼¼æ€§åˆ†æ•°
    clip_score /= (len(validation_prompts) - mis) if (len(validation_prompts) - mis) > 0 else 1
    print("ğŸ“ˆ è¯„ä¼°å®Œæˆï¼")

    # æ‰“å°è¯„ä¼°ç»“æœ
    print(f"âœ… é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ† (å¹³å‡æ¬§æ°è·ç¦»): {face_score:.4f} (è¶Šä½è¶Šå¥½ï¼Œè¡¨ç¤ºç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒæ›´ç›¸ä¼¼)")
    print(f"âœ… CLIP è¯„åˆ† (å¹³å‡ç›¸ä¼¼åº¦): {clip_score:.4f} (è¶Šé«˜è¶Šå¥½ï¼Œè¡¨ç¤ºç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„ç›¸å…³æ€§æ›´å¼º)")
    print(f"âœ… æ— é¢éƒ¨å›¾åƒæ•°é‡: {mis} (æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„ç”Ÿæˆå›¾åƒæ•°é‡)")

# è°ƒç”¨å‡½æ•°æ‰§è¡Œ
evaluate(lora_config)
```

ç”Ÿæˆçš„å›¾åƒä¼šä¿å­˜åœ¨ `./SD/Brad/inference` ä¸­ã€‚

## æ‹“å±•ä½œä¸š

1. å½“å‰ prompt çš„è§¦å‘è¯ï¼ˆ**trigger words**ï¼‰åªæ˜¯ â€œa manâ€ å—ï¼Ÿ  
   ä»”ç»†è§‚å¯Ÿä¹‹å‰æ•°æ®é›†çš„promptï¼š
   
   - a man with a beard and a suit jacket
   - a man in a suit and tie standing in front of a crowd
   - a man with long hair and a tie
   - ...
   
2. ä½¿ç”¨å½“å‰æ•°æ®é›†è®­ç»ƒå‡ºçš„æ¨¡å‹ï¼Œå¦‚æœ prompt è®¾ç½®ä¸º â€œa manâ€ï¼Œç”Ÿæˆçš„å›¾åƒåº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ

3. é™¤äº†ä¹‹å‰è®¾ç½®çš„å‚æ•°å¤–ï¼Œæ¢ç©¶ç”Ÿæˆå›¾åƒç›¸å…³å‚æ•°ï¼ˆä½äº `evaluate()`ï¼‰ã€‚
   
   ```python
   generated_images = generate_images(
           pipeline=pipeline,
           prompts=validation_prompts,
           num_inference_steps=30,  # ä¿®æ”¹æ¨ç†æ­¥æ•°
           guidance_scale=7.5,  # ä¿®æ”¹æ–‡æœ¬æç¤ºå½±å“ç¨‹åº¦
           output_folder=inference_path,
           generator=generator  # æ³¨é‡Šè¿™ä¸€è¡Œï¼Œçœ‹çœ‹ä¸ä¼ å…¥ generator æ—¶ç”Ÿæˆçš„å›¾åƒæ˜¯å¦æœ‰å˜åŒ–ï¼Ÿå°è¯•è¿è¡Œä¸‰æ¬¡è¿›è¡Œå¯¹æ¯”ã€‚
       )
   ```

å¸Œæœ›ä½ èƒ½é€šè¿‡å¯¹ä»£ç æ–‡ä»¶çš„è¿è¡Œï¼Œæ‰¾åˆ°å®ƒä»¬çš„ç­”æ¡ˆã€‚

## ç”¨ ğŸ¡ è„šæœ¬å¾®è°ƒ SDï¼ˆå¯é€‰ï¼‰

è¿™æ˜¯å¯é€‰çš„è¡Œä¸ºï¼Œè„šæœ¬çš„ä»£ç å¤„ç†é€»è¾‘ä¸æ–‡ç« å¯¹åº”ã€‚

### å…‹éš†ä»“åº“

```bash
# å¦‚æœå·²ç»å…‹éš†ä»“åº“çš„è¯è·³è¿‡è¿™è¡Œ
git clone https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN
```

### æ‰§è¡Œè„šæœ¬

1. åˆ‡æ¢åˆ° `CodePlayground` æ–‡ä»¶å¤¹ï¼š

   ```bash
   cd AI-Guide-and-Demos-zh_CN/CodePlayground
   ```

2. å‡†å¤‡æ ·ä¾‹æ•°æ®é›†ï¼š

   ```bash
   # å¦‚æœå·²ç»ä¸‹è½½è¿‡ï¼Œå¯ä»¥è·³è¿‡ï¼Œå°†ä¹‹åçš„å‘½ä»¤å‚æ•°ä¿®æ”¹ä¸ºå¯¹åº”è·¯å¾„
   wget https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/raw/refs/heads/master/Demos/data/14/Datasets.zip
   unzip Datasets.zip
   ```

3. ä½¿ç”¨æŒ‡å®šçš„æ•°æ®é›†å’Œæç¤ºæ–‡ä»¶ï¼š

   ```bash
   python sd_lora.py -d ./Datasets/Brad -gp ./Datasets/prompts/validation_prompt.txt
   ```

   - `-d` æˆ– `--dataset_path`ï¼šæ•°æ®é›†è·¯å¾„ã€‚
   - `-gp` æˆ– `--prompts_path`ï¼šç”Ÿæˆå›¾åƒæ—¶ä½¿ç”¨çš„æ–‡æœ¬æç¤ºæ–‡ä»¶è·¯å¾„ã€‚

4. æŒ‡å®šå…¶ä»–å‚æ•°ï¼š

   ```bash
   python sd_lora.py -d ./Datasets/Brad -gp ./Datasets/prompts/validation_prompt.txt -e 500 -b 4 -u 1e-4 -t 1e-5
   ```

   - `-e` æˆ– `--max_train_steps`ï¼šæ€»è®­ç»ƒæ­¥æ•°ã€‚
   - `-b` æˆ– `--batch_size`ï¼šè®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚
   - `-u` æˆ– `--unet_learning_rate`ï¼šUNet çš„å­¦ä¹ ç‡ã€‚
   - `-t` æˆ– `--text_encoder_learning_rate`ï¼šæ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡ã€‚
   - å…¶ä»–å‚æ•°ä½¿ç”¨ `--help` è¿›è¡ŒæŸ¥çœ‹ã€‚

æ›´è¯¦ç»†çš„ä»‹ç»è§ [CodePlayground](../CodePlayground/README.md#å½“å‰çš„ç©å…·)ï¼Œç‚¹å‡» `â–º` æˆ–å¯¹åº”çš„æ–‡æœ¬å±•å¼€ã€‚

## å‚è€ƒé“¾æ¥

- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- [DiffusionPipeline æ–‡æ¡£](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline), [æºç ](https://github.com/huggingface/diffusers/blob/8e7d6c03a366fdb0f551ce7b92f0871c863d4e08/src/diffusers/pipelines/pipeline_utils.py#L495)
- [Customize a pipeline - Hugging Face](https://huggingface.co/docs/diffusers/using-diffusers/loading?pipelines=generic+pipeline#customize-a-pipeline)
