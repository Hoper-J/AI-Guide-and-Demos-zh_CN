# 初识 LLM API：环境配置与多轮对话演示

> 其实 AI 应用并不是一个什么很高大上的东西，你可以将它当作一个文字的“调库”行为，“调库”只需要知道库名就行了，这里实际也是如此。甚至你只需要知道你想问什么，将你的消息作为输入，就能从大模型得到输出。而这个“库”本身，是已经部署好了的，对于这样的一个黑盒的使用并没有太多的技术要求，不用担心自己的知识储备不够，因为并不需要你对 AI 本身有了解，也不需要你去训练一个 AI，只是使用它。
>
> 毕竟在 ChatGPT 发布（*2022.11.30*）之前，AI 应用并不广泛被需要，这是大模型兴起之后的自然产物。
>
> 使用国内大模型 API 是为了降低获取门槛~~（不过获取步骤确实稍微复杂点，有种国内外教科书风格的差异感）~~ 。**DeepSeek API 的获取和使用非常简洁**。
>
> [代码文件下载](../Demos/01.%20LLM%20API%20使用演示——从环境配置到多轮对话.ipynb)
>
> 在线链接：[Kaggle](https://www.kaggle.com/code/aidemos/01-llm-api) | [Colab](https://colab.research.google.com/drive/1i3Oemsu-mdHgB-uhhJBAgzjnr0woVeiW?usp=sharing)

## 目录

- [环境变量配置](#环境变量配置)
  - [os.getenv()](#osgetenv)
- [演示](#演示)
  - [阿里 API](#阿里-api)
    - [单轮对话](#单轮对话)
    - [多轮对话](#多轮对话)
    - [流式输出](#流式输出)
    - [更换模型](#更换模型)
  - [智谱 API](#智谱-api)
    - [设置 API](#设置-api)
    - [单轮对话](#单轮对话-1)
    - [多轮对话](#多轮对话-1)
    - [流式输出](#流式输出-1)
    - [更换模型](#更换模型-1)
  - [DeepSeek API](#deepseek-api)
    - [设置 API](#设置-api-1)
    - [单轮对话](#单轮对话-2)
    - [多轮对话](#多轮对话-2)
    - [流式输出](#流式输出-2)
    - [更换模型](#更换模型-2)
- [参考链接](#参考链接)
- [下一步](#下一步)

---

> **更新**：国内大模型开启了新的篇章，考虑到以后可能会进行多模型的 API 输出对比，不再固定命名为 `OPENAI_API_KEY`。
>
> 为了后续代码正常执行，阿里云的 API 环境名暂时保持为 `OPENAI_API_KEY`。

假设你已经获取到了`API KEY`，阿里云将这个 API 命名为 `DASHSCOPE_API_KEY`（**灵积**是阿里云推出的模型服务平台，**DashScope** 是灵积的英文名）~~，不过为了更加通用，我们还是将其命名为`OPENAI_API_KEY`~~。

## 环境变量配置

环境变量是操作系统中以键值对形式存储的配置项，常用于保存敏感信息（如 API 密钥、数据库连接地址等），这样可以避免将这些私密的信息直接写在代码中。

你可以通过操作系统设置环境变量，或者直接在 Python 脚本中设置，二选一。

**在终端 (Linux/Mac) 设置：**

> 此方法仅适用于当前终端会话，关闭终端后设置会失效。

```bash
export OPENAI_API_KEY="your-api-key"
```

**在命令提示符 (Windows) 中设置：**

```cmd
set OPENAI_API_KEY=your-api-key
```

**写入配置文件（推荐）**：

> 如果使用了项目提供的[镜像](../README.md#-docker-快速部署-)，使用 Zsh 相关命令

- **Linux/Mac**

  - **Bash**

    ```bash
    echo 'export OPENAI_API_KEY="your-api-key"' >> ~/.bashrc
    source ~/.bashrc  # 重新加载配置文件
    ```

  - **Zsh**

    ```bash
    echo 'export OPENAI_API_KEY="your-api-key"' >> ~/.zshrc
    source ~/.zshrc  # 重新加载配置文件
    ```

- **Windows**

  - **PowerShell**

    ```bash
    echo export OPENAI_API_KEY="your-api-key" >> %USERPROFILE%\Documents\WindowsPowerShell\Microsoft.PowerShell_profile.ps1
    ```

  - ...（其他终端需要修改 `>>` 之后为对应的配置文件）

**通过 Python 程序设置环境变量：**

> 此方法仅在当前 Python 程序或 Notebook 中有效，其他程序或 Notebook 不会共享此设置。

```python
import os
os.environ['OPENAI_API_KEY'] = 'your-api-key'
```

### os.getenv()

`os.getenv()` 是 Python 中 `os` 模块的一个函数，用于获取系统环境变量的值，语法：

```python
os.getenv('VARIABLE_NAME', default_value)
```

- `'VARIABLE_NAME'`: 要获取的环境变量的名称。
- `default_value` (可选): 如果环境变量不存在，可以指定一个默认值，当环境变量未设置时将返回该默认值。

**示例：**

假设你已经配置好了 `DASHSCOPE_API_KEY` 的环境变量，并且想在 Python 脚本中访问它：

```python
import os

api_key = os.getenv('OPENAI_API_KEY')
print(api_key)  # 如果环境变量已设置，它将输出对应的值。

# # 设置默认值
# api_key = os.getenv('OPENAI_API_KEY', 'default_key')
# print(api_key)  # 如果环境变量没有设置，它将输出 'default_key'。
```

## 演示

### 阿里 API

首先命令行安装 openai 库。

```bash
# 项目依赖已在 pyproject.toml 中配置，运行 uv sync 即可安装
# 文章中重复的 uv add 是旧版本 pip install 的遗留（默认仅配置了 PyTorch 等基础深度学习环境）
uv add openai
#uv add 'httpx<0.28.0' # 降级 httpx 以解决关键字 'proxies' 被移除的问题，最新的 openai 库不会引发该问题，故默认注释
```

#### 单轮对话

在安装完成后，用 Python 进行访问尝试，在这里我们用通义千问-Turbo 进行演示。

```python
from openai import OpenAI
import os

# 初始化 OpenAI 客户端，使用阿里云 DashScope API
client = OpenAI(
    api_key=os.getenv('OPENAI_API_KEY'), # 如果你没有配置环境变量，使用 api_key="your-api-key" 替换
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", # 这里使用的是阿里云的大模型，如果需要使用其他平台，请参考对应的开发文档后对应修改
)

# 调用 API 获取模型回复
response = client.chat.completions.create(
    model="qwen-turbo",
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': '你是谁？'}]
    )

# 打印模型回复内容
print(response.choices[0].message.content)
```

**输出**：

```
我是来自阿里云的大规模语言模型，我叫通义千问。
```


>注意，不要误用成 `api_key=os.getenv("your-api-key")`，`os.getenv()`用于获取对应系统环境变量的值，API本身并不是这个环境变量，正确的用法是 ``api_key="your-api-key"`。
>
>否则你就会遇到一个对于新手来讲不够直接的报错：`OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable`。

#### 多轮对话

之前的代码只提供了单轮的对话，没有历史记录，没有上下文，只是一个简单的临时对话，你可以将其理解为无痕浏览，关了就没了。

接下来，我们介绍多轮的对话，这里的轮指的是一次问答。模型本身并不会因为你的问题即时得到训练，所以也不会保留你之前的对话，那我们怎么去让模型知道呢？

答：朴素的手动保存上传。

```python
from openai import OpenAI
import os

# 初始化OpenAI客户端
client = OpenAI(
    api_key=os.getenv('OPENAI_API_KEY'), 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# 初始化对话历史
messages = [{"role": "system", "content": "You are a helpful assistant."}]

# 进行多轮对话，当前为3轮
for i in range(3):
    # 获取用户输入
    user_input = input("请输入：")
    
    # 添加用户消息到对话历史
    messages.append({"role": "user", "content": user_input})
    
    # 调用API获取模型回复
    response = client.chat.completions.create(
        model="qwen-turbo",
        messages=messages
    )
    
    # 提取模型回复内容
    assistant_output = response.choices[0].message.content
    
    # 将模型回复添加到对话历史
    messages.append({"role": "assistant", "content": assistant_output})
    
    print(f'用户输入：{user_input}')
    print(f'模型输出：{assistant_output}\n')
```

**输出**：

```
请输入： 你好
用户输入：你好
模型输出：你好！很高兴为你提供帮助。

请输入： 1+2=
用户输入：1+2=
模型输出：1 + 2 = 3

请输入： 我们刚刚说了什么
用户输入：我们刚刚说了什么
模型输出：我们刚刚进行了一个简单的数学计算，1 + 2 = 3。你还问了“你好”和“我们刚刚说了什么”。有什么其他问题或需要进一步的帮助吗？
```

#### 流式输出

语言模型并不是直接得出完整的一句话，而是一个字一个字（其实是 Token，为了更大白话一点这里用字帮助理解）去生成的。前面的对话都是直接获取到了最终的生成结果，能不能实时输出它呢？

当然可以，下面的代码相当于之前的单轮对话，只是改变了输出方式。

```python
from openai import OpenAI
import os

# 初始化OpenAI客户端
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# 开启流式输出
response = client.chat.completions.create(
    model="qwen-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "你是谁？"}
    ],
    stream=True,
)

# 实时打印模型回复的增量内容
for chunk in response:
    # 判断回复内容是否非空
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='') 
```

**Q：为什么要获取流式输出？**

使用直接的对话模式需要等待大模型生成结束后，才传回每个部分拼接后的结果，而流式输出可以实时了解到生成的信息，让使用者在模型生成的时候进行阅读，从而利用上中间的等待时间，这是一个可以同步进行的事情，等待浪费了我们的时间。毕竟有些回答刚一开口就可以否决掉。

> 常见问题：
>
> 1. 通义千问、灵积、DashScope、百炼是什么关系？
>
>    **通义千问**是阿里云研发的大语言模型；**灵积**是阿里云推出的模型服务平台，提供了包括通义千问在内的多种模型的服务接口，**DashScope**是灵积的英文名，两者指的是同一平台；**百炼**是阿里云推出的一站式大模型应用开发平台，同时也提供模型调用服务。
>
> 2. 我如果想调用通义千问模型，是要通过灵积平台还是百炼平台？
>
>    对于需要调用通义千问模型的开发者而言，通过灵积平台与百炼平台调用通义千问模型**都是**通过 dashscope SDK 或 OpenAI 兼容或 HTTP 方式实现。两个平台都可以获取到 API-KEY，且是同步的。因此您只需准备好计算环境，并在两个平台任选其一创建 API-KEY，即可发起通义千问模型的调用。

#### 更换模型

阿里大模型平台同样支持很多其他的模型，比如 Llama3.1，ChatGLM3，StableDiffusion 等，感兴趣的话详细可见[模型列表](https://help.aliyun.com/zh/model-studio/getting-started/models?spm=a2c4g.11186623.0.0.5f7c65c7eVBHMa)。

### 智谱 API

> 修改以下三个参数：
>
> 1. api_key：《[00. 大模型 API 获取步骤](./00.%20大模型%20API%20获取步骤.md#智谱-api)》。
> 2. base_url：智谱当前的 `base_url` 为 `https://open.bigmodel.cn/api/paas/v4`。
> 3. model：以 `glm-4-plus` 为例。
>
> ```diff
> - client = OpenAI(
> -     api_key=os.getenv('OPENAI_API_KEY'),
> -     base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
> - )
> - completion = client.chat.completions.create(
> -     model="qwen-turbo",
> 
> + client = OpenAI(
> +     api_key=os.getenv('ZHIPUAI_API_KEY'), # 1
> +     base_url="https://open.bigmodel.cn/api/paas/v4", # 2
> + )
> + completion = client.chat.completions.create(
> +     model="glm-4-plus", # 3
> ```

#### 设置 API

```python
import os
os.environ['ZHIPUAI_API_KEY'] = 'your-api-key' # 1
```

#### 单轮对话

```python
from openai import OpenAI
import os

# 初始化 OpenAI 客户端
client = OpenAI(
    api_key=os.getenv('ZHIPUAI_API_KEY'),
    base_url="https://open.bigmodel.cn/api/paas/v4",
)

# 调用 API 获取模型回复
response = client.chat.completions.create(
    model="glm-4-plus",
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': '你是谁？'}]
    )

# 打印模型回复内容
print(response.choices[0].message.content)
```

#### 多轮对话

```python
from openai import OpenAI
import os

# 初始化 OpenAI 客户端
client = OpenAI(
    api_key=os.getenv('ZHIPUAI_API_KEY'),
    base_url="https://open.bigmodel.cn/api/paas/v4",
)

# 初始化对话历史
messages = [{"role": "system", "content": "You are a helpful assistant."}]

# 进行多轮对话，当前为3轮
for i in range(3):
    # 获取用户输入
    user_input = input("请输入：")
    
    # 添加用户消息到对话历史
    messages.append({"role": "user", "content": user_input})
    
    # 调用API获取模型回复
    response = client.chat.completions.create(
        model="glm-4-plus",
        messages=messages
    )
    
    # 提取模型回复内容
    assistant_output = response.choices[0].message.content
    
    # 将模型回复添加到对话历史
    messages.append({"role": "assistant", "content": assistant_output})
    
    print(f'用户输入：{user_input}')
    print(f'模型输出：{assistant_output}\n')
```

#### 流式输出

```python
from openai import OpenAI
import os

# 初始化 OpenAI 客户端
client = OpenAI(
    api_key=os.getenv('ZHIPUAI_API_KEY'),
    base_url="https://open.bigmodel.cn/api/paas/v4",
)

# 开启流式输出
response = client.chat.completions.create(
    model="glm-4-plus",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "你是谁？"}
    ],
    stream=True,
)

# 实时打印模型回复的增量内容
for chunk in response:
    # 判断回复内容是否非空
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='') 
```

#### 更换模型

访问 `控制台` 中的 `模型广场`，选择想要更换的模型，复制对应的 `模型编码`：

![模型广场](./assets/image-20250115104841658.png)

对应替换代码中的 `model` 即可。

### DeepSeek API

> 修改以下三个参数：
>
> 1. api_key：《[DeepSeek API 的获取与对话示例](./DeepSeek%20API%20的获取与对话示例.md)》。
>
> 2. base_url：`https://api.deepseek.com`。
>
> 3. model：`deepseek-chat` 或 `deepseek-reasoner`。
>
>
> ```diff
> - client = OpenAI(
> -     api_key=os.getenv('OPENAI_API_KEY'),
> -     base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
> - )
> - completion = client.chat.completions.create(
> -     model="qwen-turbo",
> 
> + client = OpenAI(
> +     api_key=os.getenv('DEEPSEEK_API_KEY'), # 1
> +     base_url="https://api.deepseek.com", # 2
> + )
> + completion = client.chat.completions.create(
> +     model="deepseek-chat", # 3
> ```

#### 设置 API

```python
import os
os.environ['DEEPSEEK_API_KEY'] = 'your-api-key' # 1
```

#### 单轮对话

```python
from openai import OpenAI
import os

# 初始化 OpenAI 客户端
client = OpenAI(
    api_key=os.getenv('DEEPSEEK_API_KEY'),  # 1
    base_url="https://api.deepseek.com",  # 2
)

# 调用 API 获取模型回复
response = client.chat.completions.create(
    model="deepseek-chat",  # 3
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': '你是谁？'}]
    )

# 打印模型回复内容
print(response.choices[0].message.content)
```

#### 多轮对话

```python
from openai import OpenAI
import os

# 初始化 OpenAI 客户端
client = OpenAI(
    api_key=os.getenv('DEEPSEEK_API_KEY'),
    base_url="https://api.deepseek.com",
)

# 初始化对话历史
messages = [{"role": "system", "content": "You are a helpful assistant."}]

# 进行多轮对话，当前为3轮
for i in range(3):
    # 获取用户输入
    user_input = input("请输入：")
    
    # 添加用户消息到对话历史
    messages.append({"role": "user", "content": user_input})
    
    # 调用API获取模型回复
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages
    )
    
    # 提取模型回复内容
    assistant_output = response.choices[0].message.content
    
    # 将模型回复添加到对话历史
    messages.append({"role": "assistant", "content": assistant_output})
    
    print(f'用户输入：{user_input}')
    print(f'模型输出：{assistant_output}\n')
```

#### 流式输出

```python
from openai import OpenAI
import os

# 初始化 OpenAI 客户端
client = OpenAI(
    api_key=os.getenv('DEEPSEEK_API_KEY'),
    base_url="https://api.deepseek.com",
)

# 开启流式输出
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "你是谁？"}
    ],
    stream=True,
)

# 实时打印模型回复的增量内容
for chunk in response:
    # 判断回复内容是否非空
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='') 
```

#### 更换模型

当前使用的是聊天模型（`deepseek-chat`），如果你想修改为推理模型（`deepseek-reasoner`），对应修改代码中的 `model`：

```diff
- completion = client.chat.completions.create(
-     model="deepseek-chat",
-     ...
-     )
    
+ completion = client.chat.completions.create(
+     model="deepseek-reasoner",
+     ...
+     )
```

## 参考链接

- [Chat - OpenAI Docs](https://platform.openai.com/docs/api-reference/chat/create)
- [Completions - OpenAI Docs](https://platform.openai.com/docs/guides/completions)
- [通过API使用通义千问-阿里云官方文档](https://help.aliyun.com/zh/model-studio/developer-reference/use-qwen-by-calling-api)

## 下一步

- 可以遵循导论的顺序进行阅读，如果对 API 的使用感兴趣，推荐跳转阅读 《[DeepSeek 使用手册](../README.md#deepseek-使用手册doing)》。
- 对流式输出感到疑惑的同学阅读：《[DeepSeek API 输出解析 - OpenAI SDK](./DeepSeek%20API%20输出解析%20-%20OpenAI%20SDK.md)》和 《[流式输出解析](./DeepSeek%20API%20流式输出解析%20-%20OpenAI%20SDK.md)》。
