# BPE vs WordPieceï¼šç†è§£ Tokenizer çš„å·¥ä½œåŸç†ä¸å­è¯åˆ†å‰²æ–¹æ³•

> åœ¨åº”ç”¨çš„è·¯ä¸Šâ€œè’™ç€å¤´â€èµ°äº†ä¸€æ®µï¼Œæ˜¯æ—¶å€™å›è¿‡å¤´æ¥ç†è§£å…¶ä¸­çš„å·¥ä½œåŸç†äº†ã€‚
>
> æ–‡ç« å°†ä»¥æ–‡æœ¬å¤„ç†ä¸ºä¾‹ï¼Œä»‹ç»æ•°æ®é¢„å¤„ç†ä¸­çš„å…³é”®ç»„ä»¶â€”â€”**Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰**ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæ˜¯åæ¦‚å¿µæ€§çš„è®²è§£ï¼Œä¸ä¼šæ·±å…¥å…·ä½“å‡½æ•°çš„å‚æ•°ç»†èŠ‚ã€‚
>
> ã€Œ[æ„é€ è¯æ±‡è¡¨](#æ„é€ è¯æ±‡è¡¨)ã€éƒ¨åˆ†å°†ä»‹ç»ä¸¤ç§å¸¸è§çš„å­è¯åˆ†å‰²æ–¹æ³•ï¼š
>
> - **BPEï¼ˆByte-Pair Encodingï¼‰**ï¼šç”¨äº GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTa ç­‰æ¨¡å‹ã€‚
> - **WordPiece**ï¼šç”¨äº DistilBERTã€MobileBERTã€Funnel Transformers å’Œ MPNET ç­‰æ¨¡å‹ã€‚
>
> ã€Œ[æ‹“å±•](#qæ³¨æ„åŠ›æ©ç attention-maskå’Œè¯å…ƒç±»å‹-id-token-type-idsæ˜¯ä»€ä¹ˆ)ã€éƒ¨åˆ†å°†æ¶‰åŠä¸¤ä¸ªé‡è¦æ¦‚å¿µï¼š
>
> - **æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰**
> - **è¯å…ƒç±»å‹ ID ï¼ˆToken Type IDsï¼‰**
>
> å·¥å…·ï¼š[Tiktokenizerï¼ˆæ¨èï¼‰](https://tiktokenizer.vercel.app) | [The Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)
>
> [ä»£ç æ–‡ä»¶ä¸‹è½½](../Demos/19.%20BPE%20vs%20WordPieceï¼šç†è§£%20Tokenizer%20çš„å·¥ä½œåŸç†ä¸å­è¯åˆ†å‰²æ–¹æ³•.ipynb)
>
> åœ¨çº¿é“¾æ¥ï¼š[Kaggle](https://www.kaggle.com/code/aidemos/19-bpe-vs-wordpiece-tokenizer) | [Colab](https://colab.research.google.com/drive/1J6QN0QbuoWBDIIrBe-TJ6Hi5rnzTSovM?usp=sharing)

## ç›®å½•

- [ä»€ä¹ˆæ˜¯ Tokenizerï¼Ÿ](#ä»€ä¹ˆæ˜¯-tokenizer)
  - [æ¨¡å‹è¾“å…¥ï¼ˆç¼–ç  Encodeï¼‰é˜¶æ®µ](#æ¨¡å‹è¾“å…¥ç¼–ç -encodeé˜¶æ®µ)
  - [æ¨¡å‹è¾“å‡ºï¼ˆè§£ç  Decodeï¼‰é˜¶æ®µ](#æ¨¡å‹è¾“å‡ºè§£ç -decodeé˜¶æ®µ)
  - [ç›´è§‚æ„Ÿå—](#ç›´è§‚æ„Ÿå—)
- [å®é™…ä½¿ç”¨](#å®é™…ä½¿ç”¨)
  - [å®‰è£…åº“](#å®‰è£…åº“)
  - [BPE åˆ†è¯å™¨ç¤ºä¾‹](#bpe-åˆ†è¯å™¨ç¤ºä¾‹)
  - [WordPiece åˆ†è¯å™¨ç¤ºä¾‹](#wordpiece-åˆ†è¯å™¨ç¤ºä¾‹)
  - [ä½¿ç”¨ encode() å’Œ decode() æ–¹æ³•](#ä½¿ç”¨-encode-å’Œ-decode-æ–¹æ³•)
- [äº†è§£ Tokenizer çš„åŸºç¡€å±æ€§](#äº†è§£-tokenizer-çš„åŸºç¡€å±æ€§)
  - [æŸ¥çœ‹è¯æ±‡è¡¨å¤§å°](#æŸ¥çœ‹è¯æ±‡è¡¨å¤§å°)
  - [æŸ¥çœ‹è¯æ±‡è¡¨](#æŸ¥çœ‹è¯æ±‡è¡¨)
  - [æŸ¥çœ‹ç‰¹å®š Token å’Œ ID çš„æ˜ å°„å…³ç³»](#æŸ¥çœ‹ç‰¹å®š-token-å’Œ-id-çš„æ˜ å°„å…³ç³»)
  - [æŸ¥çœ‹ç‰¹æ®Šæ ‡è®°ï¼ˆSpecial Tokensï¼‰](#æŸ¥çœ‹ç‰¹æ®Šæ ‡è®°special-tokens)
- [åˆ†è¯ï¼ˆTokenizeï¼‰](#åˆ†è¯tokenize)
  - [æ„é€ è¯æ±‡è¡¨](#æ„é€ è¯æ±‡è¡¨)
    - [Byte-Pair Encoding (BPE)](#byte-pair-encoding-bpe)
      - [æ­¥éª¤](#æ­¥éª¤)
      - [ç¤ºä¾‹](#ç¤ºä¾‹)
      - [ğŸ“ ç»ƒä¹ é¢˜](#-ç»ƒä¹ é¢˜)
    - [WordPiece](#wordpiece)
      - [æ­¥éª¤](#æ­¥éª¤-1)
      - [ç¤ºä¾‹](#ç¤ºä¾‹-1)
      - [ä½¿ç”¨å‡½æ•°å®ç°ç®€å•çš„ WordPiece](#ä½¿ç”¨å‡½æ•°å®ç°ç®€å•çš„-wordpiece)
    - [ğŸ“ ç»ƒä¹ é¢˜ç­”æ¡ˆ](#-ç»ƒä¹ é¢˜ç­”æ¡ˆ)
  - [æ ‡è®°æ–‡æœ¬](#æ ‡è®°æ–‡æœ¬)
    - [BPE](#bpe)
    - [WordPiece](#wordpiece-1)
- [æ˜ å°„ï¼ˆMappingï¼‰](#æ˜ å°„mapping)
- [æ‹“å±•ï¼ˆTransformersï¼‰](#æ‹“å±•transformers)
  - [Qï¼šæ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰å’Œè¯å…ƒç±»å‹ ID ï¼ˆToken Type IDsï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ](#qæ³¨æ„åŠ›æ©ç attention-maskå’Œè¯å…ƒç±»å‹-id-token-type-idsæ˜¯ä»€ä¹ˆ)
- [å‚è€ƒé“¾æ¥](#å‚è€ƒé“¾æ¥)

## ä»€ä¹ˆæ˜¯ Tokenizerï¼Ÿ

**Tokenizer**ï¼ˆåˆ†è¯å™¨ï¼‰å¯ä»¥å°†åŸå§‹æ–‡æœ¬ï¼ˆraw textï¼‰è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—åºåˆ—ï¼Œåœ¨æ¨¡å‹è¾“å…¥å’Œè¾“å‡ºçš„ä¸¤ä¸ªä¸»è¦é˜¶æ®µä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼š

### æ¨¡å‹è¾“å…¥ï¼ˆç¼–ç  Encodeï¼‰é˜¶æ®µ

1. **åˆ†è¯ï¼ˆTokenizeï¼‰**

   å°†æ–‡æœ¬æ‹†åˆ†ä¸ºè¯å…ƒï¼ˆTokenï¼‰ï¼Œå¸¸è§çš„åˆ†è¯æ–¹å¼åŒ…æ‹¬å­—çº§ã€è¯çº§ã€å­è¯çº§ï¼ˆå¦‚ BPEã€WordPieceï¼‰ã€ç©ºæ ¼åˆ†è¯ç­‰ã€‚

   ```sql
   è¾“å…¥: "ä½ å¥½"
   åˆ†è¯: ["ä½ ", "å¥½"]
   ```

2. **æ˜ å°„ï¼ˆMappingï¼‰**

   å°†æ¯ä¸ªè¯å…ƒæ˜ å°„ä¸ºè¯æ±‡è¡¨ä¸­çš„å”¯ä¸€ IDï¼Œç”Ÿæˆçš„æ•°å­—åºåˆ—å³ä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚

   ```sql
   åˆ†è¯: ["ä½ ", "å¥½"]
   æ˜ å°„: [1001, 1002]
   ```

### æ¨¡å‹è¾“å‡ºï¼ˆè§£ç  Decodeï¼‰é˜¶æ®µ

1. **åæ˜ å°„ï¼ˆDe-mappingï¼‰**

   æ¨¡å‹è¾“å‡ºçš„æ•°å­—åºåˆ—é€šè¿‡è¯æ±‡è¡¨æ˜ å°„å›å¯¹åº”çš„è¯å…ƒï¼ŒäºŒè€…æ˜¯ä¸€ä¸€å¯¹åº”çš„å…³ç³»ã€‚

   ```sql
   è¾“å‡º: [1001, 1002]
   åæ˜ å°„: ["ä½ ", "å¥½"]
   ```

2. **æ–‡æœ¬é‡ç»„**

   å°†è§£ç åçš„è¯å…ƒä»¥æŸç§è§„åˆ™é‡æ–°æ‹¼æ¥ä¸ºå®Œæ•´æ–‡æœ¬ã€‚

   ```sql
   åæ˜ å°„: ["ä½ ", "å¥½"]
   é‡ç»„: "ä½ å¥½"
   ```

### ç›´è§‚æ„Ÿå—

è®¿é—® [Tiktokenizer](https://tiktokenizer.vercel.app)ï¼Œé€šè¿‡å³ä¸Šè§’é€‰å–ä¸åŒçš„ Tokenizer è¿›è¡Œå°è¯•ï¼š

![image-20241022152315606](./assets/image-20241022152315606.png)

## å®é™…ä½¿ç”¨

åœ¨è¿›ä¸€æ­¥è®²è§£ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡ **Transformers** åº“ä¸­çš„ `AutoTokenizer` ç±»æ¥ä½¿ç”¨ Tokenizerã€‚

### å®‰è£…åº“

```bash
pip install transformers
```

### BPE åˆ†è¯å™¨ç¤ºä¾‹

```python
from transformers import AutoTokenizer

# ä½¿ç”¨ GPT-2 çš„åˆ†è¯å™¨ï¼ˆBPEï¼‰
tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "Hello, world!"

# ç¼–ç 
# 1. å°†æ–‡æœ¬åˆ†è¯ä¸º Tokens
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# 2. å°† Tokens è½¬æ¢ä¸º Token IDs
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Token IDs:", token_ids)

# è§£ç 
# 1. Token IDs è½¬æ¢ä¸º Tokens
tokens = tokenizer.convert_ids_to_tokens(token_ids)
print("Tokens:", tokens)

# 2. Tokens æ‹¼æ¥ä¸ºæ–‡æœ¬
decoded_text = tokenizer.convert_tokens_to_string(tokens)
print("Decoded Text:", decoded_text)
```

**è¾“å‡º**ï¼š

```python
Tokens: ['Hello', ',', 'Ä world', '!']
Token IDs: [15496, 11, 995, 0]
Tokens: ['Hello', ',', 'Ä world', '!']
Decoded Text: Hello, world!
```

> [!note]
>
> å®é™…ä¸Š GPT-2 ç”¨çš„æ˜¯ Byte-level BPEï¼Œä¹Ÿå°±æ˜¯ä»å­—ç¬¦çº§å¤„ç†å˜æˆäº†å­—èŠ‚çº§ï¼Œè¿™æ ·å¯ä»¥ç›´æ¥å¤„ç†ä¸åŒè¯­è¨€æˆ–è€…ç‰¹æ®Šçš„ç¬¦å·ã€‚

### WordPiece åˆ†è¯å™¨ç¤ºä¾‹

```python
from transformers import AutoTokenizer

# ä½¿ç”¨ BERT çš„åˆ†è¯å™¨ï¼ˆWordPieceï¼‰
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hello, world!"

# ç¼–ç 
# 1. å°†æ–‡æœ¬åˆ†è¯ä¸º Tokens
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# 2. å°† Tokens è½¬æ¢ä¸º Token IDs
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Token IDs:", token_ids)

# è§£ç 
# 1. Token IDs è½¬æ¢ä¸º Tokens
tokens = tokenizer.convert_ids_to_tokens(token_ids)
print("Tokens:", tokens)

# 2. Tokens æ‹¼æ¥ä¸ºæ–‡æœ¬
decoded_text = tokenizer.convert_tokens_to_string(tokens)
print("Decoded Text:", decoded_text)

```

**è¾“å‡º**ï¼š

```python
Tokens: ['hello', ',', 'world', '!']
Token IDs: [7592, 1010, 2088, 999]
Tokens: ['hello', ',', 'world', '!']
Decoded Text: hello, world!
```

### ä½¿ç”¨ `encode()` å’Œ `decode()` æ–¹æ³•

æ›´ç®€æ´ä¸”å¸¸è§çš„ä½¿ç”¨æ–¹å¼æ˜¯ç›´æ¥ä½¿ç”¨ `encode()` å’Œ `decode()` æ–¹æ³•ï¼š

```python
from transformers import AutoTokenizer

# å–æ¶ˆæ³¨é‡Šä»¥å¯¹æ¯”ä¸¤ç§åˆ†è¯å™¨çš„è¾“å‡ºå·®å¼‚
tokenizer = AutoTokenizer.from_pretrained("gpt2")
# tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hello, world!"

# ä½¿ç”¨ encode() å°†æ–‡æœ¬ç›´æ¥è½¬æ¢ä¸º Token IDs
token_ids = tokenizer.encode(text)
print("Token IDs:", token_ids)

# ä½¿ç”¨ decode() å°† Token IDs è½¬æ¢å›æ–‡æœ¬
decoded_text = tokenizer.decode(token_ids)
print("Decoded Text:", decoded_text)
```

**è¾“å‡º**ï¼š

```sql
Token IDs: [15496, 11, 995, 0]
Decoded Text: Hello, world!
```

## äº†è§£ Tokenizer çš„åŸºç¡€å±æ€§

å¯¼å…¥åˆ†è¯å™¨åï¼Œå¯ä»¥é€‰æ‹©æŸ¥çœ‹ä¸€äº›å±æ€§æ¥è·å¾—ç›´è§‚çš„ç†è§£ï¼Œä¾‹å¦‚æŸ¥çœ‹è¯æ±‡è¡¨ã€ç‰¹æ®Šæ ‡è®°ç­‰ï¼Œä»¥ GPT-2 ä¸ºä¾‹ã€‚

### æŸ¥çœ‹è¯æ±‡è¡¨å¤§å°

```python
# è·å–è¯æ±‡è¡¨å¤§å°
vocab_size = tokenizer.vocab_size
print("Vocabulary Size:", vocab_size)
```

**è¾“å‡º**ï¼š

```python
Vocabulary Size: 50257
```

### æŸ¥çœ‹è¯æ±‡è¡¨

```python
print("Vocabulary:", tokenizer.vocab)
```

**è¾“å‡º**ï¼š

```sql
{'Ä Naturally': 30413,
 'Ä interactive': 14333,
 'Ä Plays': 38116,
 'hemer': 39557,
 ...}
```

### æŸ¥çœ‹ç‰¹å®š Token å’Œ ID çš„æ˜ å°„å…³ç³»

```python
# æŸ¥çœ‹ç‰¹å®š Token çš„ ID
token_id = tokenizer.convert_tokens_to_ids('world')
print("Token ID for 'world':", token_id)

# æŸ¥çœ‹ç‰¹å®š ID å¯¹åº”çš„ Token
token = tokenizer.convert_ids_to_tokens(995)
print("Token for ID 995:", token)
```

**è¾“å‡º**ï¼š

```python
Token ID for 'world': 6894
Token for ID 995: Ä world
```

> [!note]
>
> è¿™é‡Œçš„ `Ä ` ä»£è¡¨ä¸€ä¸ªç©ºæ ¼å­—ç¬¦ï¼š
>
> ```python
> print(tokenizer.tokenize(' '))
> ```
>
> è¾“å‡ºä¸º `['Ä ']`

### æŸ¥çœ‹ç‰¹æ®Šæ ‡è®°ï¼ˆSpecial Tokensï¼‰

ä¸€äº›åˆ†è¯å™¨éœ€è¦ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ¥è¡¨ç¤ºå¥å­çš„å¼€å§‹ã€ç»“æŸã€å¡«å……ç­‰ã€‚

```python
# æŸ¥çœ‹æ‰€æœ‰ç‰¹æ®Šæ ‡è®°
special_tokens = tokenizer.all_special_tokens
print("All Special Tokens:", special_tokens)

# æŸ¥çœ‹ç‰¹æ®Šæ ‡è®°å¯¹åº”çš„ ID
special_token_ids = tokenizer.all_special_ids
print("Special Token IDs:", special_token_ids)
```

**è¾“å‡º**ï¼š

```python
All Special Tokens: ['<|endoftext|>']
Special Token IDs: [50256]
```

GPT-2 åªæœ‰ä¸€ä¸ªç‰¹æ®Šæ ‡è®° `<|endoftext|>`ï¼Œç”¨äºè¡¨ç¤ºæ–‡æœ¬çš„ç»“æŸã€‚

**ã€Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¢è®¨ Tokenizer çš„å…·ä½“ç»†èŠ‚ã€**

> [!note]
>
> ä¸éœ€è¦æ·±å…¥æ¥ä¸‹æ¥çš„æ‰€æœ‰ä»£ç ç»†èŠ‚ï¼Œåªéœ€è¦æŸ¥çœ‹è¾“å‡ºä¸ç›¸åº”ã€Œæ­¥éª¤ã€çš„è¡¨è¿°ã€‚

## åˆ†è¯ï¼ˆTokenizeï¼‰

æˆ‘ä»¬éœ€è¦å°†è¯­æ–™åº“ï¼ˆcorpusï¼‰çš„æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ï¼Œå‡è®¾å½“å‰è¯­æ–™åº“åŒ…å«çš„å•è¯å’Œå¯¹åº”é¢‘æ¬¡å¦‚ä¸‹ï¼š

```sql
("low", 5), ("lower", 2), ("newest", 6), ("widest", 3)
```

æœ‰äº›è®ºæ–‡ä¹Ÿç”¨ `vocab` æ¥è¡¨è¿°ï¼ŒçŸ¥é“åé¢æ˜¯é¢‘æ¬¡å³å¯ï¼Œå‘½åä¸ç”¨çº ç»“ã€‚

### æ„é€ è¯æ±‡è¡¨

#### Byte-Pair Encoding (BPE)

> **å‚è€ƒæ–‡çŒ®ï¼š**
>
> - [A new algorithm for data compression. 1994](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)
> - [Neural Machine Translation of Rare Words with Subword Units. 2015](https://arxiv.org/pdf/1508.07909v5)
>
> BPE æ˜¯ä¸€ç§åŸºäºæ•°æ®å‹ç¼©çš„æŠ€æœ¯ï¼Œæœ€æ—©ç”± Gage åœ¨ 1994 å¹´æå‡ºï¼Œåæ¥è¢«ç”¨äº GPT ç­‰æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§å­è¯åˆ†å‰²ç®—æ³•ï¼Œä»å­—ç¬¦çº§åˆ«å¼€å§‹ï¼Œé€šè¿‡è¿­ä»£åˆå¹¶é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ï¼ˆæˆ–å­—ç¬¦åºåˆ—ï¼‰æ¥æ„å»ºæ–°çš„ Tokenï¼Œä»è€Œå¯ä»¥å¤„ç†éƒ¨åˆ† OOVï¼ˆOut-Of-Vocabularyï¼‰æƒ…å†µã€‚
>
> **Q: ä»€ä¹ˆæ˜¯ OOV ï¼Ÿ**
>
> å…¶å®å°±æ˜¯ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯ï¼Œä¹Ÿç§°ä¹‹ä¸ºã€Œæœªç™»å½•è¯ã€ã€‚

BPE æ¯æ¬¡çš„è¿­ä»£ç›®æ ‡æ˜¯æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„ç›¸é‚»å­—ç¬¦å¯¹ï¼Œå®šä¹‰ Score ä»¥ä¸ WordPiece ä½œå¯¹æ¯”ï¼š

$$
\text{Score}_{\text{BPE}}(x, y) = \text{freq}(x, y)
$$

å…¶ä¸­, $\text{freq}(x, y)$ è¡¨ç¤ºå­—ç¬¦å¯¹ $(x, y)$ åœ¨è¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘æ¬¡ã€‚

##### æ­¥éª¤

1. **åˆå§‹åŒ–è¯æ±‡è¡¨ $V$**ï¼š
   - $V$ åŒ…å«è¯­æ–™åº“ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ï¼Œå³å•è¯å­—ç¬¦çš„é›†åˆã€‚
2. **ç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡**ï¼š
   - å¯¹äºæ¯ä¸ªå•è¯çš„å­—ç¬¦åºåˆ—ï¼Œç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹çš„å‡ºç°é¢‘æ¬¡ã€‚
3. **æ‰¾åˆ°é¢‘æ¬¡ï¼ˆScoreï¼‰æœ€é«˜çš„å­—ç¬¦å¯¹å¹¶åˆå¹¶**ï¼š
   - é€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ $(x, y)$ï¼Œå°†å…¶åˆå¹¶ä¸ºæ–°ç¬¦å· $xy$ã€‚
4. **æ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤æ­¥éª¤ 2 åˆ° 4**ï¼š
   - å°†æ–°ç¬¦å·æ·»åŠ åˆ°è¯æ±‡è¡¨ $V = V \cup \{xy\}$ã€‚
   - æ›´æ–°è¯­æ–™åº“ä¸­çš„å•è¯è¡¨ç¤ºï¼Œé‡å¤ç»Ÿè®¡å’Œåˆå¹¶è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œè¯æ±‡è¡¨è¾¾åˆ°é¢„å®šå¤§å°ï¼‰ã€‚

##### ç¤ºä¾‹

**æ­¥éª¤ 1ï¼šåˆå§‹åŒ–è¯æ±‡è¡¨**

- **å°†å•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åºåˆ—**ï¼š

  ```plaintext
  ("l", "o", "w"), 5  
  ("l", "o", "w", "e", "r"), 2  
  ("n", "e", "w", "e", "s", "t"), 6  
  ("w", "i", "d", "e", "s", "t"), 3
  ```

- **è¯æ±‡è¡¨ $V$**ï¼š

  ```plaintext
  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd'}
  ```

**æ­¥éª¤ 2ï¼šç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡**

ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®ç»™å®šçš„å•è¯å’Œå…¶é¢‘æ¬¡ï¼Œè‡ªåŠ¨ç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡ã€‚

```python
from collections import defaultdict

def count_char_pairs(word_freq):
    """
    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡ã€‚
    
    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯å’Œå…¶é¢‘æ¬¡
        
    è¿”å›ï¼š
        å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸
    """
    pair_freq = defaultdict(int)
    for word, freq in word_freq:
        chars = list(word)
        for i in range(len(chars) - 1):
            pair = (chars[i], chars[i + 1])
            pair_freq[pair] += freq
    return pair_freq

# ç¤ºä¾‹è¯æ±‡è¡¨å’Œå•è¯é¢‘æ¬¡
word_freq = [
    ("low", 5),
    ("lower", 2),
    ("newest", 6),
    ("widest", 3)
]

pair_freq = count_char_pairs(word_freq)
print("å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:")
for pair, freq in pair_freq.items():
    print(f"{pair}: {freq}")
```

**è¾“å‡º**ï¼š

```plaintext
å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:
('l', 'o'): 7        # 5 (low) + 2 (lower)
('o', 'w'): 7        # 5 (low) + 2 (lower)
('w', 'e'): 8        # 2 (lower) + 6 (newest)
('e', 'r'): 2
('n', 'e'): 6
('e', 'w'): 6
('e', 's'): 9        # 6 (newest) + 3 (widest)
('s', 't'): 9        # 6 (newest) + 3 (widest)
('w', 'i'): 3
('i', 'd'): 3
('d', 'e'): 3
```

**æ­¥éª¤ 3ï¼šæ‰¾åˆ°é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹å¹¶åˆå¹¶**

- **é€‰æ‹©é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹**ï¼š

  - `("e", "s")` å’Œ `("s", "t")`ï¼Œé¢‘æ¬¡å‡ä¸º 9ã€‚å¯ä»¥ä»»é€‰å…¶ä¸€è¿›è¡Œåˆå¹¶ï¼Œå‡è®¾é€‰æ‹©æ’åºç¬¬ä¸€çš„ï¼š `("e", "s")`ã€‚

- **åˆå¹¶ `("e", "s")` ä¸ºæ–°ç¬¦å· `es`**ã€‚

- **è®°å½•åˆå¹¶æ“ä½œ**ï¼š

  ```plaintext
  Merge 1: ("e", "s") -> "es"
  ```

**æ­¥éª¤ 4ï¼šæ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤**

- **æ›´æ–°å•è¯åºåˆ—**ï¼š

  ```plaintext
  ("l", "o", "w"), 5  
  ("l", "o", "w", "e", "r"), 2  
  ("n", "e", "w", "es", "t"), 6  
  ("w", "i", "d", "es", "t"), 3
  ```

- **æ›´æ–°è¯æ±‡è¡¨ $V$**ï¼š

  ```plaintext
  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es'}
  ```

- **é‡å¤æ­¥éª¤ 2 åˆ° 4ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è¯æ±‡è¡¨å¤§å°**ã€‚

> ##### ğŸ“ ç»ƒä¹ é¢˜
>
> åœä¸‹æ¥æ€è€ƒä¸€ä¸‹ï¼Œç­”æ¡ˆå’Œä»£ç ä½äº[å½“å‰æ¨¡å—æœ«å°¾](#-ç»ƒä¹ é¢˜ç­”æ¡ˆ)ã€‚
>
> **Q1.** æœ€åˆçš„è¯æ±‡è¡¨å¤§å°ä¸º 10ï¼Œå‡è®¾é¢„å®šå¤§å°ä¸º 13ï¼Œé‚£ä¹ˆå½“å‰çš„è¯æ±‡è¡¨ $V$ ä¸ºå¤šå°‘ï¼Ÿåˆå¹¶è®°å½•æ˜¯ä»€ä¹ˆï¼Ÿ
>
> **Q2.** å¦‚æœä»¥ `</w>`ï¼ˆè¡¨ç¤ºå•è¯ç»“å°¾ï¼‰ä½œä¸ºæ¯ä¸ªè¯­æ–™åº“ä¸­å•è¯çš„ç»“å°¾ï¼Œæœ€åˆçš„è¯æ±‡è¡¨ä¼šå—åˆ°ä»€ä¹ˆå½±å“ï¼Œåç»­çš„è¿‡ç¨‹ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿå‡è®¾é¢„å®šå¤§å°ä¸º 14ï¼Œå½“å‰çš„åˆå¹¶è®°å½•æ˜¯ä»€ä¹ˆï¼Ÿ

#### WordPiece

> **å‚è€ƒæ–‡çŒ®ï¼š**
>
> - [Japanese and Korean voice search. 2012](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37842.pdf)
> - [Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. 2016](https://arxiv.org/pdf/1609.08144v2)
>
> WordPiece æ˜¯ä¸€ç§å­è¯åˆ†å‰²ç®—æ³•ï¼Œæœ€åˆç”¨äºå¤„ç†æ—¥è¯­å’ŒéŸ©è¯­çš„è¯­éŸ³æœç´¢ï¼Œåæ¥åœ¨ Google çš„ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¸­å¾—åˆ°åº”ç”¨ã€‚

ä¸ BPE ä¸åŒï¼ŒWordPiece çš„ Score ç”±å­—ç¬¦å¯¹é¢‘æ¬¡ä¸å…¶ç»„æˆéƒ¨åˆ†é¢‘æ¬¡çš„æ¯”å€¼å†³å®šï¼Œå®šä¹‰ Scoreï¼š

$$
\text{Score}_{\text{WordPiece}}(x, y) = \frac{\text{freq}(xy)}{\text{freq}(x) \times \text{freq}(y)}
$$

å…¶ä¸­, $\text{freq}(x)$, $\text{freq}(y)$ å’Œ $\text{freq}(xy)$ åˆ†åˆ«è¡¨ç¤ºç¬¦å· $x$, $y$ å’Œå®ƒä»¬åˆå¹¶åçš„ç¬¦å· $xy$ çš„é¢‘æ¬¡ã€‚

##### æ­¥éª¤

1. **åˆå§‹åŒ–è¯æ±‡è¡¨ $V$**ï¼š
   - ä¸ BPE ç›¸åŒ, $V$ åŒ…å«è¯­æ–™åº“ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ï¼Œä½†å¤„ç†æ–¹å¼ç•¥æœ‰ä¸åŒï¼šå¯¹äºæ¯ä¸ªå•è¯ï¼Œé™¤äº†é¦–ä¸ªå­—ç¬¦å¤–ï¼Œå…¶ä»–å­—ç¬¦å‰éƒ½åŠ ä¸Š `##` å‰ç¼€ã€‚
2. **ç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡åŠ Score**ï¼š
   - å¯¹äºæ¯ä¸ªå¯èƒ½çš„å­—ç¬¦å¯¹ $(x, y)$ï¼Œè®¡ç®— $\text{freq}(x)$, $\text{freq}(y)$, $\text{freq}(xy)$ï¼Œå¹¶è®¡ç®— Scoreã€‚
3. **æ‰¾åˆ° Score æœ€é«˜çš„å­—ç¬¦å¯¹å¹¶åˆå¹¶**ï¼š
   - é€‰æ‹© Score æœ€é«˜çš„å­—ç¬¦å¯¹ $(x, y)$ï¼Œå°†å…¶åˆå¹¶ä¸ºæ–°ç¬¦å· $xy$ï¼Œæ³¨æ„ï¼š
     - å¦‚æœç¬¬äºŒä¸ªç¬¦å·ä»¥ `##` å¼€å¤´ï¼Œåˆå¹¶æ—¶å»æ‰ `##` å‰ç¼€å†è¿›è¡Œè¿æ¥ã€‚
     - æ–°ç¬¦å·æ˜¯å¦ä»¥ `##` å¼€å¤´ï¼Œå–å†³äºç¬¬ä¸€ä¸ªç¬¦å·æ˜¯å¦ä»¥ `##` å¼€å¤´ã€‚
4. **æ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤æ­¥éª¤ 2 åˆ° 4**ï¼š
   - å°†æ–°ç¬¦å·æ·»åŠ åˆ°è¯æ±‡è¡¨ $V = V \cup \{xy\}$ã€‚
   - æ›´æ–°è¯­æ–™åº“ä¸­çš„å•è¯è¡¨ç¤ºï¼Œé‡å¤ç»Ÿè®¡å’Œåˆå¹¶è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ã€‚

##### ç¤ºä¾‹

ä½¿ç”¨ä¸ BPE ç¤ºä¾‹ç›¸åŒçš„è¯­æ–™åº“ã€‚

**æ­¥éª¤ 1ï¼šåˆå§‹åŒ–è¯æ±‡è¡¨**

- **å°†å•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åºåˆ—**ï¼š

  ```plaintext
  ('l', '##o', '##w'), 5                       # "low"
  ('l', '##o', '##w', '##e', '##r'), 2         # "lower"
  ('n', '##e', '##w', '##e', '##s', '##t'), 6  # "newest"
  ('w', '##i', '##d', '##e', '##s', '##t'), 3  # "widest"
  ```
  
- **è¯æ±‡è¡¨ $V$**ï¼š

  ```plaintext
  {'l', '##o', '##w', '##e', '##r', 'n', '##s', '##t', 'w', '##i', '##d'}
  ```

**æ­¥éª¤ 2ï¼šç»Ÿè®¡å­—ç¬¦å’Œå­—ç¬¦å¯¹çš„é¢‘æ¬¡ï¼Œè®¡ç®— Score**

å¯ä»¥è®¾è®¡ä¸€ä¸ªå‡½æ•°å®Œæˆè¿™ä¸ªæ­¥éª¤ï¼ˆç›´æ¥è¿è¡ŒæŸ¥çœ‹è¾“å‡ºï¼‰ï¼š

```python
from collections import defaultdict

def count_char_pairs_wordpiece(word_freq):
    """
    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦çš„é¢‘æ¬¡ã€‚
    
    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡
        
    è¿”å›ï¼š
        ä¸¤ä¸ªå­—å…¸ï¼Œåˆ†åˆ«ä¸ºå­—ç¬¦å¯¹é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦é¢‘æ¬¡
    """
    pair_freq = defaultdict(int)
    char_freq = defaultdict(int)
    for word, freq in word_freq:
        for i in range(len(word)):
            char_freq[word[i]] += freq
            if i < len(word) - 1:
                pair = (word[i], word[i + 1])
                pair_freq[pair] += freq
    return pair_freq, char_freq

def compute_wordpiece_score(freq_xy, freq_x, freq_y):
    """
    æ ¹æ® WordPiece çš„å®šä¹‰è®¡ç®— Scoreã€‚
    
    å‚æ•°ï¼š
        freq_xy: ç¬¦å·å¯¹çš„é¢‘æ¬¡
        freq_x: ç¬¦å· x çš„é¢‘æ¬¡
        freq_y: ç¬¦å· y çš„é¢‘æ¬¡
        
    è¿”å›ï¼š
        è®¡ç®—å¾—åˆ°çš„ Score
    """
    if freq_x == 0 or freq_y == 0:
        return 0
    return freq_xy / (freq_x * freq_y)

# ç¤ºä¾‹è¯æ±‡è¡¨å’Œå•è¯é¢‘æ¬¡
word_freq = [
    (['l', '##o', '##w'], 5),
    (['l', '##o', '##w', '##e', '##r'], 2),
    (['n', '##e', '##w', '##e', '##s', '##t'], 6),
    (['w', '##i', '##d', '##e', '##s', '##t'], 3)
]

# ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦é¢‘æ¬¡
pair_freq, char_freq = count_char_pairs_wordpiece(word_freq)

# è®¡ç®—æ¯å¯¹å­—ç¬¦çš„ Score
scores = {}
for pair in pair_freq:
    freq_xy = pair_freq[pair]
    freq_x = char_freq[pair[0]]
    freq_y = char_freq[pair[1]]
    score = compute_wordpiece_score(freq_xy, freq_x, freq_y)
    scores[pair] = score

# è¾“å‡ºç»“æœ
print("å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:")
for pair, freq in pair_freq.items():
    print(f"{pair}: {freq}")

print("\nå•ä¸ªå­—ç¬¦é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:")
for char, freq in char_freq.items():
    print(f"{char}: {freq}")

print("\nå­—ç¬¦å¯¹ Score è®¡ç®—ç»“æœ:")
for pair, score in scores.items():
    print(f"{pair}: {score:.4f}")

```

**è¾“å‡º**ï¼š

```python
å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:
('l', '##o'): 7
('##o', '##w'): 7
('##w', '##e'): 8
('##e', '##r'): 2
('n', '##e'): 6
('##e', '##w'): 6
('##e', '##s'): 9
('##s', '##t'): 9
('w', '##i'): 3
('##i', '##d'): 3
('##d', '##e'): 3

å•ä¸ªå­—ç¬¦é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:
l: 7
##o: 7
##w: 13
##e: 17
##r: 2
n: 6
##s: 9
##t: 9
w: 3
##i: 3
##d: 3

å­—ç¬¦å¯¹ Score è®¡ç®—ç»“æœ:
('l', '##o'): 0.1429
('##o', '##w'): 0.0769
('##w', '##e'): 0.0362
('##e', '##r'): 0.0588
('n', '##e'): 0.0588
('##e', '##w'): 0.0271
('##e', '##s'): 0.0588
('##s', '##t'): 0.1111
('w', '##i'): 0.3333
('##i', '##d'): 0.3333
('##d', '##e'): 0.0588
```

- **é€‰æ‹©é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹**ï¼š

  -  `('w', '##i')` å’Œ `('##i', '##d')`ï¼ŒScore éƒ½ä¸º 0.3333ã€‚å¯ä»¥ä»»é€‰å…¶ä¸€è¿›è¡Œåˆå¹¶ï¼Œå‡è®¾é€‰æ‹©æ’åºç¬¬ä¸€çš„ï¼š `("w", "##i")`ã€‚

- **åˆå¹¶ `('w', '##i')` ä¸ºæ–°ç¬¦å· `wi`**

  - æ³¨æ„ï¼šåˆå¹¶æ—¶ï¼Œè‹¥ç¬¬äºŒä¸ªç¬¦å·ä»¥ `##` å¼€å¤´ï¼Œåˆå¹¶åçš„æ–°ç¬¦å·ä¸ºç¬¬ä¸€ä¸ªç¬¦å·åŠ ä¸Šç¬¬äºŒä¸ªç¬¦å·å»æ‰ `##` å‰ç¼€çš„éƒ¨åˆ†ã€‚

- **è®°å½•åˆå¹¶æ“ä½œï¼š**

  ```plaintext
  Merge 1: ('w', '##i') -> 'wi'
  ```

**æ­¥éª¤ 4ï¼šæ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤**

- **æ›´æ–°è¯æ±‡è¡¨ $V$**ï¼š

  ```plaintext
  {'l', '##o', '##w', '##e', '##r', 'n', '##s', '##t', 'w', '##i', '##d', 'wi'}
  ```

- **æ›´æ–°å•è¯åºåˆ—**ï¼š

  ```plaintext
  ('l', '##o', '##w'), 5                       # "low"
  ('l', '##o', '##w', '##e', '##r'), 2         # "lower"
  ('n', '##e', '##w', '##e', '##s', '##t'), 6  # "newest"
  ('wi', '##d', '##e', '##s', '##t'), 3        # "widest"
  ```

- **é‡å¤æ­¥éª¤ 2 åˆ° 4ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è¯æ±‡è¡¨å¤§å°**ã€‚

##### ä½¿ç”¨å‡½æ•°å®ç°ç®€å•çš„ WordPiece

BPE çš„å®ç°åœ¨[ç»ƒä¹ é¢˜ç­”æ¡ˆ](#-ç»ƒä¹ é¢˜ç­”æ¡ˆ)ä¸­ã€‚

```python
from collections import defaultdict

def create_new_symbol(x, y):
    """
    æ ¹æ® WordPiece çš„è§„åˆ™åˆ›å»ºæ–°ç¬¦å·ã€‚

    - å¦‚æœ y ä»¥ '##' å¼€å¤´ï¼Œåˆå¹¶æ—¶éœ€è¦å»æ‰ y çš„ '##' å‰ç¼€ã€‚
    - æ–°ç¬¦å·æ˜¯å¦ä»¥ '##' å¼€å¤´ï¼Œå–å†³äº x æ˜¯å¦ä»¥ '##' å¼€å¤´ã€‚
    """
    x_starts_hash = x.startswith('##')
    x_without_hash = x[2:] if x_starts_hash else x
    y_without_hash = y[2:] if y.startswith('##') else y
    new_symbol = x_without_hash + y_without_hash
    if x_starts_hash:
        new_symbol = '##' + new_symbol
    return new_symbol

def count_char_pairs_wordpiece(word_freq):
    """
    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦çš„é¢‘æ¬¡ã€‚
    
    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡
        
    è¿”å›ï¼š
        ä¸¤ä¸ªå­—å…¸ï¼Œåˆ†åˆ«ä¸ºå­—ç¬¦å¯¹é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦é¢‘æ¬¡
    """
    pair_freq = defaultdict(int)
    char_freq = defaultdict(int)
    for word, freq in word_freq:
        for i in range(len(word)):
            char_freq[word[i]] += freq
            if i < len(word) - 1:
                pair = (word[i], word[i + 1])
                pair_freq[pair] += freq
    return pair_freq, char_freq

def compute_wordpiece_score(freq_xy, freq_x, freq_y):
    """
    æ ¹æ® WordPiece çš„å®šä¹‰è®¡ç®— Scoreã€‚
    
    å‚æ•°ï¼š
        freq_xy: ç¬¦å·å¯¹çš„é¢‘æ¬¡
        freq_x: ç¬¦å· x çš„é¢‘æ¬¡
        freq_y: ç¬¦å· y çš„é¢‘æ¬¡
        
    è¿”å›ï¼š
        è®¡ç®—å¾—åˆ°çš„ Score
    """
    if freq_x == 0 or freq_y == 0:
        return 0
    return freq_xy / (freq_x * freq_y)

def find_best_pair_wordpiece(pair_freq, char_freq):
    """
    æ‰¾åˆ°å…·æœ‰æœ€é«˜ Score çš„å­—ç¬¦å¯¹ã€‚

    å‚æ•°ï¼š
        pair_freq: å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸
        char_freq: å•ä¸ªå­—ç¬¦é¢‘æ¬¡çš„å­—å…¸
        
    è¿”å›ï¼š
        å…·æœ‰æœ€é«˜ Score çš„å­—ç¬¦å¯¹åŠå…¶ Score
    """
    scores = {}
    for pair, freq_xy in pair_freq.items():
        x, y = pair
        freq_x = char_freq.get(x, 0)
        freq_y = char_freq.get(y, 0)
        score = compute_wordpiece_score(freq_xy, freq_x, freq_y)
        scores[pair] = score
    if not scores:
        return None, 0
    best_pair = max(scores, key=scores.get)
    return best_pair, scores[best_pair]

def merge_pair_wordpiece(word_freq, pair_to_merge):
    """
    åˆå¹¶æŒ‡å®šçš„å­—ç¬¦å¯¹åˆ°æ–°ç¬¦å·ã€‚

    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡
        pair_to_merge: è¦åˆå¹¶çš„å­—ç¬¦å¯¹
    è¿”å›ï¼š
        æ›´æ–°åçš„å•è¯é¢‘æ¬¡åˆ—è¡¨
    """
    merged_word_freq = []
    new_symbol = create_new_symbol(pair_to_merge[0], pair_to_merge[1])
    for word, freq in word_freq:
        new_word = []
        i = 0
        while i < len(word):
            # æ£€æŸ¥å½“å‰å­—ç¬¦å’Œä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯è¦åˆå¹¶çš„å­—ç¬¦å¯¹
            if (
                i < len(word) - 1
                and word[i] == pair_to_merge[0]
                and word[i + 1] == pair_to_merge[1]
            ):
                new_word.append(new_symbol)
                i += 2  # è·³è¿‡ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œå› ä¸ºå·²åˆå¹¶
            else:
                new_word.append(word[i])
                i += 1
        merged_word_freq.append((new_word, freq))
    return merged_word_freq

def wordpiece_merge(word_freq, vocab_size):
    """
    æ‰§è¡Œ WordPiece åˆå¹¶æ“ä½œï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°é¢„å®šå¤§å°ã€‚

    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡
        vocab_size: é¢„å®šçš„è¯æ±‡è¡¨å¤§å°
        
    è¿”å›ï¼š
        æœ€ç»ˆè¯æ±‡è¡¨å’Œåˆå¹¶è®°å½•
    """
    # åˆå§‹åŒ–è¯æ±‡è¡¨
    vocab = set()
    for word, _ in word_freq:
        vocab.update(word)
    merges = []

    while len(vocab) < vocab_size:
        pair_freq, char_freq = count_char_pairs_wordpiece(word_freq)
        best_pair, best_score = find_best_pair_wordpiece(pair_freq, char_freq)
        if not best_pair:
            break
        # åˆå¹¶æœ€ä½³å­—ç¬¦å¯¹
        new_symbol = create_new_symbol(best_pair[0], best_pair[1])
        word_freq = merge_pair_wordpiece(word_freq, best_pair)
        vocab.add(new_symbol)
        merges.append((best_pair, new_symbol))
        print(
            f"Merge: {best_pair} -> {new_symbol}, Score: {best_score:.4f}, è¯æ±‡è¡¨å¤§å°: {len(vocab)}"
        )

    return vocab, merges

# ç¤ºä¾‹
word_freq = [
    (['l', '##o', '##w'], 5),
    (['l', '##o', '##w', '##e', '##r'], 2),
    (['n', '##e', '##w', '##e', '##s', '##t'], 6),
    (['w', '##i', '##d', '##e', '##s', '##t'], 3)
]

# é¢„å®šè¯æ±‡è¡¨å¤§å°ä¸º15
final_vocab_wp, merge_records_wp = wordpiece_merge(word_freq, 15)

print("\næœ€ç»ˆè¯æ±‡è¡¨ V:")
print(final_vocab_wp)

print("\nåˆå¹¶è®°å½•:")
for idx, (pair, new_sym) in enumerate(merge_records_wp, 1):
    print(f"Merge {idx}: {pair} -> {new_sym}")
```

**è¾“å‡º**ï¼š

```
Merge: ('w', '##i') -> wi, Score: 0.3333, è¯æ±‡è¡¨å¤§å°: 12
Merge: ('wi', '##d') -> wid, Score: 0.3333, è¯æ±‡è¡¨å¤§å°: 13
Merge: ('l', '##o') -> lo, Score: 0.1429, è¯æ±‡è¡¨å¤§å°: 14
Merge: ('##s', '##t') -> ##st, Score: 0.1111, è¯æ±‡è¡¨å¤§å°: 15

æœ€ç»ˆè¯æ±‡è¡¨ V:
{'##st', 'n', '##i', '##s', 'wid', '##d', 'wi', '##r', '##o', 'lo', 'w', '##e', '##w', '##t', 'l'}

åˆå¹¶è®°å½•:
Merge 1: ('w', '##i') -> wi
Merge 2: ('wi', '##d') -> wid
Merge 3: ('l', '##o') -> lo
Merge 4: ('##s', '##t') -> ##st
```

#### ğŸ“ ç»ƒä¹ é¢˜ç­”æ¡ˆ

**Q1. æœ€åˆçš„è¯æ±‡è¡¨å¤§å°ä¸º 10ï¼Œå‡è®¾é¢„å®šå¤§å°ä¸º 13ï¼Œé‚£ä¹ˆå½“å‰çš„è¯æ±‡è¡¨ $V$ ä¸ºå¤šå°‘ï¼Ÿåˆå¹¶è®°å½•å‘¢ï¼Ÿ**

- **åˆå§‹è¯æ±‡è¡¨ $V$**ï¼š

  ```plaintext
  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd'}
  ```

  å¤§å°ä¸º 10ã€‚

- **åˆå¹¶è®°å½•**ï¼š

  1. åˆå¹¶ `("e", "s")` -> `es`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 11ã€‚
  2. åˆå¹¶ `("es", "t")` -> `est`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 12ã€‚
  3. åˆå¹¶ `("l", "o")` -> `lo`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 13ã€‚

- **æœ€ç»ˆè¯æ±‡è¡¨ $V$**ï¼š

  ```plaintext
  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo'}
  ```

è¿è¡Œä»£ç ï¼š

```python
from collections import defaultdict

def count_char_pairs(word_freq):
    """
    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡ã€‚
    
    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡
    
    è¿”å›ï¼š
        å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸
    """
    pair_freq = defaultdict(int)
    for word, freq in word_freq:
        for i in range(len(word) - 1):
            pair = (word[i], word[i + 1])
            pair_freq[pair] += freq
    return pair_freq

def find_best_pair(freq):
    """
    æ‰¾åˆ°é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹ã€‚
    
    å‚æ•°ï¼š
        freq: å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸
        
    è¿”å›ï¼š
        é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹åŠå…¶é¢‘æ¬¡
    """
    if not freq:
        return None, 0
    best_pair = max(freq, key=freq.get)
    return best_pair, freq[best_pair]

def merge_pair(word_freq, pair_to_merge):
    """
    åˆå¹¶æŒ‡å®šçš„å­—ç¬¦å¯¹åˆ°æ–°ç¬¦å·ã€‚
    
    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡
        pair_to_merge: è¦åˆå¹¶çš„å­—ç¬¦å¯¹
    
    è¿”å›ï¼š
        æ›´æ–°åçš„å•è¯é¢‘æ¬¡åˆ—è¡¨
    """
    merged_word_freq = []
    pair_str = ''.join(pair_to_merge)
    for word, freq in word_freq:
        new_word = []
        i = 0
        while i < len(word):
            # æ£€æŸ¥å½“å‰å­—ç¬¦å’Œä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯è¦åˆå¹¶çš„å­—ç¬¦å¯¹
            if i < len(word) - 1 and word[i] == pair_to_merge[0] and word[i + 1] == pair_to_merge[1]:
                new_word.append(pair_str)
                i += 2  # è·³è¿‡ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œå› ä¸ºå·²åˆå¹¶
            else:
                new_word.append(word[i])
                i += 1
        merged_word_freq.append((new_word, freq))
    return merged_word_freq

def bpe_merge(word_freq, vocab_size):
    """
    æ‰§è¡Œ BPE åˆå¹¶æ“ä½œï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°é¢„å®šå¤§å°ã€‚
    
    å‚æ•°ï¼š
        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡
        vocab_size: é¢„å®šçš„è¯æ±‡è¡¨å¤§å°
    
    è¿”å›ï¼š
        æœ€ç»ˆè¯æ±‡è¡¨å’Œåˆå¹¶è®°å½•
    """
    # åˆå§‹åŒ–è¯æ±‡è¡¨
    vocab = set()
    for word, _ in word_freq:
        vocab.update(word)
    merges = []
    
    while len(vocab) < vocab_size:
        pair_freq = count_char_pairs(word_freq)
        best_pair, best_freq = find_best_pair(pair_freq)
        if not best_pair:
            break
        # åˆå¹¶æœ€ä½³å­—ç¬¦å¯¹
        word_freq = merge_pair(word_freq, best_pair)
        new_symbol = ''.join(best_pair)
        vocab.add(new_symbol)
        merges.append((best_pair, new_symbol))
        print(f"Merge: {best_pair} -> {new_symbol}, è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
            
    return vocab, merges

# ç¤ºä¾‹
word_freq = [
    (['l', 'o', 'w'], 5),
    (['l', 'o', 'w', 'e', 'r'], 2),
    (['n', 'e', 'w', 'e', 's', 't'], 6),
    (['w', 'i', 'd', 'e', 's', 't'], 3)
]

# é¢„å®šè¯æ±‡è¡¨å¤§å°ä¸º13
final_vocab, merge_records = bpe_merge(word_freq, 13)

print("\næœ€ç»ˆè¯æ±‡è¡¨ V:")
print(final_vocab)

print("\nåˆå¹¶è®°å½•:")
for idx, (pair, new_sym) in enumerate(merge_records, 1):
    print(f"Merge {idx}: {pair} -> {new_sym}")

```

**è¾“å‡º**ï¼š

```python
Merge: ('e', 's') -> es, è¯æ±‡è¡¨å¤§å°: 11
Merge: ('es', 't') -> est, è¯æ±‡è¡¨å¤§å°: 12
Merge: ('l', 'o') -> lo, è¯æ±‡è¡¨å¤§å°: 13

æœ€ç»ˆè¯æ±‡è¡¨ V:  # å› ä¸ºæ˜¯å­—å…¸ï¼Œæ‰€ä»¥å®é™…é¡ºåºä¼šå’Œä¹‹å‰å±•ç¤ºçš„ä¸ä¸€è‡´
{'e', 'r', 's', 'est', 'w', 'l', 'o', 'lo', 'es', 'i', 'n', 't', 'd'}

åˆå¹¶è®°å½•:
Merge 1: ('e', 's') -> es
Merge 2: ('es', 't') -> est
Merge 3: ('l', 'o') -> lo
```

**Q2. å¦‚æœä»¥`</w>`ï¼ˆend-of-wordï¼‰ä½œä¸ºæ¯ä¸ªè¯­æ–™åº“ä¸­å•è¯çš„ç»“å°¾ï¼Œæœ€åˆçš„è¯æ±‡è¡¨ä¼šå—åˆ°ä»€ä¹ˆå½±å“ï¼Œåç»­çš„è¿‡ç¨‹å‘¢ï¼Ÿå‡è®¾é¢„å®šå¤§å°ä¸º 14ï¼Œå½“å‰çš„åˆå¹¶è®°å½•æ˜¯ä»€ä¹ˆï¼Ÿ**

- **åˆå§‹è¯æ±‡è¡¨ $V$**ï¼š

  æ·»åŠ  `</w>` åï¼Œè¯æ±‡è¡¨å˜ä¸ºï¼š

  ```plaintext
  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', '</w>'}
  ```

  å¤§å°ä¸º 11ã€‚

- **å½±å“**ï¼š

  åˆå¹¶è¿‡ç¨‹å’Œåˆå¹¶è®°å½•å°†ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå› ä¸º `</w>` çš„å­˜åœ¨ä¼šå½±å“å­—ç¬¦å¯¹çš„é¢‘æ¬¡ç»Ÿè®¡å’Œåˆå¹¶é¡ºåºã€‚

- **åˆå¹¶è®°å½•**ï¼š

  1. åˆå¹¶ `("e", "s")` -> `es`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 12ã€‚
  2. åˆå¹¶ `("es", "t")` -> `est`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 13ã€‚
  3. åˆå¹¶ `("est", "<\w>")` -> `est<\w>`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 14ã€‚

è¿è¡Œä»£ç ï¼š
```python
# ç¤ºä¾‹
word_freq = [
    (['l', 'o', 'w', '</w>'], 5),
    (['l', 'o', 'w', 'e', 'r', '</w>'], 2),
    (['n', 'e', 'w', 'e', 's', 't', '</w>'], 6),
    (['w', 'i', 'd', 'e', 's', 't', '</w>'], 3)
]

# é¢„å®šè¯æ±‡è¡¨å¤§å°ä¸º14
final_vocab, merge_records = bpe_merge(word_freq, 14)

print("\næœ€ç»ˆè¯æ±‡è¡¨ V:")
print(final_vocab)

print("\nåˆå¹¶è®°å½•:")
for idx, (pair, new_sym) in enumerate(merge_records, 1):
    print(f"Merge {idx}: {pair} -> {new_sym}")
```

**è¾“å‡º**ï¼š

```
Merge: ('e', 's') -> es, è¯æ±‡è¡¨å¤§å°: 12
Merge: ('es', 't') -> est, è¯æ±‡è¡¨å¤§å°: 13
Merge: ('est', '</w>') -> est</w>, è¯æ±‡è¡¨å¤§å°: 14

æœ€ç»ˆè¯æ±‡è¡¨ V:
{'e', 'r', 's', 'est', 'w', 'l', 'est</w>', 'o', 'es', 'i', 'n', 't', 'd', '</w>'}

åˆå¹¶è®°å½•:
Merge 1: ('e', 's') -> es
Merge 2: ('es', 't') -> est
Merge 3: ('est', '</w>') -> est</w>
```

### æ ‡è®°æ–‡æœ¬

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæ¯æ¬¡åˆå¹¶æ—¶æˆ‘ä»¬éƒ½ä¼šè®°å½•å¯¹åº”çš„ **merge** è§„åˆ™ï¼Œä½†å¹¶æœªè¯¦ç»†è¯´æ˜å…¶ä½œç”¨ï¼Œä¸‹é¢å°†ä»¥ BPE ä¸ºä¾‹è¿›è¡Œè§£é‡Šã€‚

#### BPE

åœ¨ä¹‹å‰çš„ç¤ºä¾‹ä¸­ï¼Œä¸‰è½®åˆå¹¶åå°†å¾—åˆ°ä»¥ä¸‹åˆå¹¶è§„åˆ™ï¼ˆæŒ‰åˆå¹¶é¡ºåºæ’åˆ—ï¼‰ï¼š  

1. åˆå¹¶å­—ç¬¦å¯¹ `'e'` å’Œ `'s'`ï¼Œå¾—åˆ° `'es'`ã€‚
2. åˆå¹¶å­—ç¬¦å¯¹ `'es'` å’Œ `'t'`ï¼Œå¾—åˆ° `'est'`ã€‚
3. åˆå¹¶å­—ç¬¦å¯¹ `'l'` å’Œ `'o'`ï¼Œå¾—åˆ° `'lo'`ã€‚

å‡è®¾å½“å‰è¯æ±‡è¡¨åŒ…å«æ‰€æœ‰å•ä¸ªå­—ç¬¦ï¼Œä¿®æ”¹[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/learn/nlp-course/en/chapter6/5)æœ€åæä¾›çš„ tokenize() ç¤ºä¾‹ä»£ç è¿›è¡Œæ¼”ç¤ºï¼š

```python
def tokenize(text):
    # é¢„åˆ†è¯å¤„ç†ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºåˆæ­¥çš„å•è¯åˆ—è¡¨
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]

    print("åˆå§‹é¢„åˆ†è¯ç»“æœ:")
    print(pre_tokenized_text)

    # å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åˆ—è¡¨
    splits = [[l for l in word] for word in pre_tokenized_text]
    print("\nåˆå§‹æ‹†åˆ†ç»“æœ:")
    print(splits)

    # éå†æ‰€æœ‰åˆå¹¶è§„åˆ™ï¼ˆmergesï¼‰ï¼Œé€æ­¥åº”ç”¨åˆ°æ‹†åˆ†åçš„ç»“æœä¸­
    for pair, merge in merges.items():
        print(f"\nåº”ç”¨åˆå¹¶è§„åˆ™: {pair} -> {merge}")

        # éå†æ¯ä¸ªå·²æ‹†åˆ†çš„å•è¯
        for idx, split in enumerate(splits):
            print(f"  åˆå¹¶å‰ç¬¬ {idx+1} ä¸ªå•è¯: {split}")
            i = 0
            # åœ¨å½“å‰æ‹†åˆ†çš„å­—ç¬¦ä¸­æŸ¥æ‰¾åŒ¹é…çš„å­—ç¬¦å¯¹
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    # åˆå¹¶å­—ç¬¦å¯¹
                    split = split[:i] + [merge] + split[i + 2 :]
                    print(f"    åœ¨ä½ç½® {i} å¤„åˆå¹¶: {split}")
                else:
                    i += 1
            # æ›´æ–°æ‹†åˆ†åçš„ç»“æœ
            splits[idx] = split

    print("\næœ€ç»ˆæ‹†åˆ†ç»“æœ:")
    print(splits)

    # å°†æ‰€æœ‰æ‹†åˆ†åçš„ç»“æœåˆå¹¶ä¸ºä¸€ä¸ª Token åˆ—è¡¨å¹¶è¿”å›
    return sum(splits, [])

# ç¤ºä¾‹ merges å­—å…¸
merges = {
    ('e', 's'): 'es',
    ('es', 't'): 'est',
    ('l', 'o'): 'lo'
}

# ç¤ºä¾‹æ–‡æœ¬
text = "estimate, local"

# è°ƒç”¨ tokenize å‡½æ•°ï¼Œå¹¶æ‰“å°ä¸­é—´è¿‡ç¨‹
tokens = tokenize(text)
print("\næœ€ç»ˆç”Ÿæˆçš„ Tokens:")
print(tokens)

```

**è¾“å‡º**ï¼š

```python
åˆå§‹é¢„åˆ†è¯ç»“æœ:
['estimate', ',', 'local']

åˆå§‹æ‹†åˆ†ç»“æœ:
[['e', 's', 't', 'i', 'm', 'a', 't', 'e'], [','], ['l', 'o', 'c', 'a', 'l']]

åº”ç”¨åˆå¹¶è§„åˆ™: ('e', 's') -> es
  åˆå¹¶å‰ç¬¬ 1 ä¸ªå•è¯: ['e', 's', 't', 'i', 'm', 'a', 't', 'e']
    åœ¨ä½ç½® 0 å¤„åˆå¹¶: ['es', 't', 'i', 'm', 'a', 't', 'e']
  åˆå¹¶å‰ç¬¬ 2 ä¸ªå•è¯: [',']
  åˆå¹¶å‰ç¬¬ 3 ä¸ªå•è¯: ['l', 'o', 'c', 'a', 'l']

åº”ç”¨åˆå¹¶è§„åˆ™: ('es', 't') -> est
  åˆå¹¶å‰ç¬¬ 1 ä¸ªå•è¯: ['es', 't', 'i', 'm', 'a', 't', 'e']
    åœ¨ä½ç½® 0 å¤„åˆå¹¶: ['est', 'i', 'm', 'a', 't', 'e']
  åˆå¹¶å‰ç¬¬ 2 ä¸ªå•è¯: [',']
  åˆå¹¶å‰ç¬¬ 3 ä¸ªå•è¯: ['l', 'o', 'c', 'a', 'l']

åº”ç”¨åˆå¹¶è§„åˆ™: ('l', 'o') -> lo
  åˆå¹¶å‰ç¬¬ 1 ä¸ªå•è¯: ['est', 'i', 'm', 'a', 't', 'e']
  åˆå¹¶å‰ç¬¬ 2 ä¸ªå•è¯: [',']
  åˆå¹¶å‰ç¬¬ 3 ä¸ªå•è¯: ['l', 'o', 'c', 'a', 'l']
    åœ¨ä½ç½® 0 å¤„åˆå¹¶: ['lo', 'c', 'a', 'l']

æœ€ç»ˆæ‹†åˆ†ç»“æœ:
[['est', 'i', 'm', 'a', 't', 'e'], [','], ['lo', 'c', 'a', 'l']]

æœ€ç»ˆç”Ÿæˆçš„ Tokens:
['est', 'i', 'm', 'a', 't', 'e', ',', 'lo', 'c', 'a', 'l']
```

ä¸è¿‡ï¼Œåœ¨ä¹‹å‰çš„è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æœ€ç»ˆè¯æ±‡è¡¨ $V$ å¹¶æœªåŒ…å«æ‰€æœ‰å•ä¸ªå­—ç¬¦ï¼Œè€Œæ˜¯ï¼š  

```sql
{'e', 'r', 's', 'est', 'w', 'l', 'o', 'lo', 'es', 'i', 'n', 't', 'd'}
```

å› æ­¤ï¼Œå¯¹äºè¾“å…¥ `"estimate, local"`ï¼Œå…¶æ ‡è®°ç»“æœä¸ºï¼š  

```sql
['est', 'i', '[UNK]', 'a', 't', 'e', '[UNK]', 'lo', '[UNK]', '[UNK]', l]
```

è¿™é‡Œçš„ `'[UNK]'`ï¼ˆUNKNOWNï¼‰è¡¨ç¤ºè¯¥å­è¯ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå³å±äº **OOVï¼ˆOut-of-Vocabularyï¼‰** çš„æƒ…å†µã€‚

#### WordPiece

å’Œ BPE ä¸åŒï¼ŒWordPiece å¯¹ OOV é‡‡å–çš„æ˜¯ã€Œå®æ€é”™ä¸æ”¾è¿‡ã€ç­–ç•¥ï¼Œå³åªè¦æœ‰ä¸€ä¸ªå­—ç¬¦æ²¡è§è¿‡ï¼Œæ•´ä¸ªå•è¯éƒ½æ ‡è®°ä¸º `'[UNK]'`ã€‚

ä¿®æ”¹[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/learn/nlp-course/en/chapter6/6)æœ€åæä¾›çš„ tokenize() ç¤ºä¾‹ä»£ç è¿›è¡Œæ¼”ç¤ºï¼š

```python
from transformers import AutoTokenizer

def tokenize(text):
    # é¢„åˆ†è¯å¤„ç†ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºåˆæ­¥çš„å•è¯åˆ—è¡¨
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]

    print("\nåˆå§‹é¢„åˆ†è¯ç»“æœ:")
    print(pre_tokenized_text)

    # å¯¹æ¯ä¸ªå•è¯è¿›è¡Œæ ‡è®°
    tokenized_words = []
    for word in pre_tokenized_text:
        tokens = []
        print(f"\næ­£åœ¨æ ‡è®°å•è¯: {word}")
        
        while len(word) > 0:
            i = len(word)
            # å°è¯•åŒ¹é…è¯æ±‡è¡¨ä¸­çš„æœ€é•¿å­è¯
            while i > 0 and word[:i] not in vocab:
                i -= 1
            if i == 0:
                print(f"  [UNK] æ ‡è®°: {word}")
                tokens = ["[UNK]"]  # æ²¡æœ‰åŒ¹é…åˆ°åˆ™è¿”å› [UNK]
                break  # è·³å‡ºå¾ªç¯ï¼Œä¸å†ç»§ç»­å¤„ç†è¯¥å•è¯

            # åŒ¹é…åˆ°å­è¯ï¼Œæ·»åŠ åˆ° tokens åˆ—è¡¨ä¸­
            matched_token = word[:i]
            tokens.append(matched_token)
            print(f"  åŒ¹é…åˆ° Token: {matched_token}")

            # æ›´æ–°å‰©ä½™éƒ¨åˆ†ï¼Œå¹¶æ·»åŠ â€œ##â€ä½œä¸ºå‰ç¼€
            word = word[i:]
            if len(word) > 0:
                word = f"##{word}"
                print(f"  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: {word}")

        print(f"  æ ‡è®°ç»“æœ: {tokens}")
        tokenized_words.append(tokens)

    print("\næœ€ç»ˆæ ‡è®°ç»“æœ:")
    flattened_tokens = sum(tokenized_words, [])  # å±•å¹³æˆå•å±‚åˆ—è¡¨
    print(flattened_tokens)

    return flattened_tokens

# ç¤ºä¾‹è¯æ±‡è¡¨
vocab = {'##st', 'n', '##i', '##s', 'wid', '##d', 'wi', '##r', '##o', 
         'lo', 'w', '##e', '##w', '##t', 'l'}

# ç¤ºä¾‹æ–‡æœ¬
text = "estimate, local, lows"

# ä½¿ç”¨ BERT çš„åˆ†è¯å™¨ï¼ˆWordPieceï¼‰
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# è°ƒç”¨ tokenize å‡½æ•°ï¼Œå¹¶æ‰“å°ä¸­é—´è¿‡ç¨‹
tokens = tokenize(text)

```

**è¾“å‡º**ï¼š

```sql
åˆå§‹é¢„åˆ†è¯ç»“æœ:
['estimate', ',', 'local', ',', 'lows']

æ­£åœ¨æ ‡è®°å•è¯: estimate
  [UNK] æ ‡è®°: estimate
  æ ‡è®°ç»“æœ: ['[UNK]']

æ­£åœ¨æ ‡è®°å•è¯: ,
  [UNK] æ ‡è®°: ,
  æ ‡è®°ç»“æœ: ['[UNK]']

æ­£åœ¨æ ‡è®°å•è¯: local
  åŒ¹é…åˆ° Token: lo
  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: ##cal
  [UNK] æ ‡è®°: ##cal
  æ ‡è®°ç»“æœ: ['[UNK]']

æ­£åœ¨æ ‡è®°å•è¯: ,
  [UNK] æ ‡è®°: ,
  æ ‡è®°ç»“æœ: ['[UNK]']

æ­£åœ¨æ ‡è®°å•è¯: lows
  åŒ¹é…åˆ° Token: lo
  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: ##ws
  åŒ¹é…åˆ° Token: ##w
  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: ##s
  åŒ¹é…åˆ° Token: ##s
  æ ‡è®°ç»“æœ: ['lo', '##w', '##s']

æœ€ç»ˆæ ‡è®°ç»“æœ:
['[UNK]', '[UNK]', '[UNK]', '[UNK]', 'lo', '##w', '##s']
```

## æ˜ å°„ï¼ˆMappingï¼‰

ä»¥ BPE ä¸ºä¾‹ï¼Œæœ€ç»ˆè¯æ±‡è¡¨ $V$ ä¸­çš„ Token å’Œå¯¹åº”çš„é¢‘æ¬¡åˆ†åˆ«ä¸ºï¼š

```
vocab = {
    'lo': 7,
    'w': 16,
    'e': 8,
    'r': 2,
    'n': 6,
    'est': 9,
    'i': 3,
    'd': 3
}
```

ç®€å•å®ç° Token å’Œ ID ä¹‹é—´çš„æ˜ å°„å…³ç³»çš„ä»£ç ï¼š

```python
# åˆ›å»º token åˆ° ID çš„æ˜ å°„
token_to_id = {token: idx for idx, token in enumerate(vocab)}

# åˆ›å»º ID åˆ° token çš„æ˜ å°„
id_to_token = {idx: token for token, idx in token_to_id.items()}

# æ‰“å°æ˜ å°„å…³ç³»
print("Token to ID:", token_to_id)
print("ID to Token:", id_to_token)
```

**è¾“å‡º**ï¼š

```
Token to ID: {'lo': 0, 'w': 1, 'e': 2, 'r': 3, 'n': 4, 'est': 5, 'i': 6, 'd': 7}
ID to Token: {0: 'lo', 1: 'w', 2: 'e', 3: 'r', 4: 'n', 5: 'est', 6: 'i', 7: 'd'}
```

å½“ç„¶ï¼Œä¹Ÿå¯ä»¥æ ¹æ®é¢‘æ¬¡æˆ–è€…å…¶ä»–è§„åˆ™è¿›è¡Œç‰¹æ®Šå¤„ç†ã€‚

ä»¥ä¸Šæ˜¯ç¼–ç éƒ¨åˆ†çš„æ¦‚è¿°ï¼Œå®é™…ä¸Šåœ¨æ–‡æœ¬é¢„å¤„ç†çš„æ—¶å€™è¿˜ä¼šå¢åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä½†è¿™äº›ä»¥åŠåç»­çš„è§£ç éƒ¨åˆ†å¤§å¤šæ˜¯ä¸€äº›æ–‡æœ¬å¤„ç†çš„è§„åˆ™ï¼Œè¿™é‡Œå°±ä¸è¿‡å¤šèµ˜è¿°äº†ï¼ŒTokenizer ä¹‹é—´çš„æ ¸å¿ƒå·®å¼‚åœ¨äºä½¿ç”¨çš„åˆ†å‰²æ–¹æ³•å’Œè¯æ±‡è¡¨çš„æ„å»ºç­–ç•¥ã€‚

## æ‹“å±•ï¼ˆTransformersï¼‰

åœ¨ Transformers ä¸­ï¼Œ**åˆ†è¯ï¼ˆtokenizationï¼‰** å®é™…ä¸ŠåŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š  

1. **æ ‡å‡†åŒ–ï¼ˆNormalizationï¼‰**ï¼šå¯¹æ–‡æœ¬è¿›è¡Œå¿…è¦çš„æ¸…ç†æ“ä½œï¼Œä¾‹å¦‚åˆ é™¤å¤šä½™ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€è¿›è¡Œ Unicode æ ‡å‡†åŒ–ç­‰ã€‚
2. **é¢„åˆ†è¯ï¼ˆPre-tokenizationï¼‰**ï¼šå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ã€‚
3. **é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥ï¼ˆRunning the input through the modelï¼‰**ï¼šä½¿ç”¨é¢„åˆ†è¯åçš„å•è¯ç”Ÿæˆä¸€ç³»åˆ—è¯å…ƒï¼ˆtokensï¼‰ã€‚
4. **åå¤„ç†ï¼ˆPost-processingï¼‰**ï¼šæ·»åŠ åˆ†è¯å™¨çš„ç‰¹æ®Šæ ‡è®°ï¼Œç”Ÿæˆæ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰å’Œè¯å…ƒç±»å‹ IDï¼ˆtoken type IDsï¼‰ã€‚

[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/learn/nlp-course/en/chapter6/8)ç»™å‡ºäº†ä¸€å¼ æ•´ä½“æµç¨‹å›¾ï¼š

![en_chapter6_tokenization_pipeline](./assets/20241022225857.svg)

è¿è¡Œä»£ç ï¼š

```python
from transformers import AutoTokenizer

# åŠ è½½ BERT çš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# åŸå§‹æ–‡æœ¬
text = "Hello how are U tday"
print("åŸå§‹æ–‡æœ¬:", text)

# 1. æ ‡å‡†åŒ–ï¼šè½¬æ¢ä¸ºå°å†™
normalized_text = text.lower()
print("æ ‡å‡†åŒ–åçš„æ–‡æœ¬:", normalized_text)

# 2. é¢„åˆ†è¯ï¼ˆPre-tokenizationï¼‰ï¼šå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯
pre_tokenized = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized_text)
print("é¢„åˆ†è¯ç»“æœ:", pre_tokenized)

# 3. åˆ†è¯ï¼šå°†é¢„åˆ†è¯åçš„ç»“æœè½¬æ¢ä¸ºå­è¯çº§è¯å…ƒ
tokens = tokenizer.tokenize(normalized_text)
print("è¯å…ƒï¼ˆTokensï¼‰:", tokens)

# 4. å°† tokens è½¬æ¢ä¸º token IDs
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print("è¯å…ƒ IDï¼ˆToken IDsï¼‰:", token_ids)

# 5. ç¼–ç ï¼ˆåŒ…å«ç‰¹æ®Šæ ‡è®°å’Œåå¤„ç†ï¼‰
encoded = tokenizer(normalized_text, return_tensors="pt")
print("ç¼–ç ç»“æœ:", encoded)

# 6. æ‰“å°æ³¨æ„åŠ›æ©ç å’Œè¯å…ƒç±»å‹ IDï¼ˆåå¤„ç†éƒ¨åˆ†ï¼‰
print("æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰:", encoded["attention_mask"])
print("è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰:", encoded["token_type_ids"])

# 7. è§£ç ï¼šå°† token IDs è½¬æ¢å›æ–‡æœ¬
decoded_text = tokenizer.decode(token_ids)
print("è§£ç åçš„æ–‡æœ¬:", decoded_text)

```

**è¾“å‡º**ï¼š

```sql
åŸå§‹æ–‡æœ¬: Hello how are U tday
æ ‡å‡†åŒ–åçš„æ–‡æœ¬: hello how are u tday
é¢„åˆ†è¯ç»“æœ: [('hello', (0, 5)), ('how', (6, 9)), ('are', (10, 13)), ('u', (14, 15)), ('tday', (16, 20))]
è¯å…ƒï¼ˆTokensï¼‰: ['hello', 'how', 'are', 'u', 'td', '##ay']
è¯å…ƒ IDï¼ˆToken IDsï¼‰: [7592, 2129, 2024, 1057, 14595, 4710]
ç¼–ç ç»“æœ: {'input_ids': tensor([[  101,  7592,  2129,  2024,  1057, 14595,  4710,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}
æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])
è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰: tensor([[0, 0, 0, 0, 0, 0, 0, 0]])
è§£ç åçš„æ–‡æœ¬: hello how are u tday
```

### Qï¼šæ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰å’Œè¯å…ƒç±»å‹ ID ï¼ˆToken Type IDsï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ

> ![Bert](./assets/Bert.png)

**æ³¨æ„åŠ›æ©ç **ç¡®ä¿æ¨¡å‹åªå…³æ³¨å®é™…çš„è¯å…ƒï¼Œå¿½ç•¥å¡«å……éƒ¨åˆ†ï¼Œä»è€Œé¿å…æ— æ•ˆçš„è®¡ç®—ï¼š

- **1**ï¼šè¡¨ç¤ºæ¨¡å‹åº”å…³æ³¨çš„è¯å…ƒï¼ˆTokensï¼‰
- **0**ï¼šè¡¨ç¤ºæ¨¡å‹åº”å¿½ç•¥çš„è¯å…ƒï¼ˆé€šå¸¸æ˜¯å¡«å…… `padding` çš„éƒ¨åˆ†ï¼‰ã€‚

åœ¨ä¹‹å‰çš„[æ–‡ç« ](../Guide/16.%20ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡%20AI%20ç»˜ç”».md#æ€ä¹ˆè®©æ¨¡å‹ç†è§£æ–‡æœ¬)ä¸­æ›¾å±•ç¤ºè¿‡æ³¨æ„åŠ›æ©ç åœ¨ `padding="max_length"` ä¸‹çš„è¡¨ç°ã€‚

**è¯å…ƒç±»å‹ ID** ç”¨äºåŒºåˆ†è¾“å…¥ä¸­çš„ä¸åŒå¥å­æˆ–æ®µè½ï¼š

- **0**ï¼šè¡¨ç¤ºç¬¬ä¸€ä¸ªå¥å­çš„è¯å…ƒã€‚
- **1**ï¼šè¡¨ç¤ºç¬¬äºŒä¸ªå¥å­çš„è¯å…ƒã€‚

è¿è¡Œä»£ç ï¼š

```python
from transformers import AutoTokenizer

# åŠ è½½ BERT çš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# ä¸¤ä¸ªå¥å­
text_a = "Hello how are you"
text_b = "I am fine thank you"

# ç¼–ç ä¸¤ä¸ªå¥å­
encoded = tokenizer(text_a, text_b, return_tensors="pt", padding=True, truncation=True)

# æ‰“å°è¯å…ƒç±»å‹ ID
print("è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰:", encoded["token_type_ids"])

# è§£ç 
decoded_text = tokenizer.decode(encoded["input_ids"][0])
print("è§£ç åçš„æ–‡æœ¬:", decoded_text)
```

**è¾“å‡º**ï¼š

```
è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰: tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])
è§£ç åçš„æ–‡æœ¬: [CLS] hello how are you [SEP] i am fine thank you [SEP]
```

## å‚è€ƒé“¾æ¥

- [Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)
- [WordPiece tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/6)
- [BERT å›¾æº](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW07.pdf)
