# b. ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨ Llama-cpp-python æœ¬åœ°è¿è¡Œé‡åŒ– LLM å¤§æ¨¡å‹ï¼ˆGGUFï¼‰

> å»ºè®®é˜…è¯»å®Œ [19a](../Guide/19a.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Transformers%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGPTQ%20%26%20AWQï¼‰%20.md) çš„ã€Œå‰è¨€ã€å’Œã€Œæ¨¡å‹ä¸‹è½½ã€éƒ¨åˆ†åå†è¿›è¡Œæœ¬æ–‡çš„é˜…è¯»ã€‚
>
> ä»£ç æ–‡ä»¶ä¸‹è½½ï¼š[Llama-cpp-python](../Demos/16b.%20ä½¿ç”¨%20Llama-cpp-python%20åŠ è½½é‡åŒ–åçš„%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.ipynb) | [ğŸ¡ AI Chat è„šæœ¬](../CodePlayground/chat.py)
>
> åœ¨çº¿é“¾æ¥ï¼š[Kaggle - b](https://www.kaggle.com/code/aidemos/16b-llama-cpp-python-llm-gguf) | [Colab - b](https://colab.research.google.com/drive/1AhgC0qDaqWBXAI9eSbwTStGgvgFfLOpf?usp=sharing)

## ç›®å½•

- [Llama-cpp-python](#llama-cpp-python)
   - [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
   - [GGUF](#gguf)
      - [å®‰è£…åº“](#å®‰è£…åº“)
      - [å¯¼å…¥åº“](#å¯¼å…¥åº“)
      - [æœ¬åœ°å¯¼å…¥æ¨¡å‹](#æœ¬åœ°å¯¼å…¥æ¨¡å‹)
      - [è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹](#è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹)
      - [æ¨ç†æµ‹è¯•](#æ¨ç†æµ‹è¯•)
      - [å¸è½½åˆ° GPU åŠ é€Ÿæ¨ç†](#å¸è½½åˆ°-gpu-åŠ é€Ÿæ¨ç†)
   - [æµå¼è¾“å‡º](#æµå¼è¾“å‡º)
   - [å¤šè½®å¯¹è¯](#å¤šè½®å¯¹è¯)
- [ğŸ“ ä½œä¸š](#-ä½œä¸š)
- [ç”¨ ğŸ¡ è„šæœ¬æ„Ÿå— AI å¯¹è¯ï¼ˆå¯é€‰ï¼‰](#ç”¨--è„šæœ¬æ„Ÿå—-ai-å¯¹è¯å¯é€‰)
   - [å…‹éš†ä»“åº“](#å…‹éš†ä»“åº“)
   - [æ‰§è¡Œè„šæœ¬](#æ‰§è¡Œè„šæœ¬)
   - [åŠ è½½å’Œä¿å­˜å†å²å¯¹è¯](#åŠ è½½å’Œä¿å­˜å†å²å¯¹è¯)
- [å‚è€ƒé“¾æ¥](#å‚è€ƒé“¾æ¥)

### ç¯å¢ƒé…ç½®

> ã€**æ›´æ–°**ã€‘å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ä¸€é”®å®‰è£…ï¼š
>
> ```bash
> CUDA_HOME="$(find /usr/local -name "cuda" -exec readlink -f {} \; \
>              | awk '{print length($0), $0}' \
>              | sort -n \
>                 | head -n1 \
>                 | cut -d ' ' -f 2)" && \
> CMAKE_ARGS="-DGGML_CUDA=on \
>             -DCUDA_PATH=${CUDA_HOME} \
>             -DCUDAToolkit_ROOT=${CUDA_HOME} \
>             -DCUDAToolkit_INCLUDE_DIR=${CUDA_HOME} \
>             -DCUDAToolkit_LIBRARY_DIR=${CUDA_HOME}/lib64 \
>                -DCMAKE_CUDA_COMPILER=${CUDA_HOME}/bin/nvcc" \
>    FORCE_CMAKE=1 \
>    pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir --verbose
>    ```
>    
> å¦‚æœå¯¹ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œå¯ä»¥ç»§ç»­é˜…è¯»ç¯å¢ƒé…ç½®éƒ¨åˆ†ã€‚

ä¸ºäº†ç¡®ä¿åç»­çš„ "offload"ï¼ˆå¸è½½åˆ° GPUï¼‰åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œéœ€è¦è¿›è¡Œä¸€äº›é¢å¤–çš„é…ç½®ã€‚

é¦–å…ˆï¼Œæ‰¾åˆ° CUDA çš„å®‰è£…è·¯å¾„ï¼ˆä½ éœ€è¦ç¡®ä¿å·²ç»å®‰è£…äº† CUDAï¼‰ï¼š

```bash
find /usr/local -name "cuda" -exec readlink -f {} \;
```

**å‚æ•°è§£é‡Š**ï¼š

- `-name "cuda"`ï¼šåœ¨ `/usr/local` ç›®å½•ä¸‹æœç´¢åä¸º "cuda" çš„æ–‡ä»¶æˆ–ç›®å½•ã€‚
- `-exec readlink -f {} \;`ï¼šå¯¹æ‰¾åˆ°çš„æ¯ä¸ªæ–‡ä»¶æˆ–ç›®å½•æ‰§è¡Œ `readlink -f`ï¼Œè·å–å…¶å®Œæ•´çš„ç»å¯¹è·¯å¾„ã€‚

å‡è®¾è¾“å‡ºå¦‚ä¸‹ï¼ˆè·¯å¾„æœ€çŸ­çš„ï¼‰ï¼š

```
/usr/local/cuda-12.1
...
```

å¤åˆ¶è¿™ä¸ªè·¯å¾„ï¼Œè®¾ç½® `CUDA_HOME` ç¯å¢ƒå˜é‡ï¼š

```bash
export CUDA_HOME=/usr/local/cuda-12.1
```

æ¥ä¸‹æ¥ï¼Œå®‰è£… `llama-cpp-python`ï¼š

```bash
CMAKE_ARGS="-DGGML_CUDA=on \
            -DCUDA_PATH=${CUDA_HOME} \
            -DCUDAToolkit_ROOT=${CUDA_HOME} \
            -DCUDAToolkit_INCLUDE_DIR=${CUDA_HOME} \
            -DCUDAToolkit_LIBRARY_DIR=${CUDA_HOME}/lib64 \
            -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc" \
FORCE_CMAKE=1 \
pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir --verbose

# å¯ä»¥ä½¿ç”¨ç®€çŸ­çš„ç‰ˆæœ¬ï¼Œä½†å¦‚æœç³»ç»Ÿè£…äº†å¤šä¸ªç‰ˆæœ¬çš„cudaï¼Œå¯èƒ½éœ€è¦é‡‡å–ä¸Šé¢çš„å®‰è£…æ­¥éª¤
#CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python
```

> [!note]
>
> å¦‚æœä»…åœ¨ CPU ä¸Šè¿è¡Œï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ `pip install llama-cpp-python` è¿›è¡Œå®‰è£…ã€‚
>
> å¦åˆ™ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿå·²å®‰è£… CUDAï¼Œå¯ä»¥é€šè¿‡ `nvcc --version` æ£€æŸ¥ã€‚

### GGUF

ä»¥ [bartowski/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF) ä¸ºä¾‹è¿›è¡Œæ¼”ç¤ºã€‚ä½ å°†åœ¨æ¨¡å‹ç•Œé¢æŸ¥çœ‹åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

![image-20241007093704658](./assets/image-20241007093704658.png)

å¯ä»¥çœ‹åˆ° 4-bit é‡åŒ–æœ‰ **IQ4_XS**ï¼Œ**Q4_K_S**ï¼Œ **IQ4_NL**ï¼Œ**Q4_K_M** å››ç§ï¼Œå‡ºäºæ€§èƒ½çš„è€ƒè™‘ï¼Œæˆ‘ä»¬å°†é€‰æ‹© **Q4_K_M** è¿›è¡ŒåŠ è½½ã€‚

| æ–‡ä»¶å                                                       | é‡åŒ–ç±»å‹ | æ–‡ä»¶å¤§å° | æè¿°                                                |
| ------------------------------------------------------------ | -------- | -------- | --------------------------------------------------- |
| [Mistral-7B-Instruct-v0.3-Q4_K_M.gguf](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/blob/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf) | Q4_K_M   | 4.37GB   | è´¨é‡è¾ƒå¥½ï¼Œæƒé‡æ¯ä½çº¦å  4.83 æ¯”ç‰¹ï¼Œ*æ¨èä½¿ç”¨*ã€‚      |
| [Mistral-7B-Instruct-v0.3-Q4_K_S.gguf](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/blob/main/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf) | Q4_K_S   | 4.14GB   | ç•¥ä½äº Q4_K_M çš„è´¨é‡ï¼Œä½†èŠ‚çœæ›´å¤šç©ºé—´ï¼Œ*æ¨èä½¿ç”¨*ã€‚  |
| [Mistral-7B-Instruct-v0.3-IQ4_NL.gguf](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/blob/main/Mistral-7B-Instruct-v0.3-IQ4_NL.gguf) | IQ4_NL   | 4.13GB   | è´¨é‡ä¸é”™ï¼Œä½“ç§¯ç•¥å°äº Q4_K_Sï¼Œæ€§èƒ½ç›¸è¿‘ï¼Œ*æ¨èä½¿ç”¨*ã€‚ |
| [Mistral-7B-Instruct-v0.3-IQ4_XS.gguf](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/blob/main/Mistral-7B-Instruct-v0.3-IQ4_XS.gguf) | IQ4_XS   | 3.91GB   | è´¨é‡ä¸é”™ï¼Œä½“ç§¯å°äº Q4_K_Sï¼Œæ€§èƒ½ç›¸è¿‘ï¼Œ*æ¨èä½¿ç”¨*ã€‚   |

> [!NOTE]
>
> **Qï¼šè¿™äº›é‡åŒ–ç±»å‹åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ**
> Aï¼šæ‹“å±•é˜…è¯»ï¼š[ã€Šd. å¦‚ä½•åŠ è½½ GGUF æ¨¡å‹ï¼ˆåˆ†ç‰‡ & Shared & Split & 00001-of-0000...çš„è§£å†³æ–¹æ³•ï¼‰ã€‹](../Guide/d.%20å¦‚ä½•åŠ è½½%20GGUF%20æ¨¡å‹ï¼ˆåˆ†ç‰‡%20%26%20Shared%20%26%20Split%20%26%2000001-of-0000...çš„è§£å†³æ–¹æ³•ï¼‰.md)ï¼Œå…¶ä¸­è¿˜ä¼šä»¥ `Qwen2.5-7B` ä¸ºä¾‹è®²è¿°åˆ†ç‰‡æ¨¡å‹çš„åŠ è½½æ–¹å¼ã€‚

#### å®‰è£…åº“

```bash
pip install gguf
```

#### å¯¼å…¥åº“

```python
from llama_cpp import Llama
```

ä¸‹é¢ä»‹ç»ä¸¤ç§å¯¼å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œå®é™…æ‰§è¡Œæ—¶åœ¨æœ¬åœ°å¯¼å…¥å’Œè‡ªåŠ¨ä¸‹è½½ä¸­é€‰æ‹©ä¸€ç§å³å¯ã€‚

#### æœ¬åœ°å¯¼å…¥æ¨¡å‹

æ ¹æ®æ¨¡å‹è·¯å¾„å¯¼å…¥æ¨¡å‹ï¼Œæ³¨æ„ï¼Œæ–‡ä»¶ä½äº `<model_name>` æ–‡ä»¶å¤¹ä¸‹ï¼Œä»¥å½“å‰ä¸‹è½½çš„æ–‡ä»¶ä¸ºä¾‹ï¼š

```python
# æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„
model_path = "./Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"

# åŠ è½½æ¨¡å‹
llm = Llama(
    model_path=model_path,
    #n_gpu_layers=-1,  # å–æ¶ˆæ³¨é‡Šä½¿ç”¨ GPU åŠ é€Ÿ
    #verbose=False,  # å–æ¶ˆæ³¨é‡Šç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
)
```

#### è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥æ¨¡å‹

å¯¹äº `llama-cpp-python`ï¼Œå…¥ä¹¡éšä¿—ä½¿ç”¨ `repo_id` å˜é‡åï¼Œä½†æœ¬è´¨æ˜¯å’Œä¹‹å‰ä¸€è‡´çš„ï¼Œ`filename` å¯ä»¥ä½¿ç”¨é€šé…ç¬¦ï¼Œæ¯”å¦‚ `"*Q4_K_M.gguf"`ã€‚

```python
# æŒ‡å®šä»“åº“çš„åç§°å’Œæ–‡ä»¶å
repo_id = "bartowski/Mistral-7B-Instruct-v0.3-GGUF"
filename = "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"
#filename = "*Q4_K_M.gguf"  # ä½¿ç”¨é€šé…ç¬¦ä¹Ÿæ˜¯å¯ä»¥çš„

# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
llm = Llama.from_pretrained(
    repo_id=repo_id,
    filename=filename,
    #n_gpu_layers=-1,  # å–æ¶ˆæ³¨é‡Šä½¿ç”¨ GPU åŠ é€Ÿ
    #verbose=False,  # å–æ¶ˆæ³¨é‡Šç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
)
```

> [!tip]
>
> äºŒè€…çš„å‡½æ•°åŒºåˆ«åœ¨äº `Llama()` å’Œ `Llama.from_pretrained()`ã€‚

#### æ¨ç†æµ‹è¯•

ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œç®€å•çš„æ¨ç†æµ‹è¯•ï¼š

```python
# è¾“å…¥æ–‡æœ¬
input_text = "Hello, World!"

# ç”Ÿæˆè¾“å‡º
output = llm(input_text, max_tokens=50)

# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
print(output['choices'][0]['text'])
```

**è¾“å‡º**ï¼š

```
Llama.generate: 4 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =      28.32 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =    1563.56 ms /   101 tokens


Welcome to the latest post on my blog. Today, we will discuss an interesting topic: â€œHow to create a website using JavaScript, HTML, and CSSâ€œ. This article is for those who are new to web development or want to learn the basics of creating a website using these technologies. Letâ€™s dive in!

## Prerequisites

Before we start, I would like to mention that I am assuming that you have some basic knowledge of HTML, CSS,
```

æ¯æ¬¡ç”Ÿæˆéƒ½ä¼šæ‰“å°ä¸€äº›æ—¶é—´æ–¹é¢çš„ä¿¡æ¯ï¼Œè®¾ç½® Llama() çš„å‚æ•° `verbose` ä¸º False å¯ä»¥ç¦æ­¢è¿™ä¸ªè¡Œä¸ºã€‚

#### å¸è½½åˆ° GPU åŠ é€Ÿæ¨ç†

å½“å‰çš„æ¨¡å‹é»˜è®¤è¢«éƒ¨ç½²åœ¨ CPU ä¸Šï¼Œå¦‚æœä½ çš„ç”µè„‘æ‹¥æœ‰æ˜¾å¡ä¸”å¤§äº 5G æ˜¾å­˜ï¼Œé‚£ä¹ˆå¯ä»¥å¢åŠ  `n_gpu_layers` å‚æ•°å°†éƒ¨åˆ†è®¡ç®—å¸è½½ï¼ˆoffloadï¼‰åˆ° GPUï¼Œä»¥åŠ é€Ÿæ¨ç†ã€‚ä¿®æ”¹åŠ è½½æ¨¡å‹çš„ä»£ç å¦‚ä¸‹ï¼š

```python
# æœ¬åœ°åŠ è½½å¹¶å¸è½½åˆ° GPU
llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1  # å°†æ‰€æœ‰å±‚å¸è½½åˆ° GPU
    verbose=False,  # ç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
)

# æˆ–è€…ï¼Œè‡ªåŠ¨ä¸‹è½½å¹¶å¸è½½åˆ° GPU
llm = Llama.from_pretrained(
    repo_id=repo_id,
    filename=filename,
    n_gpu_layers=-1  # å°†æ‰€æœ‰å±‚å¸è½½åˆ° GPU
    verbose=False,  # ç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
)
```

å¦‚æœä½ çš„æ˜¾å¡ä¸è¶³ 5Gï¼Œå¯ä»¥è®¾ç½®å¸è½½çš„å…·ä½“å±‚æ•°ï¼Œä¾‹å¦‚ `n_gpu_layers=20`ï¼Œæ ¹æ®ä½ çš„æ˜¾å­˜å¤§å°è°ƒæ•´è¯¥å‚æ•°ã€‚

P.S. ä¸å¸è½½å±‚æ˜¯å…è®¸çš„ï¼Œä½¿ç”¨ CPU ä¸€æ ·å¯ä»¥è¿›è¡Œæ¨ç†ï¼Œç®€å•å‚è€ƒä¸‹é¢çš„è¡¨æ ¼ï¼š

| è®¾å¤‡ | tokens/s | ms/token | s/100 tokens |
| ---- | -------- | -------- | ------------ |
| CPU  | 11.48    | 87.08    | 8.71         |
| GPU  | 66.85    | 14.96    | 1.50         |

æ³¨ï¼š`tokens/s` ä¸ºæ¯ç§’ç”Ÿæˆçš„ Token æ•°é‡ï¼Œ`ms/token` ä¸ºç”Ÿæˆæ¯ä¸ª Token æ‰€éœ€çš„æ¯«ç§’æ•°ï¼Œ`s/100 tokens` ä¸ºç”Ÿæˆ 100 ä¸ª Token æ‰€éœ€çš„ç§’æ•°ã€‚



### æµå¼è¾“å‡º

`Llama-cpp-python` çš„æµå¼è¾“å‡ºåªéœ€è¦åœ¨ create_chat_completion() ä¸­ä¼ é€’å‚æ•° `stream=True` å°±å¯ä»¥å¼€å¯ï¼Œä»¥æœ¬åœ°æ¨¡å‹å¯¼å…¥ä¸ºä¾‹ï¼š

```python
prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ"

output = llm.create_chat_completion(
    messages=[{
        "role": "user",
        "content": prompt
    }],
    max_tokens=200,
    stream=True
)

for chunk in output:
    delta = chunk['choices'][0]['delta']
    if 'role' in delta:
        print(delta['role'], end=': ', flush=True)
    elif 'content' in delta:
        print(delta['content'], end='', flush=True)
```

**è¾“å‡º**ï¼š

![æµå¼è¾“å‡º llama-cpp-python](./assets/%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA%20llama-cpp-python.gif)

**ä»£ç è§£é‡Š**ï¼š

- **`for chunk in output:`**ï¼šéå†æ¨¡å‹ç”Ÿæˆçš„æ¯ä¸€ä¸ªæ•°æ®å—ï¼ˆchunkï¼‰ã€‚
  - **`delta = chunk['choices'][0]['delta']`**ï¼š
    - æ¯ä¸ª`chunk`åŒ…å«ä¸€ä¸ª`choices`åˆ—è¡¨ï¼Œè¿™é‡Œåªå–ç¬¬ä¸€ä¸ªé€‰æ‹©ï¼ˆ`choices[0]`ï¼‰ã€‚
    - `delta`åŒ…å«äº†å½“å‰æ•°æ®å—ä¸­çš„å¢é‡ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯è§’è‰²ï¼ˆroleï¼‰ä¿¡æ¯æˆ–å†…å®¹ï¼ˆcontentï¼‰ä¿¡æ¯ã€‚
  
  - **`if 'role' in delta:`**ï¼š
    - å¦‚æœ`delta`ä¸­åŒ…å«`'role'`é”®ï¼Œè¯´æ˜è¿™æ˜¯è§’è‰²ä¿¡æ¯ï¼ˆä¾‹å¦‚ â€œassistantâ€ï¼‰ã€‚
      - **`print(delta['role'], end=': ')`**ï¼šæ‰“å°è§’è‰²åï¼Œå¹¶ä»¥å†’å·å’Œç©ºæ ¼ç»“å°¾ï¼Œä¾‹å¦‚â€œassistant: â€ï¼Œè¿™æ˜¯è‡ªå®šä¹‰è¡Œä¸ºï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ `pass` æ‰ã€‚
  
  - **`elif 'content' in delta:`**ï¼š
    - å¦‚æœ`delta`ä¸­åŒ…å«`'content'`é”®ï¼Œè¯´æ˜è¿™æ˜¯å®é™…çš„å›ç­”å†…å®¹ã€‚
      - **`print(delta['content'], end='')`**ï¼šæ‰“å°å†…å®¹ï¼Œä¸æ¢è¡Œï¼Œä»¥ä¾¿é€æ­¥æ˜¾ç¤ºç”Ÿæˆçš„å›ç­”ï¼Œæ³¨æ„ï¼Œåœ¨è¿™é‡Œå‚æ•° `end='' `æ˜¯æ­£ç¡®æ‰“å°æ‰€å¿…é¡»çš„ã€‚

> [!note]
>
> **æŸ¥çœ‹ output çš„æ„é€ **ï¼š
>
> ```python
> from itertools import islice
> 
> prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ"
> 
> output = llm.create_chat_completion(
>        messages=[{
>            "role": "user",
>            "content": prompt
>        }],
>        max_tokens=200,
>        stream=True
> )
> 
> print(type(output))
> 
> # å°†ç”Ÿæˆå™¨è½¬æ¢ä¸ºåˆ—è¡¨
> output_list = list(islice(output, 3))
> 
> # è·å–å‰ 3 ä¸ªæ¡ç›®
> output_list[:3]
> ```
>
> **è¾“å‡º**ï¼ˆåªéœ€è¦æŸ¥çœ‹å…¶ä¸­çš„ `delta`ï¼‰ï¼š
>
> ```
> <class 'generator'>
> [{'id': 'chatcmpl-848b2e9b-7d70-4a7b-99aa-74b8206721db',
> 'model': './Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf',
> 'created': 1728562647,
> 'object': 'chat.completion.chunk',
> 'choices': [{'index': 0,
>  'delta': {'role': 'assistant'},
>  'logprobs': None,
>  'finish_reason': None}]},
> {'id': 'chatcmpl-848b2e9b-7d70-4a7b-99aa-74b8206721db',
> 'model': './Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf',
> 'created': 1728562647,
> 'object': 'chat.completion.chunk',
> 'choices': [{'index': 0,
>  'delta': {'content': ' '},
>  'logprobs': None,
>  'finish_reason': None}]},
> {'id': 'chatcmpl-848b2e9b-7d70-4a7b-99aa-74b8206721db',
> 'model': './Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf',
> 'created': 1728562647,
> 'object': 'chat.completion.chunk',
> 'choices': [{'index': 0,
>  'delta': {'content': 'äºº'},
>  'logprobs': None,
>  'finish_reason': None}]}]
> ```

å°†åˆšåˆšå¯¹äºæµå¼è¾“å‡ºçš„å¤„ç†æŠ½è±¡ä¸ºå‡½æ•°ä¾¿äºåç»­è°ƒç”¨ï¼š
```python
def handle_stream_output(output):
    """
    å¤„ç†æµå¼è¾“å‡ºï¼Œå°†ç”Ÿæˆçš„å†…å®¹é€æ­¥æ‰“å°å‡ºæ¥ã€‚
    
    å‚æ•°ï¼š
        output: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œæ¥è‡ª create_chat_completion çš„æµå¼è¾“å‡º
    """
    for chunk in output:
        delta = chunk['choices'][0]['delta']
        if 'role' in delta:
            print(f"{delta['role']}: ", end='', flush=True)
        elif 'content' in delta:
            print(delta['content'], end='', flush=True)
            
# ä½¿ç”¨ç¤ºä¾‹
prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ"

output = llm.create_chat_completion(
    messages=[{
        "role": "user",
        "content": prompt
    }],
    max_tokens=200,
    stream=True
)

handle_stream_output(output)
```

**å‡½æ•°è§£é‡Š**ï¼š

- **`handle_stream_output`**ï¼š
  - æ¥æ”¶ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ `output`ï¼Œéå†æ¯ä¸ªæ•°æ®å— `chunk`ã€‚
  - ä»æ¯ä¸ª `chunk` ä¸­æå– `delta` ä¿¡æ¯ã€‚
  - æ ¹æ® `delta` ä¸­çš„é”®å€¼ï¼Œåˆ†åˆ«å¤„ç† `role` å’Œ `content` ä¿¡æ¯ã€‚
  - ä½¿ç”¨ `flush=True` ç¡®ä¿å†…å®¹å®æ—¶æ‰“å°ã€‚

### å¤šè½®å¯¹è¯

è®©æˆ‘ä»¬è‡ªå®šä¹‰ä¸€ä¸ªäº¤äº’çš„å¯¹è¯ç±»ï¼ˆéœ€è¦æ³¨æ„åˆ° handle_stream_output() æœ‰æ‰€ä¿®æ”¹ï¼‰ã€‚

```python
from llama_cpp import Llama

def handle_stream_output(output):
    """
    å¤„ç†æµå¼è¾“å‡ºï¼Œå°†ç”Ÿæˆçš„å†…å®¹é€æ­¥æ‰“å°å‡ºæ¥ï¼Œå¹¶æ”¶é›†å®Œæ•´çš„å›å¤ã€‚

    å‚æ•°ï¼š
        output: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œæ¥è‡ª create_chat_completion çš„æµå¼è¾“å‡º

    è¿”å›ï¼š
        response: å®Œæ•´çš„å›å¤æ–‡æœ¬
    """
    response = ""
    for chunk in output:
        delta = chunk['choices'][0]['delta']
        if 'role' in delta:
            print(f"{delta['role']}: ", end='', flush=True)
        elif 'content' in delta:
            content = delta['content']
            print(content, end='', flush=True)
            response += content
    return response

class ChatSession:
    def __init__(self, llm):
        self.llm = llm
        self.messages = []

    def add_message(self, role, content):
        """
        æ·»åŠ ä¸€æ¡æ¶ˆæ¯åˆ°ä¼šè¯ä¸­ã€‚

        å‚æ•°ï¼š
            role: æ¶ˆæ¯è§’è‰²ï¼Œé€šå¸¸ä¸º 'user' æˆ– 'assistant'
            content: æ¶ˆæ¯å†…å®¹
        """
        self.messages.append({"role": role, "content": content})

    def get_response_stream(self, user_input):
        """
        è·å–æ¨¡å‹å¯¹ç”¨æˆ·è¾“å…¥çš„å“åº”ï¼ˆæµå¼è¾“å‡ºï¼‰ã€‚

        å‚æ•°ï¼š
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬

        è¿”å›ï¼š
            response: å®Œæ•´çš„å›å¤æ–‡æœ¬
        """
        self.add_message("user", user_input)
        
        try:
            output = self.llm.create_chat_completion(
                messages=self.messages,
                stream=True  # å¼€å¯æµå¼è¾“å‡º
            )
            
            response = handle_stream_output(output)  # åŒæ—¶æ‰“å°å’Œæ”¶é›†å›å¤
            
            self.add_message("assistant", response.strip())
            return response.strip()
        except Exception as e:
            print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
            return ""

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆå‡è®¾ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
model_path = "./Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"
llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,  # æ ¹æ®éœ€è¦å¸è½½åˆ° GPU
    verbose=False,    # ç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
)

# åˆ›å»ºä¼šè¯å®ä¾‹
chat = ChatSession(llm)
        
# å¼€å§‹å¯¹è¯
while True:
    prompt = input("User: ")
    # é€€å‡ºå¯¹è¯æ¡ä»¶ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ç»ˆæ­¢ä»£ç å—ï¼‰
    if prompt.lower() in ["exit", "quit", "bye"]:
        print("Goodbye!")
        break
    chat.get_response_stream(prompt)
    print()  # æ¢è¡Œä»¥ä¾¿ä¸‹ä¸€æ¬¡è¾“å…¥ï¼Œè¿™æ˜¯å› ä¸ºä¹‹å‰çš„ print éƒ½è®¾ç½®äº† end=''
```

**è¾“å‡º**ï¼š

```
User:  å¦‚æœä½ æ˜¯å¤§æ¨¡å‹é¢è¯•å®˜ï¼Œä½ ä¼šæ€ä¹ˆå‡ºé¢è¯•é¢˜
assistant:  ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„å¤§æ¨¡å‹é¢è¯•é¢˜ï¼š

1. è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥åŠå®ƒä»¬çš„åº”ç”¨åœºæ™¯ã€‚
2. æè¿°ä½ å¯¹æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹çš„äº†è§£ï¼Œå¹¶æä¾›ä¸€ä¸ªå®é™…ä½¿ç”¨ä¾‹å­ã€‚
3. å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥åŠå¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Ÿ
4. è§£é‡Šä½ å¯¹TensorFlowå’ŒPyTorchçš„äº†è§£ï¼Œå¹¶æä¾›ä¸€ä¸ªä½¿ç”¨å®ƒä»¬çš„å®é™…ä¾‹å­ã€‚
5. å¦‚ä½•å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ï¼Œä»¥åŠä½ å¯¹æ ·æœ¬å¹³è¡¡å’Œæ•°æ®å¢å¼ºæ–¹æ³•çš„äº†è§£ã€‚
6. å¦‚ä½•ä½¿ç”¨Transfer Learningæ¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æä¾›ä¸€ä¸ªå®é™…ä¾‹å­ã€‚
7. å¦‚ä½•ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Seq2Seqæ¨¡å‹ï¼‰æ¥è¿›è¡Œæœºå™¨ç¿»è¯‘ï¼Œæ–‡æœ¬æ‘˜è¦å’Œæƒ…æ„Ÿåˆ†æï¼Ÿ
8. å¦‚ä½•ä½¿ç”¨å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼ˆå¦‚Faster R-CNNå’ŒYOLOï¼‰æ¥è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Ÿ
9. å¦‚ä½•ä½¿ç”¨è‡ªç¼–ç å™¨æ¥è¿›è¡Œæ•°æ®å‹ç¼©å’Œç‰¹å¾å­¦ä¹ ï¼Ÿ
10. å¦‚ä½•ä½¿ç”¨å–‚ç»™ç½‘ç»œï¼ˆFeeding Networksï¼‰å’ŒGenerative Adversarial Networksï¼ˆGANsï¼‰æ¥ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬ï¼Ÿ
11. å¦‚ä½•ä½¿ç”¨æ—¶åºæ•°æ®æ¨¡å‹ï¼ˆå¦‚ARIMAå’ŒLSTMï¼‰æ¥è¿›è¡Œé¢„æµ‹ï¼Ÿ
12. å¦‚ä½•ä½¿ç”¨å›å½’æ ‘å’Œéšæœºæ£®æ—æ¥è¿›è¡Œé¢„æµ‹å’Œåˆ†ç±»ï¼Ÿ
13. æè¿°ä½ å¯¹è¶…å‚æ•°è°ƒä¼˜çš„äº†è§£ï¼ŒåŒ…æ‹¬ç½‘ç»œæ¶æ„ã€å­¦ä¹ ç‡å’Œæ‰¹å¤§å°ç­‰æ–¹é¢ã€‚
14. å¦‚ä½•ä½¿ç”¨K-meanså’Œæœ´ç´ è´å¶æ–¯ç­‰èšç±»å’Œåˆ†ç±»æ–¹æ³•ï¼Ÿ
15. æè¿°ä½ å¯¹å‡¸ä¼˜åŒ–å’Œ
User:  å¯¹äºç¬¬åä¸ªé—®é¢˜èƒ½å¦ç»™æˆ‘ç­”æ¡ˆ

å‘ç”Ÿé”™è¯¯: Requested tokens (530) exceed context window of 512
```

å¯ä»¥çœ‹åˆ°æŠ¥é”™è¶…è¿‡äº†ä¸Šä¸‹æ–‡çª—å£çš„é•¿åº¦ï¼Œè®©æˆ‘ä»¬å¢åŠ å®ƒï¼š

```python
llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,  # æ ¹æ®éœ€è¦å¸è½½åˆ° GPU
    n_ctx=4096,       # è®¾ç½®ä¸Šä¸‹æ–‡çª—å£å¤§å°
    verbose=False,    # ç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
)
```

æ­¤æ—¶æ¨¡å‹è¾“å‡ºæ­£å¸¸ï¼š

```
User:  å¦‚æœä½ æ˜¯å¤§æ¨¡å‹é¢è¯•å®˜ï¼Œä½ ä¼šæ€ä¹ˆå‡ºé¢è¯•é¢˜
assistant:  ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„å¤§æ¨¡å‹é¢è¯•é¢˜ï¼š

1. è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥åŠå®ƒä»¬çš„åº”ç”¨åœºæ™¯ã€‚
2. æè¿°ä½ å¯¹æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹çš„äº†è§£ï¼Œå¹¶æä¾›ä¸€ä¸ªå®é™…ä½¿ç”¨ä¾‹å­ã€‚
3. å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥åŠå¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Ÿ
4. è§£é‡Šä½ å¯¹TensorFlowå’ŒPyTorchçš„äº†è§£ï¼Œå¹¶æä¾›ä¸€ä¸ªä½¿ç”¨å®ƒä»¬çš„å®é™…ä¾‹å­ã€‚
5. å¦‚ä½•å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ï¼Œä»¥åŠä½ å¯¹æ ·æœ¬å¹³è¡¡å’Œæ•°æ®å¢å¼ºæ–¹æ³•çš„äº†è§£ã€‚
6. å¦‚ä½•ä½¿ç”¨Transfer Learningæ¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æä¾›ä¸€ä¸ªå®é™…ä¾‹å­ã€‚
7. å¦‚ä½•ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Seq2Seqæ¨¡å‹ï¼‰æ¥è¿›è¡Œæœºå™¨ç¿»è¯‘ï¼Œæ–‡æœ¬æ‘˜è¦å’Œæƒ…æ„Ÿåˆ†æï¼Ÿ
8. å¦‚ä½•ä½¿ç”¨å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼ˆå¦‚Faster R-CNNå’ŒYOLOï¼‰æ¥è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Ÿ
9. å¦‚ä½•ä½¿ç”¨è‡ªç¼–ç å™¨æ¥è¿›è¡Œæ•°æ®å‹ç¼©å’Œç‰¹å¾å­¦ä¹ ï¼Ÿ
10. å¦‚ä½•ä½¿ç”¨å–‚ç»™ç½‘ç»œï¼ˆFeeding Networksï¼‰å’ŒGenerative Adversarial Networksï¼ˆGANsï¼‰æ¥ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬ï¼Ÿ
11. å¦‚ä½•ä½¿ç”¨æ—¶åºæ•°æ®æ¨¡å‹ï¼ˆå¦‚ARIMAå’ŒLSTMï¼‰æ¥è¿›è¡Œé¢„æµ‹ï¼Ÿ
12. å¦‚ä½•ä½¿ç”¨å›å½’æ ‘å’Œéšæœºæ£®æ—æ¥è¿›è¡Œé¢„æµ‹å’Œåˆ†ç±»ï¼Ÿ
13. æè¿°ä½ å¯¹è¶…å‚æ•°è°ƒä¼˜çš„äº†è§£ï¼ŒåŒ…æ‹¬ç½‘ç»œæ¶æ„ã€å­¦ä¹ ç‡å’Œæ‰¹å¤§å°ç­‰æ–¹é¢ã€‚
14. å¦‚ä½•ä½¿ç”¨K-meanså’Œæœ´ç´ è´å¶æ–¯ç­‰èšç±»å’Œåˆ†ç±»æ–¹æ³•ï¼Ÿ
15. æè¿°ä½ å¯¹å‡¸ä¼˜åŒ–å’Œéšæœº Forestsç­‰ç®—æ³•çš„äº†è§£ã€‚
User:  å¯¹äºç¬¬åä¸ªé—®é¢˜èƒ½å¦ç»™æˆ‘ç­”æ¡ˆ
assistant:  ç»™å®šä¸€ä¸ªç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬çš„é—®é¢˜ï¼Œä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨Generative Adversarial Networksï¼ˆGANsï¼‰ã€‚

GANsæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”±ä¸¤ä¸ªå­ç½‘ç»œç»„æˆï¼šç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ã€‚ç”Ÿæˆå™¨ç”Ÿæˆä¸€ç»„éšæœºå™ªå£°å¹¶æ ¹æ®è¯¥å™ªå£°ç”Ÿæˆæ–°çš„æ•°æ®ï¼Œè€Œåˆ¤åˆ«å™¨è¯•å›¾åŒºåˆ†ç”Ÿæˆçš„æ•°æ®å’ŒçœŸå®æ•°æ®ã€‚è¿™ä¸¤ä¸ªå­ç½‘ç»œé€šè¿‡æœ€å°åŒ–ä¸€ä¸ªå¯¹æŠ—æ€§æŸå¤±å‡½æ•°æ¥äº’ç›¸å­¦ä¹ ã€‚

åœ¨ç”Ÿæˆå›¾åƒæ–¹é¢ï¼Œå¸¸ç”¨çš„GANæ¨¡å‹åŒ…æ‹¬DCGANï¼ˆDeep Convolutional GANï¼‰ã€CGANï¼ˆConditional GANï¼‰å’ŒWGANï¼ˆWasserstein GANï¼‰ç­‰ã€‚DCGANä½¿ç”¨ convolutional neural network ä½œä¸ºç”Ÿæˆå™¨ï¼Œå¯¹äºç”Ÿæˆå›¾åƒæ¥è¯´ï¼ŒDCGANå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†æ˜¯å®ƒå¯èƒ½ä¼šç”Ÿæˆä¸€äº›ä¸å¤ªå¯é çš„å›¾åƒï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ç§æ— æ¡ä»¶ç”Ÿæˆå™¨ã€‚

CGANæ˜¯DCGANçš„ä¸€ç§æ‰©å±•ï¼Œå®ƒå¼•å…¥äº†æ¡ä»¶ä¿¡æ¯ï¼Œå…è®¸ç”Ÿæˆå™¨æ ¹æ®ç‰¹å®šæ¡ä»¶ï¼ˆå¦‚ç±»åˆ«æ ‡ç­¾ï¼‰ç”Ÿæˆå›¾åƒã€‚WGANæ˜¯DCGANçš„ä¸€ç§æ”¹è¿›ç‰ˆæœ¬ï¼Œå®ƒä½¿ç”¨ Wasserstein è·ç¦»æ¥æ›¿æ¢äº†åŸæ¥çš„å¯¹æŠ—æ€§æŸå¤±å‡½æ•°ï¼Œä»è€Œä½¿å¾—æ¨¡å‹æ›´åŠ ç¨³å®šã€‚

åœ¨ç”Ÿæˆæ–‡æœ¬æ–¹é¢ï¼Œå¸¸ç”¨çš„GANæ¨¡å‹åŒ…æ‹¬SeqGANï¼ˆSequence Generative Adversarial Netsï¼‰å’ŒStackGANï¼ˆStack Generative Adversarial Networksï¼‰ã€‚SeqGANé€šè¿‡ä½¿ç”¨RNNsï¼ˆRecurrent Neural Networksï¼‰ç”Ÿæˆä¸€ç³»åˆ—å•è¯æ¥ç”Ÿæˆæ–‡æœ¬ã€‚StackGANä½¿ç”¨å¤šä¸ªå †å çš„GANå­ç½‘ç»œæ¥ç”Ÿæˆå¤æ‚çš„æ–‡æœ¬ã€‚

æ€»ä¹‹ï¼ŒGANsæ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒå’Œæ–‡æœ¬ï¼Œä½†æ˜¯å®ƒä»¬ä¹Ÿæœ‰ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚ç”Ÿæˆçš„æ•°æ®å¯èƒ½å­˜åœ¨æ¨¡å¼ç¼ºé™·ï¼Œå¹¶ä¸”è®­ç»ƒè¿‡ç¨‹å¯èƒ½ä¼šæ”¶æ•›å¾ˆæ…¢ã€‚
```

è‡³æ­¤ï¼Œç¯‡ç« å‘Šä¸€æ®µè½ :)ã€‚

## ğŸ“ ä½œä¸š

ä¿®æ”¹ `model_path`ï¼ŒåŠ è½½ [bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q5_K_L.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-7B-Q5_K_L.gguf) è¿›è¡Œå¯¹è¯ï¼š

```python
model_path = "bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q5_K_L.gguf"
```

è§‚å¯Ÿ <think> </think> éƒ¨åˆ†çš„è¾“å‡ºã€‚

## ç”¨ ğŸ¡ è„šæœ¬æ„Ÿå— AI å¯¹è¯ï¼ˆå¯é€‰ï¼‰

è¿™æ˜¯å¯é€‰çš„è¡Œä¸ºï¼Œè„šæœ¬çš„ä»£ç å¤„ç†é€»è¾‘ä¸æ–‡ç« å¯¹åº”ã€‚

### å…‹éš†ä»“åº“

```bash
# å¦‚æœå·²ç»å…‹éš†ä»“åº“çš„è¯è·³è¿‡è¿™è¡Œ
git clone https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN
```

### æ‰§è¡Œè„šæœ¬

1. åˆ‡æ¢åˆ° `CodePlayground` æ–‡ä»¶å¤¹ï¼š

   ```bash
   cd AI-Guide-and-Demos-zh_CN/CodePlayground
   ```

2. å¼€å§‹å¯¹è¯ï¼š

   ```bash
   # ä½¿ç”¨æ–¹æ³•ï¼špython chat.py <model_path> [å¯é€‰å‚æ•°]ï¼Œæ›¿æ¢ä¸ºä½ æƒ³ç”¨çš„ <model_path>
   # æœ¬åœ°åŠ è½½
   python chat.py ../Demos/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf
   
   # è¿œç¨‹åŠ è½½
   python chat.py bartowski/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf --remote
   
   # è¿œç¨‹åŠ è½½ï¼ˆé€šé…ç¬¦ç‰ˆï¼‰
   python chat.py 'bartowski/Mistral-7B-Instruct-v0.3-GGUF/*Q4_K_M.gguf' --remote
   
   # æˆ–è€…è¯•è¯• DeepSeek
   python chat.py 'bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/*Q5_K_L.gguf' --remote
   ```

### åŠ è½½å’Œä¿å­˜å†å²å¯¹è¯

ä½¿ç”¨ `-i` å‚æ•°æŒ‡å®šæ–‡ä»¶åŠ è½½ï¼Œ`-o` å‚æ•°è¿›è¡Œä¿å­˜ï¼Œæˆ–è€… `-io` å‚æ•°æŒ‡æ˜åŠ è½½å’Œä¿å­˜çš„æ–‡ä»¶è·¯å¾„ç›¸åŒï¼š

```bash
python chat.py <model_path> -io history
```

æ³¨æ„ï¼Œ`Ctrl + C` å°†ç›´æ¥ç»ˆæ­¢å¯¹è¯ï¼Œåªæœ‰ä½¿ç”¨ 'exit'ã€'quit' æˆ– 'bye' ç»“æŸï¼Œæˆ–è€…ä½¿ç”¨ `Ctrl + D` (EOF) é€€å‡ºæ—¶æ‰ä¼šä¿å­˜å¯¹è¯ã€‚

> [!note]
>
> æš‚æ—¶ä»…æ”¯æŒä¸æ‹¥æœ‰ `tokenizer.chat_template` å±æ€§çš„æ¨¡å‹æ­£å¸¸å¯¹è¯ã€‚

## å‚è€ƒé“¾æ¥

- [llama-cpp-python - Docs](https://llama-cpp-python.readthedocs.io/en/latest/)
- [Example with `stream = True`? #319](https://github.com/abetlen/llama-cpp-python/discussions/319#discussioncomment-8422590)

