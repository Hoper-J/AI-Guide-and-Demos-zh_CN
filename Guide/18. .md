> æœªå®Œå¾…ç»­ï¼Œæ”¥å†™âœï¸ä¸­.. ï¼ˆå½“å‰commitä¼šæ ¹æ®å‚è€ƒæ–‡çŒ®ä¿®æ­£è¡¨è¿°ç›´åˆ°ä¸‹ä¸€æ¬¡çš„æäº¤ï¼‰
>
> æ¥ç”¨ç‚¹ä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚
>
> å‰æ–‡ã€Š17. æµ…è°ˆæ¨¡å‹é‡åŒ–ï¼šéå¯¹ç§° vs å¯¹ç§°ã€‹ä¸­æˆ‘ä»¬æœ‰æåˆ°ç”¨ PyTorch å»åšä¸€äº›æ¨¡å‹é‡åŒ–ï¼Œä½†å®é™…åº”ç”¨æ—¶ï¼Œä¸éœ€è¦è¿™ä¹ˆéº»çƒ¦ï¼Œä½ çŸ¥é“çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ transformer åº“ä¸­çš„è½®å­æ¥å®Œæˆè¿™äº›æ“ä½œã€‚
>
> ä½ å¯èƒ½ä¼šäº§ç”Ÿä¸€ä¸ªç–‘æƒ‘ï¼šåœ¨ä¹‹å‰ä¸æ˜¯æœ‰ä¸€ç¯‡å”è¯—å¾®è°ƒ LLM çš„æ–‡ç« å—ï¼Ÿä¸ºä»€ä¹ˆè¿˜è¦å†å†™ä¸€ç¯‡ã€‚
>
> å› ä¸ºä¹‹å‰çš„ LLM å¾®è°ƒä»£ç æœ¬è´¨ç›®çš„æ˜¯å¸¦ä½ å»â€œç”¨â€ï¼Œè€Œéâ€œå†™â€ï¼Œè€Œè¿™ç¯‡æ–‡ç« ä¼šä»å¤´å¼€å§‹è¿›è¡Œæœ¬åœ°çš„éƒ¨ç½²ï¼Œä½ å°†çœŸæ­£äº†è§£åˆ°å…¶ä¸­çš„æ–¹æ–¹é¢é¢ã€‚
>

è®¿é—® [Hugging Face](https://huggingface.co/models)ï¼Œè®©æˆ‘ä»¬å…ˆé€‰æ‹©ä¸€ä¸ª7Bå·¦å³çš„è¯­è¨€æ¨¡å‹ï¼š

1. [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)
2. [Qwen/Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
3. [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)

> [!NOTE]
>
> ä½ å¯ä»¥éšæ„æ›´æ¢ä½ å–œæ¬¢çš„æ¨¡å‹ï¼Œä¸Šé¢åªæ˜¯ç®€å•åˆ—ä¸¾å‡ºã€‚

åœä¸€ä¸‹ï¼Œè¿˜è®°å¾— FP32 ä¸‹ 7B æ¨¡å‹çš„å‚æ•°æœ‰å¤šå¤§å—ï¼Ÿ

â€œä¸ä¸¥è°¨çš„è¯´ï¼Œå¥½åƒæ˜¯ 28 GBï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ç”¨æ¨¡å‹é‡åŒ–æ¥å¯¼å…¥æ¨¡å‹ï¼Œå°±æ˜¯å¤ªå¤§äº†ï¼Œå¯èƒ½è¦ä¸‹è½½æ¯”è¾ƒä¹…çš„æ—¶é—´:(â€

æ˜¯çš„ï¼Œè¿™ä¸ªè¯´æ³•æ²¡æœ‰é—®é¢˜ï¼Œä¸è¿‡ä¸Šé¢åˆ—å‡ºçš„æ¨¡å‹é‡‡ç”¨çš„æ˜¯ BF16ï¼Œæ‰€ä»¥è¿˜ä¼šæ›´å°ç‚¹ã€‚

â€œé‚£å¤§æ¦‚ 14 GBï¼Œæˆ‘æ™šç‚¹å†å¼€å§‹æ­£å¼å­¦ä¹ ï¼Œè¿˜æ˜¯è¦ä¸‹è½½å¾ˆä¹…â€

è¯¶ï¼Œé‚£ä½ æœ‰æ²¡æœ‰æƒ³è¿‡ï¼Œæ—¢ç„¶è¿™äº›æ¨¡å‹ä¸‹è½½ä¸‹æ¥éœ€è¦é‡åŒ–ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥å»ä¸‹ä¸€ä¸ªé‡åŒ–ç‰ˆçš„æ¨¡å‹ï¼Ÿ

æ˜¯çš„ï¼Œä»¥ä¸Šæ‰€åˆ—çš„ä¸‰ä¸ªæ¨¡å‹ï¼Œåœ¨ Hugging Face ä¸­éƒ½æœ‰ç€é‡åŒ–ç‰ˆæœ¬ï¼š

1. [bartowski/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF)
2. [Qwen/Qwen2.5-7B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF)
3. [bartowski/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF)

## GPTQ vs AWQ vs GGUFï¼ˆGGMLï¼‰ é€Ÿè§ˆ

> å‚è€ƒé“¾æ¥ï¼š[GPTQ - 2210.17323](https://arxiv.org/pdf/2210.17323) | [AWQ - 2306.00978](https://arxiv.org/pdf/2306.00978) | [GGML](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#historical-state-of-affairs) | [GGUF - docs](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) | [What is GGUF and GGML?](https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c)

- **GPTQ** (Generalized **Post-Training** Quantization)
  
  GPTQ æ˜¯ä¸€ç§åŸºäºè¿‘ä¼¼äºŒé˜¶ä¿¡æ¯çš„**åè®­ç»ƒ**é‡åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿå°†æ¨¡å‹çš„æƒé‡ä½å®½é™ä½åˆ° 3-4 bitsï¼Œåœ¨å¤§å¹…å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶è¿˜èƒ½ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚åœ¨æç«¯æƒ…å†µä¸‹è¿˜èƒ½é‡åŒ–åˆ° 2 bits ç”šè‡³ 3 è¿›åˆ¶ï¼Œä½†ä¼šæœ‰ä¸€å®šçš„æ€§èƒ½æŸå¤±ã€‚
  
- **AWQ** (**Activation-aware** Weight Quantization)

  ![image-20241004182540373](./assets/20241004200458.png)

  AWQ ä¸ä¼šé‡åŒ–æ¨¡å‹çš„æ‰€æœ‰æƒé‡ï¼Œä¿ç•™äº†å¯¹æ¨¡å‹æ€§èƒ½é‡è¦çš„ä¸€å°éƒ¨åˆ†æƒé‡ï¼Œå¤§å¤§å‡å°‘äº†é‡åŒ–æŸå¤±ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œè¿™é‡Œæ¯”è¾ƒæç«¯ï¼Œæ˜¯ INT3 é‡åŒ–ï¼š

  - å›¾ aï¼š**RTNé‡åŒ–ï¼ˆRound-to-Nearestï¼‰**
    
    å°†æƒé‡ç›´æ¥å››èˆäº”å…¥åˆ°ç›®æ ‡ä½å®½ï¼Œå¯¼è‡´æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼ŒPPL è¾¾åˆ° 43.2ã€‚
    
  - å›¾ bï¼š**ä¿æŠ¤ 1% çš„æ˜¾è‘—æƒé‡ï¼Œä½¿ç”¨æ··åˆç²¾åº¦å½¢å¼**
    
    è¿™é‡Œå±•ç¤ºäº†ä¸€ç§æ”¹è¿›ç­–ç•¥ï¼Œå³ä¿ç•™ 1% æœ€é‡è¦çš„æƒé‡é€šé“ä½¿ç”¨é«˜ç²¾åº¦ï¼ˆFP16ï¼‰ï¼Œå…¶ä½™ä½¿ç”¨ä½ç²¾åº¦ï¼ˆINT3ï¼‰ã€‚PPL é™ä½åˆ° 13.0ã€‚è™½ç„¶è¿™ç§æ–¹æ³•èƒ½ä¿ä½æ€§èƒ½ï¼Œä½†ç”±äºéœ€è¦ä¸åŒç²¾åº¦åˆ‡æ¢ï¼Œç¡¬ä»¶æ•ˆç‡ä¸é«˜ã€‚ä½†è¿™ä¸€ç­–ç•¥è¯æ˜äº†å¹¶éæ‰€æœ‰æƒé‡éƒ½å¯¹æ¨¡å‹æ€§èƒ½åŒç­‰é‡è¦ã€‚
    
  - å›¾ cï¼š**AWQ æå‡ºçš„é€šé“ç¼©æ”¾é‡åŒ–æ–¹æ³•**

    AWQ é€šè¿‡**é€šé“ç¼©æ”¾**ä¿æŠ¤æ˜¾è‘—æƒé‡ï¼Œåˆ©ç”¨æ¿€æ´»åˆ†å¸ƒæ‰¾åˆ°é‡è¦çš„æƒé‡å¹¶ç¼©æ”¾å®ƒä»¬çš„å€¼æ¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚ç›¸æ¯”æ··åˆç²¾åº¦å½¢å¼ï¼ŒAWQ æå‡äº†ç¡¬ä»¶æ•ˆç‡ï¼ŒåŒæ—¶æ€§èƒ½ä¸å›¾ b ä¸€è‡´ï¼ŒPPL ä¹Ÿä¸ºåˆ° 13.0ã€‚

- **GGML** (GPT-Generated Model Language)
  
  ã€Œæ˜¾å­˜ä¸å¤Ÿå†…å­˜æ¥å‡‘ã€ï¼Œè¿™æ˜¯ä¸€ç§æ–‡ä»¶æ ¼å¼ï¼Œæ”¯æŒåœ¨ CPU å’Œ GPU ä¸Šè¿›è¡Œæ¨ç†ã€‚
  
- **GGUF** (GPT-Generated Unified Format)ï¼š
  
  GGUF æ˜¯ GGML çš„å‡çº§ç‰ˆï¼Œæå‡äº†æ‰©å±•å’Œå…¼å®¹æ€§ã€‚

> [!NOTE]
>
> #### ä»€ä¹ˆæ˜¯ PPLï¼Ÿ
>
> > ä»¥ä¸‹å™è¿°å‚è€ƒè‡ª [Hugging Face - docs](https://huggingface.co/docs/transformers/perplexity#perplexity-of-fixed-length-models)ã€‚
>
> PPL æ˜¯ **Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰** çš„ç¼©å†™ï¼Œå®ƒæ˜¯è¡¡é‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å¸¸ç”¨æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°**ç»å…¸è¯­è¨€æ¨¡å‹**ï¼ˆ**è‡ªå›å½’**æˆ–**å› æœè¯­è¨€æ¨¡å‹**ï¼‰çš„ç”Ÿæˆä»»åŠ¡æ—¶ã€‚
>
> **Perplexity** ä¸»è¦ç”¨äºè¡¡é‡ä¸€ä¸ªæ¨¡å‹å¯¹ç»™å®šæ–‡æœ¬çš„é¢„æµ‹èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒPPL åæ˜ äº†æ¨¡å‹åœ¨å¤„ç†ä¸€æ®µæ–‡æœ¬æ—¶çš„â€œä¸ç¡®å®šæ€§â€æˆ–â€œå›°æƒ‘åº¦â€ï¼Œæ•°å€¼è¶Šä½ï¼Œè¡¨ç¤ºæ¨¡å‹è¶Šæ“…é•¿é¢„æµ‹ä¸‹ä¸€æ­¥çš„è¯æ±‡ã€‚PPL å®šä¹‰ä¸ºåºåˆ—çš„æŒ‡æ•°åŒ–å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ª**æ ‡è®°åŒ–åºåˆ—** $X = (x_0, x_1, \dots, x_t)$ï¼Œé‚£ä¹ˆè¯¥åºåˆ—çš„å›°æƒ‘åº¦å®šä¹‰ä¸ºï¼š
>
> $\text{PPL}(X) = \exp \left( -\frac{1}{t} \sum_{i=1}^{t} \log p_\theta (x_i \mid x_{<i}) \right)$
>
> å…¶ä¸­, $\log p_\theta (x_i \mid x_{<i})$ æ˜¯æ¨¡å‹åœ¨ç»™å®šå‰ $i-1$ ä¸ªè¯ $x_{<i}$ çš„æ¡ä»¶ä¸‹ï¼Œå¯¹ç¬¬ $i$ ä¸ªè¯ $x_i$ çš„å¯¹æ•°ä¼¼ç„¶ã€‚ç›´è§‚åœ°ï¼ŒPPL å¯ä»¥è¢«çœ‹ä½œæ˜¯æ¨¡å‹åœ¨è¯­æ–™åº“ä¸­æŒ‡å®šè¯æ±‡é›†åˆä¸Šè¿›è¡Œå‡åŒ€é¢„æµ‹çš„èƒ½åŠ›è¯„ä¼°ã€‚
>
> - **è¾ƒä½çš„ PPL**ï¼šè¡¨ç¤ºæ¨¡å‹æ›´å‡†ç¡®åœ°é¢„æµ‹äº†æ–‡æœ¬ä¸­çš„è¯æ±‡ï¼Œæ¨¡å‹å¯¹è¯­è¨€ç»“æ„çš„æŒæ¡æ›´å¥½ã€‚
> - **è¾ƒé«˜çš„ PPL**ï¼šè¡¨ç¤ºæ¨¡å‹å¯¹æ–‡æœ¬çš„é¢„æµ‹è¾ƒä¸ºä¸ç¡®å®šï¼Œå›°æƒ‘åº¦é«˜ï¼Œæ¨¡å‹çš„è¡¨ç°è¾ƒå·®ã€‚
>
> åœ¨ AWQ å’Œ GPTQ çš„é‡åŒ–ç ”ç©¶ä¸­ï¼ŒPPL è¢«ç”¨æ¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé‡åŒ–ä½å®½ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œä¾‹å¦‚ 3-bit æˆ– 4-bit é‡åŒ–å¯¹æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„å½±å“ã€‚
>
> **è®¡ç®—å›ºå®šé•¿åº¦æ¨¡å‹çš„ PPL**
>
> å¦‚æœä¸å—æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦çš„é™åˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è‡ªå›å½’åœ°åˆ†è§£åºåˆ—ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥æ ¹æ®æ•´ä¸ªå‰åºå­åºåˆ—æ¥è¯„ä¼°æ¨¡å‹çš„å›°æƒ‘åº¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
>
> ![Full decomposition of a sequence with unlimited context length](./assets/ppl_full.gif)
>
> ä½†æ˜¯åœ¨å®é™…ä¸­ï¼Œæ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æ˜¯æœ‰é™çš„ï¼Œä¾‹å¦‚ [GPT-2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2) çš„æœ€å¤§é•¿åº¦ä¸º 1024 ä¸ªæ ‡è®°ã€‚å› æ­¤ï¼Œå½“ $t$ è¶…è¿‡ 1024 æ—¶ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¡ç®— $p_\theta(x_t | x_{<t})$ã€‚
>
> é€šå¸¸ï¼Œåºåˆ—ä¼šè¢«åˆ†å‰²ä¸ºæœ€å¤§è¾“å…¥å¤§å°çš„å­åºåˆ—ã€‚å¦‚æœæœ€å¤§è¾“å…¥å¤§å°ä¸º $k$ï¼Œæˆ‘ä»¬å°±åŸºäºå‰ $k-1$ ä¸ªæ ‡è®°æ¥è¿‘ä¼¼è®¡ç®— $x_t$ çš„ä¼¼ç„¶ã€‚å¦ä¸€ç§è¿‘ä¼¼æ–¹æ³•æ˜¯å°†åºåˆ—åˆ†å‰²ä¸ºä¸é‡å çš„å—ï¼Œç‹¬ç«‹ç´¯åŠ æ¯ä¸ªæ®µçš„åˆ†è§£å¯¹æ•°ä¼¼ç„¶ï¼š
>
> ![Suboptimal PPL not taking advantage of full available context](./assets/ppl_chunked.gif)
>
> è¿™ç§æ–¹æ³•è®¡ç®—é€Ÿåº¦å¿«ï¼Œä½†ä»å›¾ç¤ºå¯ä»¥çœ‹å‡ºå­˜åœ¨çš„é—®é¢˜ï¼Œåœ¨æ–°çš„ä¸€ä¸ªçª—å£å¼€å§‹æ—¶ï¼Œä¸Šä¸‹æ–‡æ˜¯æœ‰é™çš„ï¼Œè¿™ä¼šå¯¼è‡´æ›´é«˜çš„ PPLã€‚æ‰€ä»¥å¾ˆè‡ªç„¶çš„æƒ³åˆ°**æ»‘åŠ¨çª—å£ç­–ç•¥**ã€‚
>
> ![Sliding window PPL taking advantage of all available context](./assets/ppl_sliding.gif)
>
> è¿™ç§ç­–ç•¥æ›´æ¥è¿‘çœŸå®çš„åºåˆ—æ¦‚ç‡åˆ†è§£ï¼Œå¹¶ä¸”é€šå¸¸ä¼šå¾—åˆ°æ›´ä½çš„ PPLã€‚ç¼ºç‚¹æ˜¯å®ƒéœ€è¦ä¸ºè¯­æ–™åº“ä¸­çš„æ¯ä¸ªæ ‡è®°è¿›è¡Œå•ç‹¬çš„å‰å‘ä¼ æ’­ã€‚ä¸€ä¸ªæŠ˜è¡·æ–¹æ¡ˆæ˜¯é‡‡ç”¨**è·¨æ­¥æ»‘åŠ¨çª—å£**ï¼Œå³é€šè¿‡æ›´å¤§çš„æ­¥å¹…ï¼ˆStrideï¼‰ç§»åŠ¨ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯æ¯æ¬¡æ»‘åŠ¨ä¸€ä¸ªæ ‡è®°ï¼ˆTokenï¼‰ã€‚
>
> ï¼ˆğŸ¤”è€ƒè™‘æ˜¯å¦ç¼©å‡è¿™éƒ¨åˆ†çš„è¡¨è¿°åç»­å•å¼€ä¸€ç« ä»£ç è¿›è¡Œè®²è§£ï¼Œå› ä¸ºæ€»æ„Ÿè§‰ä¸å¤Ÿç›´è§‚ï¼Œå³ä¾¿æœ‰ç€hugging faceæ–‡æ¡£é“¾æ¥ï¼‰

### GGUF æ–‡ä»¶å‘½å

> å‚è€ƒé“¾æ¥ï¼š[GGUF - docs](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) 

GGUF æ ¼å¼å°†åŠ è½½æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯å°è£…åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œç®€åŒ–äº†æ¨¡å‹çš„åˆ†å‘å’Œéƒ¨ç½²ã€‚åŒæ—¶ï¼ŒGGUF æ–‡ä»¶å‘½åéµå¾ª `<BaseName><SizeLabel><FineTune><Version><Encoding><Type><Shard>.gguf` çš„è§„åˆ™ï¼Œæ–¹ä¾¿äººä»¬å¿«é€Ÿè¯†åˆ«æ¨¡å‹çš„å…³é”®ä¿¡æ¯ã€‚å…·ä½“è¯´æ˜å¦‚ä¸‹ï¼š

- **BaseName**ï¼šæ¨¡å‹çš„åŸºç¡€åç§°æˆ–æ¶æ„åç§°ï¼Œä¾‹å¦‚ `Llama`ã€‚

- **SizeLabel**ï¼šæ¨¡å‹çš„å‚æ•°è§„æ¨¡æ ‡ç­¾ï¼Œè¡¨ç¤ºæ¨¡å‹çš„å‚æ•°æ•°é‡åŠå¯èƒ½çš„ä¸“å®¶æ•°é‡ï¼Œæ ¼å¼ä¸º `<expertCount>x<count><scale-prefix>`ã€‚

  - **expertCount**ï¼šè¡¨ç¤ºä¸“å®¶æ¨¡å‹ä¸­çš„ä¸“å®¶æ•°é‡ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ä½¿ç”¨ Mixture of Experts (MoE) æ¶æ„ï¼Œå¯ä»¥çœç•¥ã€‚

  - **Count**ï¼š

    - `Q`: è¡¨ç¤ºç™¾ä¸‡äº¿ï¼ˆquadrillionï¼‰å‚æ•°ã€‚
    - `T`: è¡¨ç¤ºä¸‡äº¿ï¼ˆtrillionï¼‰å‚æ•°ã€‚
    - `B`: è¡¨ç¤ºåäº¿ï¼ˆbillionï¼‰å‚æ•°ã€‚
    - `M`: è¡¨ç¤ºç™¾ä¸‡ï¼ˆmillionï¼‰å‚æ•°ã€‚
    - `K`: è¡¨ç¤ºåƒï¼ˆthousandï¼‰å‚æ•°ã€‚

    å½“å‰ä¸»æµå¤§æ¨¡å‹å¤šä¸º B çº§å‚æ•°ï¼ˆåäº¿çº§ï¼‰ï¼Œä½†æœªæ¥ Tï¼ˆä¸‡äº¿çº§ï¼‰æ¨¡å‹å¯èƒ½ä¼šæˆä¸ºä¸»æµã€‚

    æ›´è¯¦ç»†çš„å†…å®¹è§[é™„å½•](#é™„å½•)éƒ¨åˆ†ã€‚

  - **é™„åŠ å±æ€§**ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ`-<attributes><count><scale-prefix>` å¯ä»¥è¿›ä¸€æ­¥ç»†åŒ–æ¨¡å‹çš„æè¿°ï¼Œæ·»åŠ é¢å¤–çš„å‚æ•°ï¼Œä¾‹å¦‚ `Q`, `K`, `T`ï¼Œè¿™äº›è¡¨ç¤ºé‡åŒ–æ–¹å¼æˆ–å…¶ä»–æ¨¡å‹ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼š

    - `Q4`: è¡¨ç¤º 4-bit é‡åŒ–ã€‚

    ç¤ºä¾‹ï¼š

    - `7B`: è¡¨ç¤º 70 äº¿å‚æ•°çš„æ¨¡å‹ã€‚
    - `4x3T`: è¡¨ç¤ºæœ‰ 4 ä¸ªä¸“å®¶çš„ 3 ä¸‡äº¿å‚æ•°æ¨¡å‹ã€‚
    - `2x10B-Q4`: è¡¨ç¤ºæœ‰ 2 ä¸ªä¸“å®¶ä¸”é‡‡ç”¨ Q4 é‡åŒ–çš„ 100 äº¿å‚æ•°æ¨¡å‹ã€‚

- **FineTune**ï¼šå¾®è°ƒç›®æ ‡æè¿°ï¼ˆå¦‚ `Chat`ã€`Instruct`ï¼‰ã€‚

- **Version**ï¼ˆå¯é€‰ï¼‰ï¼šæ¨¡å‹çš„ç‰ˆæœ¬å·ï¼Œæ ¼å¼ä¸º `v<Major>.<Minor>`ï¼Œæ²¡æä¾›åˆ™å‡è®¾ä¸º `v1.0`ã€‚

- **Encoding**ï¼šæƒé‡ç¼–ç æ–¹æ¡ˆï¼ˆå¦‚ `Q4_0` è¡¨ç¤º 4-bit é‡åŒ–ï¼‰ã€‚

- **Type**ï¼šæ–‡ä»¶ç±»å‹ï¼Œå¦‚ `LoRA`ï¼ˆé€‚é…å™¨ï¼‰æˆ– `vocab`ï¼ˆä»…åŒ…å«è¯æ±‡è¡¨ï¼‰ã€‚

- **Shard**ï¼ˆå¯é€‰ï¼‰ï¼šæ¨¡å‹åˆ†ç‰‡ä¿¡æ¯ï¼Œæ ¼å¼ä¸º `<ShardNum>-of-<ShardTotal>`ï¼Œé€‚ç”¨äºå¤§å‹æ¨¡å‹ã€‚ä¾‹å¦‚ `00003-of-00009` è¡¨ç¤ºç¬¬ 3 ä¸ªåˆ†ç‰‡ï¼Œå…± 9 ä¸ªåˆ†ç‰‡ï¼Œæ³¨æ„åˆ†ç‰‡ç¼–å·ä» `00001` å¼€å§‹ï¼Œè€Œé `00000`ã€‚

> [!TIP]
>
> éªŒè¯å‘½åæ˜¯å¦ç¬¦åˆè§„èŒƒçš„æ­£åˆ™ï¼š
>
> ```
> ^(?<BaseName>[A-Za-z0-9\s]*(?:(?:-(?:(?:[A-Za-z\s][A-Za-z0-9\s]*)|(?:[0-9\s]*)))*))-(?:(?<SizeLabel>(?:\d+x)?(?:\d+\.)?\d+[A-Za-z](?:-[A-Za-z]+(\d+\.)?\d+[A-Za-z]+)?)(?:-(?<FineTune>[A-Za-z0-9\s-]+))?)?-(?:(?<Version>v\d+(?:\.\d+)*))(?:-(?<Encoding>(?!LoRA|vocab)[\w_]+))?(?:-(?<Type>LoRA|vocab))?(?:-(?<Shard>\d{5}-of-\d{5}))?\.gguf$
> ```

å°è¯•ç†è§£ä¸‹é¢ä¸‰ä¸ªæ¥è‡ªå®˜æ–¹æ–‡æ¡£çš„æ–‡ä»¶å‘½åï¼Œçœ‹çœ‹ä½ èƒ½å¦æ­£ç¡®è§£æï¼š

1. **Mixtral-8x7B-v0.1-KQ2.gguf**
2. **Hermes-2-Pro-Llama-3-8B-F16.gguf**
3. **Grok-100B-v1.0-Q4_0-00003-of-00009.gguf**

åœ¨æ–‡ç« çš„æœ«å°¾ä¼šç»™å‡ºè§£æç­”æ¡ˆï¼Œç°åœ¨è¯·åœä¸‹æ¥æ€è€ƒã€‚

### GGUF æ–‡ä»¶ç»“æ„

![*diagram by @mishig25(GGUF v3)*](./assets/313174776-c3623641-3a1d-408e-bfaf-1b7c4e16aa63-2.png)

å¦‚æœæƒ³è¿›ä¸€æ­¥äº†è§£ï¼ŒæŸ¥çœ‹[é™„å½•](#é™„å½•)éƒ¨åˆ†çš„ä»£ç ã€‚


> [!NOTE]
>
> 1. ä¸è¦å› ä¸º ChatGPT çš„å­˜åœ¨å°† GPT çš„æ¦‚å¿µç›´æ¥æ˜ å°„ä¸º OpenAIï¼ŒGPTï¼ˆGenerative Pre-trained Transformerï¼‰æŒ‡çš„æ˜¯**ç”Ÿæˆå¼é¢„è®­ç»ƒ Transformer**ã€‚
> 2. å¦‚æœä½ é€‰æ‹©çš„æ˜¯å…¶ä»–æ¨¡å‹ï¼Œç”¨ä¸‹é¢çš„æ–¹å¼å»æœç´¢æ˜¯å¦å­˜åœ¨é‡åŒ–ç‰ˆæœ¬ï¼Œå‡è®¾ä½ è¦æ‰¾çš„æ˜¯ INT4ï¼š
>
> - [æ¨¡å‹åç§°]-[AWQ]/[GPTQ]/[GGUF]
> - [æ¨¡å‹åç§°]-[INT4]

## æœªå®Œå¾…ç»­...







### æ–‡ä»¶åè§£æç­”æ¡ˆ

- **Mixtral-8x7B-v0.1-KQ2.gguf**ï¼š
  - **BaseName**ï¼šMixtral
  - **SizeLabel**ï¼š
    - Expert Count: 8
    - Parameter Count: 7B
  - **Version**ï¼šv0.1
  - **Encoding**ï¼šKQ2

- **Hermes-2-Pro-Llama-3-8B-F16.gguf**ï¼š
  - **BaseName**ï¼šHermes 2 Pro Llama 3
  - **SizeLabel**ï¼š
    - Expert Count: 0
    - Parameter Count: 8B
  - **Version**ï¼šv1.0
  - **Encoding**ï¼šF16

- **Grok-100B-v1.0-Q4_0-00003-of-00009.gguf**ï¼š
  - **BaseName**ï¼šGrok
  - **SizeLabel**ï¼š
    - Expert Count: 0
    - Parameter Count: 100B
  - **Version**ï¼šv1.0
  - **Encoding**ï¼šQ4_0
  - **Shard**ï¼šç¬¬ 3 ä¸ªåˆ†ç‰‡ï¼Œå…± 9 ä¸ªåˆ†ç‰‡

## é™„å½•

### GGUF æ–‡ä»¶å‘½å

> [Quantization Types](https://huggingface.co/docs/hub/gguf#quantization-types)

| ç±»å‹    | æ¥æº                                                         | æè¿°                                                         |
| ------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| F64     | [Wikipedia](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) | 64 ä½æ ‡å‡† IEEE 754 åŒç²¾åº¦æµ®ç‚¹æ•°ã€‚                            |
| I64     | [GH](https://github.com/ggerganov/llama.cpp/pull/6062)       | 64 ä½å®šå®½æ•´æ•°ã€‚                                              |
| F32     | [Wikipedia](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) | 32 ä½æ ‡å‡† IEEE 754 å•ç²¾åº¦æµ®ç‚¹æ•°ã€‚                            |
| I32     | [GH](https://github.com/ggerganov/llama.cpp/pull/6045)       | 32 ä½å®šå®½æ•´æ•°ã€‚                                              |
| F16     | [Wikipedia](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) | 16 ä½æ ‡å‡† IEEE 754 åŠç²¾åº¦æµ®ç‚¹æ•°ã€‚                            |
| BF16    | [Wikipedia](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) | 32 ä½ IEEE 754 å•ç²¾åº¦æµ®ç‚¹æ•°çš„ 16 ä½ç®€åŒ–ç‰ˆæœ¬ã€‚                |
| I16     | [GH](https://github.com/ggerganov/llama.cpp/pull/6045)       | 16 ä½å®šå®½æ•´æ•°ã€‚                                              |
| Q8_0    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249) | 8 ä½å››èˆäº”å…¥é‡åŒ–ï¼ˆqï¼‰ã€‚æ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale`ã€‚ç›®å‰å·²ä¸å¹¿æ³›ä½¿ç”¨çš„è¿‡æ—¶é‡åŒ–æ–¹æ³•ã€‚ |
| Q8_1    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290) | 8 ä½å››èˆäº”å…¥é‡åŒ–ï¼ˆqï¼‰ã€‚æ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale + block_minimum`ã€‚ç›®å‰å·²ä¸å¹¿æ³›ä½¿ç”¨çš„è¿‡æ—¶é‡åŒ–æ–¹æ³•ã€‚ |
| Q8_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 8 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚æ¯ä¸ªå—æœ‰ 256 ä¸ªæƒé‡ã€‚ä»…ç”¨äºé‡åŒ–ä¸­é—´ç»“æœã€‚æ­¤é‡åŒ–ç±»å‹æ”¯æŒæ‰€æœ‰ 2-6 ä½ç‚¹ç§¯ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale`ã€‚ |
| I8      | [GH](https://github.com/ggerganov/llama.cpp/pull/6045)       | 8 ä½å®šå®½æ•´æ•°ã€‚                                               |
| Q6_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 6 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 16 ä¸ªå—ï¼Œæ¯ä¸ªå—æœ‰ 16 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale(8-bit)`ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 6.5625 ä½ã€‚ |
| Q5_0    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249) | 5 ä½å››èˆäº”å…¥é‡åŒ–ï¼ˆqï¼‰ã€‚æ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale`ã€‚ç›®å‰å·²ä¸å¹¿æ³›ä½¿ç”¨çš„è¿‡æ—¶é‡åŒ–æ–¹æ³•ã€‚ |
| Q5_1    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290) | 5 ä½å››èˆäº”å…¥é‡åŒ–ï¼ˆqï¼‰ã€‚æ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale + block_minimum`ã€‚ç›®å‰å·²ä¸å¹¿æ³›ä½¿ç”¨çš„è¿‡æ—¶é‡åŒ–æ–¹æ³•ã€‚ |
| Q5_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 5 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 8 ä¸ªå—ï¼Œæ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale(6-bit) + block_min(6-bit)`ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 5.5 ä½ã€‚ |
| Q4_0    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249) | 4 ä½å››èˆäº”å…¥é‡åŒ–ï¼ˆqï¼‰ã€‚æ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale`ã€‚ç›®å‰å·²ä¸å¹¿æ³›ä½¿ç”¨çš„è¿‡æ—¶é‡åŒ–æ–¹æ³•ã€‚ |
| Q4_1    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290) | 4 ä½å››èˆäº”å…¥é‡åŒ–ï¼ˆqï¼‰ã€‚æ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale + block_minimum`ã€‚ç›®å‰å·²ä¸å¹¿æ³›ä½¿ç”¨çš„è¿‡æ—¶é‡åŒ–æ–¹æ³•ã€‚ |
| Q4_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 4 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 8 ä¸ªå—ï¼Œæ¯ä¸ªå—æœ‰ 32 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale(6-bit) + block_min(6-bit)`ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 4.5 ä½ã€‚ |
| Q3_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 3 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 16 ä¸ªå—ï¼Œæ¯ä¸ªå—æœ‰ 16 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale(6-bit)`ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 3.4375 ä½ã€‚ |
| Q2_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 2 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 16 ä¸ªå—ï¼Œæ¯ä¸ªå—æœ‰ 16 ä¸ªæƒé‡ã€‚æƒé‡å…¬å¼ï¼š`w = q * block_scale(4-bit) + block_min(4-bit)`ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 2.5625 ä½ã€‚ |
| IQ4_NL  | [GH](https://github.com/ggerganov/llama.cpp/pull/5590)       | 4 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ã€‚ |
| IQ4_XS  | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 4 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 4.25 ä½ã€‚ |
| IQ3_S   | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 3 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 3.44 ä½ã€‚ |
| IQ3_XXS | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 3 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 3.06 ä½ã€‚ |
| IQ2_XXS | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 2 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 2.06 ä½ã€‚ |
| IQ2_S   | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 2 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 2.5 ä½ã€‚ |
| IQ2_XS  | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 2 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 2.31 ä½ã€‚ |
| IQ1_S   | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 1 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 1.56 ä½ã€‚ |
| IQ1_M   | [GH](https://github.com/ggerganov/llama.cpp/pull/6302)       | 1 ä½é‡åŒ–ï¼ˆqï¼‰ã€‚è¶…å—åŒ…å« 256 ä¸ªæƒé‡ã€‚æƒé‡ `w` é€šè¿‡ `super_block_scale` å’Œ `importance matrix` è®¡ç®—å¾—åˆ°ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 1.75 ä½ã€‚ |

### GGUF æ–‡ä»¶ç»“æ„

```c
enum ggml_type: uint32_t {
    GGML_TYPE_F32     = 0,
    GGML_TYPE_F16     = 1,
    GGML_TYPE_Q4_0    = 2,
    GGML_TYPE_Q4_1    = 3,
    // GGML_TYPE_Q4_2 = 4, support has been removed
    // GGML_TYPE_Q4_3 = 5, support has been removed
    GGML_TYPE_Q5_0    = 6,
    GGML_TYPE_Q5_1    = 7,
    GGML_TYPE_Q8_0    = 8,
    GGML_TYPE_Q8_1    = 9,
    GGML_TYPE_Q2_K    = 10,
    GGML_TYPE_Q3_K    = 11,
    GGML_TYPE_Q4_K    = 12,
    GGML_TYPE_Q5_K    = 13,
    GGML_TYPE_Q6_K    = 14,
    GGML_TYPE_Q8_K    = 15,
    GGML_TYPE_IQ2_XXS = 16,
    GGML_TYPE_IQ2_XS  = 17,
    GGML_TYPE_IQ3_XXS = 18,
    GGML_TYPE_IQ1_S   = 19,
    GGML_TYPE_IQ4_NL  = 20,
    GGML_TYPE_IQ3_S   = 21,
    GGML_TYPE_IQ2_S   = 22,
    GGML_TYPE_IQ4_XS  = 23,
    GGML_TYPE_I8      = 24,
    GGML_TYPE_I16     = 25,
    GGML_TYPE_I32     = 26,
    GGML_TYPE_I64     = 27,
    GGML_TYPE_F64     = 28,
    GGML_TYPE_IQ1_M   = 29,
    GGML_TYPE_COUNT,
};

enum gguf_metadata_value_type: uint32_t {
    // The value is a 8-bit unsigned integer.
    GGUF_METADATA_VALUE_TYPE_UINT8 = 0,
    // The value is a 8-bit signed integer.
    GGUF_METADATA_VALUE_TYPE_INT8 = 1,
    // The value is a 16-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT16 = 2,
    // The value is a 16-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT16 = 3,
    // The value is a 32-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT32 = 4,
    // The value is a 32-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT32 = 5,
    // The value is a 32-bit IEEE754 floating point number.
    GGUF_METADATA_VALUE_TYPE_FLOAT32 = 6,
    // The value is a boolean.
    // 1-byte value where 0 is false and 1 is true.
    // Anything else is invalid, and should be treated as either the model being invalid or the reader being buggy.
    GGUF_METADATA_VALUE_TYPE_BOOL = 7,
    // The value is a UTF-8 non-null-terminated string, with length prepended.
    GGUF_METADATA_VALUE_TYPE_STRING = 8,
    // The value is an array of other values, with the length and type prepended.
    ///
    // Arrays can be nested, and the length of the array is the number of elements in the array, not the number of bytes.
    GGUF_METADATA_VALUE_TYPE_ARRAY = 9,
    // The value is a 64-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT64 = 10,
    // The value is a 64-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT64 = 11,
    // The value is a 64-bit IEEE754 floating point number.
    GGUF_METADATA_VALUE_TYPE_FLOAT64 = 12,
};

// A string in GGUF.
struct gguf_string_t {
    // The length of the string, in bytes.
    uint64_t len;
    // The string as a UTF-8 non-null-terminated string.
    char string[len];
};

union gguf_metadata_value_t {
    uint8_t uint8;
    int8_t int8;
    uint16_t uint16;
    int16_t int16;
    uint32_t uint32;
    int32_t int32;
    float float32;
    uint64_t uint64;
    int64_t int64;
    double float64;
    bool bool_;
    gguf_string_t string;
    struct {
        // Any value type is valid, including arrays.
        gguf_metadata_value_type type;
        // Number of elements, not bytes
        uint64_t len;
        // The array of values.
        gguf_metadata_value_t array[len];
    } array;
};

struct gguf_metadata_kv_t {
    // The key of the metadata. It is a standard GGUF string, with the following caveats:
    // - It must be a valid ASCII string.
    // - It must be a hierarchical key, where each segment is `lower_snake_case` and separated by a `.`.
    // - It must be at most 2^16-1/65535 bytes long.
    // Any keys that do not follow these rules are invalid.
    gguf_string_t key;

    // The type of the value.
    // Must be one of the `gguf_metadata_value_type` values.
    gguf_metadata_value_type value_type;
    // The value.
    gguf_metadata_value_t value;
};

struct gguf_header_t {
    // Magic number to announce that this is a GGUF file.
    // Must be `GGUF` at the byte level: `0x47` `0x47` `0x55` `0x46`.
    // Your executor might do little-endian byte order, so it might be
    // check for 0x46554747 and letting the endianness cancel out.
    // Consider being *very* explicit about the byte order here.
    uint32_t magic;
    // The version of the format implemented.
    // Must be `3` for version described in this spec, which introduces big-endian support.
    //
    // This version should only be increased for structural changes to the format.
    // Changes that do not affect the structure of the file should instead update the metadata
    // to signify the change.
    uint32_t version;
    // The number of tensors in the file.
    // This is explicit, instead of being included in the metadata, to ensure it is always present
    // for loading the tensors.
    uint64_t tensor_count;
    // The number of metadata key-value pairs.
    uint64_t metadata_kv_count;
    // The metadata key-value pairs.
    gguf_metadata_kv_t metadata_kv[metadata_kv_count];
};

uint64_t align_offset(uint64_t offset) {
    return offset + (ALIGNMENT - (offset % ALIGNMENT)) % ALIGNMENT;
}

struct gguf_tensor_info_t {
    // The name of the tensor. It is a standard GGUF string, with the caveat that
    // it must be at most 64 bytes long.
    gguf_string_t name;
    // The number of dimensions in the tensor.
    // Currently at most 4, but this may change in the future.
    uint32_t n_dimensions;
    // The dimensions of the tensor.
    uint64_t dimensions[n_dimensions];
    // The type of the tensor.
    ggml_type type;
    // The offset of the tensor's data in this file in bytes.
    //
    // This offset is relative to `tensor_data`, not to the start
    // of the file, to make it easier for writers to write the file.
    // Readers should consider exposing this offset relative to the
    // file to make it easier to read the data.
    //
    // Must be a multiple of `ALIGNMENT`. That is, `align_offset(offset) == offset`.
    uint64_t offset;
};

struct gguf_file_t {
    // The header of the file.
    gguf_header_t header;

    // Tensor infos, which can be used to locate the tensor data.
    gguf_tensor_info_t tensor_infos[header.tensor_count];

    // Padding to the nearest multiple of `ALIGNMENT`.
    //
    // That is, if `sizeof(header) + sizeof(tensor_infos)` is not a multiple of `ALIGNMENT`,
    // this padding is added to make it so.
    //
    // This can be calculated as `align_offset(position) - position`, where `position` is
    // the position of the end of `tensor_infos` (i.e. `sizeof(header) + sizeof(tensor_infos)`).
    uint8_t _padding[];

    // Tensor data.
    //
    // This is arbitrary binary data corresponding to the weights of the model. This data should be close
    // or identical to the data in the original model file, but may be different due to quantization or
    // other optimizations for inference. Any such deviations should be recorded in the metadata or as
    // part of the architecture definition.
    //
    // Each tensor's data must be stored within this array, and located through its `tensor_infos` entry.
    // The offset of each tensor's data must be a multiple of `ALIGNMENT`, and the space between tensors
    // should be padded to `ALIGNMENT` bytes.
    uint8_t tensor_data[];
};
```

