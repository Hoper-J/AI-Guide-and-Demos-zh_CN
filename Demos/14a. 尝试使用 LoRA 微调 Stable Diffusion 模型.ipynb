{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13831c9-2a09-40e7-98ae-3ee6be8a52cd",
   "metadata": {},
   "source": [
    "# a. å°è¯•ä½¿ç”¨ LoRA å¾®è°ƒ Stable Diffusion æ¨¡å‹ï¼ˆæ–‡ç”Ÿå›¾ï¼‰\n",
    "\n",
    "> [HW10: Stable Diffusion Fine-tuning](https://colab.research.google.com/drive/1dI_-HVggxyIwDVoreymviwg6ZOvEHiLS?usp=sharing#scrollTo=CnJtiRaRuTFX) ä¸­æ–‡é•œåƒç‰ˆ\n",
    ">\n",
    "> è¿™æ¬¡ä¸æ˜¯1:1çš„é•œåƒï¼Œæˆ‘é‡æ„äº†ä¸»è¦çš„ä»£ç é€»è¾‘ï¼Œä»¥ä¾¿äºé˜…è¯»ã€‚å¯¹äºå¯ä»¥è®¿é—® Colab çš„åŒå­¦æ¥è¯´ï¼Œä¸€æ ·å»ºè®®ä½¿ç”¨ä¸‹é¢è¿™ä»½ä»£ç è¿›è¡Œå­¦ä¹ ã€‚\n",
    ">\n",
    "> ä¿®æ”¹ï¼š\n",
    "> 1. Colab ä¸­çš„ä»£ç æ¯æ¬¡è®­ç»ƒæ—¶å…ˆå¯¼å…¥äº† checkpoint-last æ–‡ä»¶å¤¹ä¸‹çš„ unet å’Œ text_encoderï¼Œè¿™å°±æ„å‘³ç€æ¯æ¬¡é‡æ–°è¿è¡Œæ˜¯å¼ºåˆ¶ resume çš„ã€‚\n",
    "> æˆ‘å¯¹è¿™ä¸ªè¡Œä¸ºè¿›è¡Œäº†ä¿®æ”¹ï¼Œä½ ç°åœ¨å¯ä»¥é€šè¿‡æŒ‡å®š prepare_lora_model ä¸­çš„ resume å‚æ•°æ¥é€‰æ‹©æ˜¯å¦æ¥ç€ä¹‹å‰çš„è¿›è¡Œè®­ç»ƒã€‚\n",
    "> å»ºè®®è®¾ç½®ä¸º Falseï¼Œæ¯æ¬¡é‡æ–°è¿è¡Œæ‰èƒ½å¯¹ä¸åŒå‚æ•°æœ‰ç›´è§‚çš„æ„Ÿå—ã€‚\n",
    ">\n",
    "> 2. åŸä»£ç ä¸­å¯¼å…¥äº† PEFT å¹¶è®¾ç½®äº† LoRA å‚æ•°å´æ²¡æœ‰ç”¨åˆ°ï¼Œå‡ºäºå­¦ä¹ è€Œéæäº¤ä½œä¸šçš„ç›®çš„ï¼Œæˆ‘ä¼šé‡æ–°å†™è¿™ä¸€æ¨¡å—ã€‚\n",
    "> é‡å†™ä¹‹åçš„å¯è®­ç»ƒå‚æ•°å±‚ä¸æ•°é‡å·²ç»ç»è¿‡éªŒè¯ï¼Œä¸ [yahcreeper/GenAI-HW10-Model](https://huggingface.co/yahcreeper/GenAI-HW10-Model/tree/main/checkpoint-last) å®Œå…¨ä¸€è‡´ï¼Œå¯æ”¾å¿ƒä½¿ç”¨å½“å‰ç‰ˆæœ¬ï¼ˆå…³äºå¯è®­ç»ƒæ•°é‡ï¼Œå¯ä»¥å‚è€ƒ[ã€Š14. PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRA.mdã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/14.%20PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨%20LoRA.md)ä½¿ç”¨ `print_trainable_parameters()` è¿›è¡Œæ‰“å°ï¼‰ã€‚\n",
    ">\n",
    "> 3. ä¼šå¢åŠ æ³¨é‡Šä»¥ä¾¿ç†è§£ã€‚\n",
    ">\n",
    "> æŒ‡å¯¼æ–‡ç« ï¼š[16. ç”¨ LoRA å¾®è°ƒ Stable Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡ AI ç»˜ç”»](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/16.%20ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡%20AI%20ç»˜ç”».md)\n",
    "> \n",
    "\n",
    "ä½ å°†å¾®è°ƒä½ è‡ªå·±çš„ Stable Diffusion æ¨¡å‹ï¼Œä»ç»™å®šçš„æ–‡æœ¬æè¿°ç”Ÿæˆè‡ªå®šä¹‰å›¾åƒï¼Œåœ¨åˆå§‹åŒ–é¡¹ç›®ä¹‹åï¼Œä½ å°†å¯ä»¥åœ¨ `SD/Datasets/Brad` ä¸‹çœ‹åˆ°å›¾åƒå’Œå¯¹åº”çš„æ–‡æœ¬æè¿°ã€‚\n",
    "\n",
    "å½“å‰ä»£ç ä½¿ç”¨åˆ°çš„æ•°æ®é›† `dataset.zip` å·²ç»æ”¾åœ¨ Demos/data/14 ä¸‹é¢ã€‚\n",
    "\n",
    "ä½ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä»½ä»£ç å€¾å‘äºå¦‚ä½•å¾®è°ƒè€ŒéåŠ è½½æ•°æ®é›†æˆ–è€…è¯„ä¼°ï¼Œå¯¹äºåˆå­¦è€…ï¼Œå¯ä»¥ä»…å…³æ³¨è®­ç»ƒå‚æ•°å’ŒLoRAéƒ¨åˆ†ï¼Œå…¶ä½™éƒ¨åˆ†çš„ä»£ç ç»†èŠ‚ä¸ç”¨å…³æ³¨ã€‚\n",
    "\n",
    "è¿™æ¬¡çš„å­¦ä¹ å¯ä»¥åœ¨ä¸¤ç§ç‰ˆæœ¬ä¸­äºŒé€‰ä¸€ï¼Œå­¦ä¹ æ•ˆæœä¸€è‡´ã€‚\n",
    "\n",
    "[ç‰ˆæœ¬ b - ç²¾ç®€ç‰ˆ](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/14b.%20å°è¯•ä½¿ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusion%20æ¨¡å‹%20-%20ç²¾ç®€ç‰ˆ.ipynb)\n",
    "\n",
    "è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç®€å•çš„ [ğŸ¡ SD LoRA è„šæœ¬](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/sd_lora.py)ä¾›ä½ å°è¯•ï¼Œè¯¦è§ï¼š[CodePlayground](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/README.md#å½“å‰çš„ç©å…·)ï¼Œç‚¹å‡» `â–º` æˆ–å¯¹åº”çš„æ–‡æœ¬å±•å¼€ã€‚\n",
    "\n",
    "åœ¨çº¿é“¾æ¥ï¼ˆç²¾ç®€ç‰ˆï¼‰ï¼š[Kaggle](https://www.kaggle.com/code/aidemos/14b-lora-stable-diffusion) | [Colab](https://colab.research.google.com/drive/1idmnaQZwRhjUPw7ToEXlVo82Mihfl_aA?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8deb6-6844-400c-bb10-388d89d3a020",
   "metadata": {},
   "source": [
    "## å®‰è£…å¿…è¦çš„åº“\n",
    "\n",
    "æœ¬å•å…ƒä¸­å°†å®‰è£…ä¸€äº›åº“ç”¨äºå¾®è°ƒã€‚\n",
    "\n",
    "å®‰è£…å¤§çº¦éœ€è¦ 5 åˆ†é’Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629cf04-6988-483d-bfc9-509f8b59a9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "!pip install fairscale\n",
    "!pip install transformers\n",
    "!pip install requests\n",
    "!pip install accelerate\n",
    "!pip install diffusers\n",
    "!pip install einop\n",
    "!pip install safetensors\n",
    "!pip install voluptuous\n",
    "!pip install jax\n",
    "!pip install jaxlib\n",
    "!pip install peft\n",
    "!pip install deepface==0.0.90\n",
    "!pip install tensorflow==2.9.0  # ä¸ºäº†é¿å…æœ€åè¯„ä¼°é˜¶æ®µä½¿ç”¨deepfaceæ—¶çš„é”™è¯¯ï¼Œè¿™é‡Œé€‰æ‹©é™çº§ç‰ˆæœ¬\n",
    "!pip install keras\n",
    "!pip install opencv-python\n",
    "#!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a92e17-0911-4f29-9bc1-6f81e1dc287d",
   "metadata": {},
   "source": [
    "## å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2c745-a37a-42d8-940c-2cdcbc784b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ ‡å‡†åº“æ¨¡å— ==========\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# ========== ç¬¬ä¸‰æ–¹åº“ ==========\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ========== æ·±åº¦å­¦ä¹ ç›¸å…³åº“ ==========\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import AutoProcessor, AutoModel, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Diffusers (Hugging Face)\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DiffusionPipeline,\n",
    "    StableDiffusionPipeline,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import convert_state_dict_to_diffusers\n",
    "from diffusers.training_utils import compute_snr\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "\n",
    "# ========== LoRA æ¨¡å‹åº“ ==========\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ========== é¢éƒ¨æ£€æµ‹åº“ ==========\n",
    "from deepface import DeepFace\n",
    "\n",
    "# ========== IPython å’Œ Widgets ==========\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62438d83-b6c5-43a3-afe9-30ceebc50d46",
   "metadata": {},
   "source": [
    "## å‡†å¤‡é¡¹ç›®\n",
    "\n",
    "ç›´æ¥è¿è¡Œä»£ç ï¼Œè¿™ä¸ªæ¨¡å—ä¸ç”¨ä¿®æ”¹å‚æ•°ï¼Œä¸ç”¨å…³å¿ƒè¿™é‡Œçš„ä»£ç ç»†èŠ‚ï¼Œé™¤éä½ å¯¹äº¤äº’æ„Ÿå…´è¶£ã€‚\n",
    "\n",
    "æ³¨æ„ï¼šè®°å¾—ç‚¹å‡»**åˆå§‹åŒ–é¡¹ç›®**\n",
    "\n",
    "å½“ä½ çœ‹è§âœ…æ—¶ï¼Œä»£è¡¨é¡¹ç›®å·²ç»å‡†å¤‡å¥½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbde642-16f6-4728-8b7b-05d220644de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºé¡¹ç›®åç§°è¾“å…¥æ¡†\n",
    "project_name_widget = widgets.Text(\n",
    "    value=\"Brad\",\n",
    "    description=\"é¡¹ç›®åç§°:\",\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†åç§°è¾“å…¥æ¡†\n",
    "dataset_name_widget = widgets.Text(\n",
    "    value=\"Brad\",  # \"Brad-512\", \"Anne-512\"\n",
    "    description=\"æ•°æ®é›†:\",\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæäº¤æŒ‰é’®\n",
    "submit_button = widgets.Button(description=\"åˆå§‹åŒ–é¡¹ç›®\")\n",
    "\n",
    "# è¾“å‡ºåŒºåŸŸ\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# å®šä¹‰é¡¹ç›®åˆå§‹åŒ–é€»è¾‘\n",
    "def initialize_project(b):\n",
    "    global project_name, dataset_name, root_dir, main_dir, project_dir, model_path, images_folder, prompts_folder, captions_folder\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        project_name = project_name_widget.value.strip()\n",
    "        dataset_name = dataset_name_widget.value.strip()\n",
    "        \n",
    "        if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
    "            print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„é¡¹ç›®åç§°ã€‚\")\n",
    "        else:\n",
    "            # åˆ›å»ºé¡¹ç›®ç›®å½•\n",
    "            project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
    "            project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
    "            root_dir = \"./\"  # å½“å‰ç›®å½•\n",
    "            main_dir = os.path.join(root_dir, \"SD\")  # ä¸»ç›®å½•\n",
    "            project_dir = os.path.join(main_dir, project_name)  # é¡¹ç›®ç›®å½•\n",
    "            \n",
    "            # ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "            os.makedirs(main_dir, exist_ok=True)\n",
    "            os.makedirs(project_dir, exist_ok=True)\n",
    "            \n",
    "            # å®šä¹‰æ•°æ®é›†å’Œæ¨¡å‹è·¯å¾„\n",
    "            zip_file = os.path.join(\"./\", \"data/14/Datasets.zip\")\n",
    "            model_path = os.path.join(project_dir, \"logs\", \"checkpoint-last\")\n",
    "            images_folder = os.path.join(main_dir, \"Datasets\", dataset_name)\n",
    "            prompts_folder = os.path.join(main_dir, \"Datasets\", \"prompts\")\n",
    "            captions_folder = images_folder\n",
    "\n",
    "            # æ£€æŸ¥å¹¶è§£å‹æ•°æ®é›†\n",
    "            print(\"ğŸ“‚ æ­£åœ¨æ£€æŸ¥å¹¶è§£å‹æ ·ä¾‹æ•°æ®é›†...\")\n",
    "            \n",
    "            if not os.path.exists(zip_file):\n",
    "                print(\"âŒ æœªæ‰¾åˆ°æ•°æ®é›†å‹ç¼©æ–‡ä»¶ Datasets.zipï¼\")\n",
    "                print(\"è¯·ä¸‹è½½æ•°æ®é›†:\\nhttps://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/data/14/Datasets.zip\\nå¹¶æ”¾åœ¨ ./data/14 æ–‡ä»¶å¤¹ä¸‹\")\n",
    "            else:\n",
    "                subprocess.run(f\"unzip -q -o {zip_file} -d {main_dir}\", shell=True)\n",
    "                \n",
    "                # # å…‹éš†æ¨¡å‹æ–‡ä»¶\n",
    "                print(\"ğŸ“‚ é»˜è®¤ä¸å…‹éš†LoRAæ¨¡å‹æ–‡ä»¶è€Œæ˜¯è‡ªå·±è®­ç»ƒï¼Œå› ä¸ºä¸‹è½½éœ€è¦å ç”¨2Gä¸”ç­‰ä»·äºä½ å…ˆè¿›è¡Œ1000ä¸ªstepè®­ç»ƒã€‚\\n\")\n",
    "    \n",
    "                # åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹\n",
    "                os.makedirs(images_folder, exist_ok=True)\n",
    "    \n",
    "                print(f\"âœ… é¡¹ç›® {project_name} å·²å‡†å¤‡å¥½ï¼\")\n",
    "\n",
    "# å°†æäº¤æŒ‰é’®ä¸é¡¹ç›®åˆå§‹åŒ–é€»è¾‘ç»‘å®š\n",
    "submit_button.on_click(initialize_project)\n",
    "\n",
    "# æ˜¾ç¤ºæ§ä»¶\n",
    "display(project_name_widget, dataset_name_widget, submit_button, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44210b9-245d-4640-ae1c-d4a55420c984",
   "metadata": {},
   "source": [
    "## è®¾ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253cc2c-42eb-476d-aa9b-a740c432caf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# è¯·å‹¿éšæ„æ›´æ”¹ä»¥ä¸‹å‚æ•°ï¼Œé™¤éä½ è‡ªå·±çŸ¥é“æ”¹çš„å‚æ•°å¯¹åº”çš„ä»€ä¹ˆï¼Œå¦åˆ™å¯èƒ½å›  GPU å†…å­˜ä¸è¶³å¯¼è‡´è¿›ç¨‹å´©æºƒã€‚\n",
    "output_folder = os.path.join(project_dir, \"logs\")  # å­˜æ”¾ model checkpoints å’Œ validation çš„æ–‡ä»¶å¤¹\n",
    "seed = 1126  # éšæœºæ•°ç§å­\n",
    "train_batch_size = 2  # è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œå³æ¯æ¬¡è®­ç»ƒä¸­å¤„ç†çš„æ ·æœ¬æ•°é‡\n",
    "resolution = 512  # è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡\n",
    "weight_dtype = torch.bfloat16  # æƒé‡æ•°æ®ç±»å‹ï¼Œä½¿ç”¨ bfloat16 ä»¥èŠ‚çœå†…å­˜å¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦\n",
    "snr_gamma = 5  # SNR å‚æ•°ï¼Œç”¨äºä¿¡å™ªæ¯”åŠ æƒæŸå¤±çš„è°ƒèŠ‚ç³»æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ee76f-e838-4296-bf6d-fe175d9e08b8",
   "metadata": {},
   "source": [
    "### Stable Diffusion LoRA çš„å¾®è°ƒå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a3378-e074-4793-ac5e-f8ba7bab2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹è·¯å¾„ï¼Œç”¨äºåŠ è½½æ¨¡å‹è¿›è¡Œå¾®è°ƒ\n",
    "pretrained_model_name_or_path = \"stablediffusionapi/cyberrealistic-41\"\n",
    "\n",
    "# LoRA é…ç½®\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # LoRA çš„ç§©ï¼Œå³ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œå†³å®šäº†å‚æ•°è°ƒæ•´çš„è‡ªç”±åº¦\n",
    "    lora_alpha=16,  # ç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶ LoRA æƒé‡å¯¹æ¨¡å‹çš„å½±å“\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\",  # æŒ‡å®š Text encoder çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŠ•å½±çŸ©é˜µï¼‰\n",
    "        \"to_k\", \"to_q\", \"to_v\", \"to_out.0\"  # æŒ‡å®š UNet çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´ UNet ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰\n",
    "    ],\n",
    "    lora_dropout=0  # LoRA dropout æ¦‚ç‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨ dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d27ec-5425-454a-bb6c-506db30fe69c",
   "metadata": {},
   "source": [
    "### å…¶ä»–è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b00988-3201-4c16-b363-17528f5cadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ ç‡è°ƒåº¦å™¨éƒ¨åˆ†\n",
    "lr_scheduler_name = \"cosine_with_restarts\" # è®¾ç½®å­¦ä¹ ç‡çš„è°ƒåº¦å™¨\n",
    "lr_warmup_steps = 100 # è®¾ç½®å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°\n",
    "\n",
    "# prompt å¤„ç†\n",
    "validation_prompt_name = \"validation_prompt.txt\"\n",
    "validation_prompt_path = os.path.join(prompts_folder, validation_prompt_name)\n",
    "with open(validation_prompt_path, \"r\") as f:\n",
    "    validation_prompt = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e1b82-00a7-4e4e-92cb-8aaf50aa2a60",
   "metadata": {},
   "source": [
    "ç›´æ¥è¿è¡Œä»£ç ï¼Œäº¤äº’å¼ä¿®æ”¹å‚æ•°ï¼Œä¸ç”¨å…³å¿ƒè¿™é‡Œçš„ä»£ç ç»†èŠ‚ï¼Œé™¤éä½ å¯¹äº¤äº’æ„Ÿå…´è¶£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fdd8b-3e5b-4657-bacd-206b087375b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ ç‡éƒ¨åˆ†\n",
    "learning_rate_markdown = Markdown(\"â–¶ï¸ **Learning Rate**<br>å­¦ä¹ ç‡æ˜¯å½±å“ç»“æœæœ€é‡è¦çš„å‚æ•°ï¼Œå½“å‰é»˜è®¤ä¸‰è€…ä¸€è‡´ã€‚<br>å¦‚æœä½ æƒ³æ…¢é€Ÿè®­ç»ƒä¸”æœ‰å¤§é‡å›¾åƒï¼Œæˆ–è€…å¦‚æœ LoRA rankï¼ˆæœ‰äº›æ–‡ç”Ÿå›¾UIå¯èƒ½ç”¨çš„æ˜¯dimï¼‰ å’Œ alpha å€¼è¾ƒé«˜ï¼Œå»ºè®®å°† unet çš„å­¦ä¹ ç‡è°ƒè‡³ 5e-5 æˆ–æ›´ä½ã€‚<br>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®é™…ä¸Šlearning_rateåœ¨è¿™é‡Œä¸èµ·ä½œç”¨ï¼Œæˆ‘ä»¬åªä½¿ç”¨å¯¹åº”æ¨¡å—çš„å­¦ä¹ ç‡ã€‚<br>è¿™é‡Œè¿›è¡Œå±•ç¤ºæ˜¯ä¸ºäº†è®©ä½ ä¹‹ååœ¨ç‚¼ä¸¹ç‚‰ä¸Šèƒ½å¤Ÿæ›´å¥½çš„ç†è§£è®¾ç½®ã€‚\")\n",
    "learning_rate_widget = widgets.FloatText(\n",
    "    value=1e-4,\n",
    "    description='learning_rate:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='300px'),\n",
    "    disabled=True  # ç¦ç”¨æ‰‹åŠ¨ä¿®æ”¹\n",
    ")\n",
    "\n",
    "unet_learning_rate_widget = widgets.FloatText(\n",
    "    value=1e-4,\n",
    "    description='unet_learning_rate:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "text_encoder_learning_rate_widget = widgets.FloatText(\n",
    "    value=1e-4,\n",
    "    description='text_encoder_learning_rate:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# è®­ç»ƒæ­¥éª¤éƒ¨åˆ†\n",
    "max_train_steps_markdown = Markdown(\"â–¶ï¸ **Steps**<br>é€‰æ‹©è®­ç»ƒæ­¥éª¤å’Œæ¯æ¬¡éªŒè¯ç”Ÿæˆçš„å›¾åƒæ•°é‡\")\n",
    "max_train_steps_widget = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=200,\n",
    "    max=2000,\n",
    "    step=100,\n",
    "    description='max_train_steps:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "\n",
    "validation_prompt_num_widget = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='validation_prompt_num:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "\n",
    "validation_step_ratio_widget = widgets.FloatSlider(\n",
    "    value=1,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=0.1,\n",
    "    description='validation_step_ratio:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "\n",
    "validation_prompt_widget = widgets.Text(\n",
    "    value = validation_prompt_path,\n",
    "    description='validation_prompt_path:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='550px'),\n",
    "    disabled=True  # ç¦ç”¨æ‰‹åŠ¨ä¿®æ”¹\n",
    ")\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºåŒºåŸŸæ¥æ˜¾ç¤ºæ‰“å°å†…å®¹\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# è·å–ç”¨æˆ·è¾“å…¥å€¼å¹¶æ‰“å°é…ç½®\n",
    "def on_button_click(b):\n",
    "    global unet_learning_rate, text_encoder_learning_rate, max_train_steps, validation_prompt, validation_prompt_num, validation_step_ratio\n",
    "    with output_area:  # ä½¿ç”¨ Output å°éƒ¨ä»¶æ•è·è¾“å‡º\n",
    "        output_area.clear_output()  # æ¸…é™¤ä¹‹å‰çš„è¾“å‡º\n",
    "        unet_learning_rate = unet_learning_rate_widget.value\n",
    "        text_encoder_learning_rate = text_encoder_learning_rate_widget.value\n",
    "        max_train_steps = max_train_steps_widget.value\n",
    "        validation_prompt_num = validation_prompt_num_widget.value\n",
    "        validation_step_ratio = validation_step_ratio_widget.value\n",
    "\n",
    "        # æ‰“å°é…ç½®\n",
    "        print(f\"UNet å­¦ä¹ ç‡: {unet_learning_rate}\")\n",
    "        print(f\"æ–‡æœ¬ç¼–ç å™¨å­¦ä¹ ç‡: {text_encoder_learning_rate}\")\n",
    "        print(f\"è®­ç»ƒæ­¥éª¤: {max_train_steps}\")\n",
    "        print(f\"éªŒè¯promptæ•°é‡: {validation_prompt_num}\")\n",
    "        print(f\"éªŒè¯æ­¥éª¤æ¯”ä¾‹: {validation_step_ratio}\")\n",
    "        print(f\"ç”¨äºéªŒè¯çš„promptæ–‡ä»¶ä½ç½®: {validation_prompt_path}\")\n",
    "\n",
    "# åˆ›å»ºæäº¤æŒ‰é’®\n",
    "submit_button = widgets.Button(description=\"æäº¤é…ç½®\")\n",
    "submit_button.on_click(on_button_click)\n",
    "\n",
    "# æ˜¾ç¤ºå¸¦æœ‰è¯´æ˜çš„æ‰€æœ‰å°éƒ¨ä»¶\n",
    "display(learning_rate_markdown, learning_rate_widget, unet_learning_rate_widget, text_encoder_learning_rate_widget)\n",
    "display(max_train_steps_markdown, max_train_steps_widget, validation_prompt_num_widget, validation_step_ratio_widget, validation_prompt_widget)\n",
    "\n",
    "# æ˜¾ç¤ºæäº¤æŒ‰é’®\n",
    "display(submit_button)\n",
    "\n",
    "# æ˜¾ç¤ºè¾“å‡ºåŒºåŸŸ\n",
    "display(output_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98bedfc-4764-4a33-b883-cc1e8f3a89dc",
   "metadata": {},
   "source": [
    "## å®šä¹‰ä¸€äº›æœ‰ç”¨çš„å‡½æ•°å’Œç±»\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953edb1b-a08f-4cd0-876d-a0b352ba7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\", \".BMP\"]\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°\n",
    "        transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ\n",
    "        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬\n",
    "        transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡\n",
    "    ]\n",
    ")\n",
    "class Text2ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºæ„å»ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¾®è°ƒæ•°æ®é›†\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
    "        \"\"\"\n",
    "        (2) å‚æ•°:\n",
    "            - images_folder: str, å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„\n",
    "            - captions_folder: str, æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„\n",
    "            - transform: function, å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º torch.tensor\n",
    "            - tokenizer: CLIPTokenizer, å°†æ–‡æœ¬æ ‡æ³¨è½¬ä¸º word ids\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–å›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ‰©å±•åæ‰¾åˆ°æ‰€æœ‰å›¾åƒæ–‡ä»¶\n",
    "        self.image_paths = []\n",
    "        for ext in IMAGE_EXTENSIONS:\n",
    "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
    "        self.image_paths = sorted(self.image_paths)\n",
    "\n",
    "        # é€šè¿‡ DeepFace åº“æå–æ¯å¼ å›¾åƒçš„é¢éƒ¨åµŒå…¥ï¼ˆç‰¹å¾å‘é‡ï¼‰\n",
    "        self.train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in self.image_paths])\n",
    "\n",
    "        # åŠ è½½å¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨ï¼Œä¾æ¬¡è¯»å–æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­çš„å†…å®¹\n",
    "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
    "        captions = []\n",
    "        for p in caption_paths:\n",
    "            with open(p, \"r\") as f:\n",
    "                captions.append(f.readline())\n",
    "\n",
    "        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º word ids\n",
    "        inputs = tokenizer(\n",
    "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        self.input_ids = inputs.input_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        input_id = self.input_ids[idx]\n",
    "        try:\n",
    "            # åŠ è½½å›¾åƒå¹¶å°†å…¶è½¬æ¢ä¸º RGB æ¨¡å¼ï¼Œç„¶ååº”ç”¨æ•°æ®å¢å¼º\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            tensor = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"æ— æ³•åŠ è½½å›¾åƒè·¯å¾„: {img_path}, é”™è¯¯: {e}\")\n",
    "            return None\n",
    "\n",
    "        return tensor, input_id  # è¿”å›å¤„ç†åçš„å›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æ ‡æ³¨\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "def prepare_lora_model(lora_config, pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\", model_path=None, resume=False):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºåŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ Lora å±‚ï¼Œå¹¶å†»ç»“é Lora å‚æ•°ã€‚è¿™åŒ…æ‹¬ Tokenizerã€å™ªå£°è°ƒåº¦å™¨ã€UNetã€VAE å’Œ æ–‡æœ¬ç¼–ç å™¨ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - pretrained_model_name_or_path: str, ä» Hugging Face è·å–çš„æ¨¡å‹åç§°\n",
    "        - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„\n",
    "        - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - è¾“å‡º: Tokenizer, å™ªå£°è°ƒåº¦å™¨, UNet, VAE, æ–‡æœ¬ç¼–ç å™¨\n",
    "\n",
    "    \"\"\"\n",
    "    # åŠ è½½å™ªå£°è°ƒåº¦å™¨ï¼Œç”¨äºæ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å™ªå£°æ·»åŠ å’Œç§»é™¤è¿‡ç¨‹\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "    # åŠ è½½ Tokenizerï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º tokens\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸ºç‰¹å¾å‘é‡\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        subfolder=\"text_encoder\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ VAE æ¨¡å‹ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¤„ç†å›¾åƒçš„æ½œåœ¨è¡¨ç¤º\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"vae\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ UNet æ¨¡å‹ï¼Œè´Ÿè´£å¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒç”Ÿæˆå’Œæ¨ç†è¿‡ç¨‹\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        subfolder=\"unet\"\n",
    "    )\n",
    "\n",
    "    # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet\n",
    "    text_encoder = get_peft_model(text_encoder, lora_config)\n",
    "    unet = get_peft_model(unet, lora_config)\n",
    "    \n",
    "    # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡\n",
    "    text_encoder.print_trainable_parameters()\n",
    "    unet.print_trainable_parameters()\n",
    "    \n",
    "    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡ï¼Œå½“ç„¶ï¼Œä½ å¯ä»¥ä¿®æ”¹ model_path æ¥æŒ‡å®šå…¶ä»–çš„è·¯å¾„\n",
    "    if resume:\n",
    "        if model_path is None or not os.path.exists(model_path):\n",
    "            raise ValueError(\"å½“resumeè®¾ç½®ä¸ºTrueçš„æ—¶å€™ä½ å¿…é¡»æä¾›æœ‰æ•ˆçš„model_path\")\n",
    "            \n",
    "        # åŠ è½½ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹æƒé‡ï¼Œæ³¨æ„è¿™é‡ŒåªåŠ è½½æƒé‡ï¼Œè€Œä¸æ˜¯è¦†ç›–æ•´ä¸ªæ¨¡å‹ï¼Œè¦†ç›–ï¼šmodel = torch.load(...)\n",
    "        text_encoder = torch.load(os.path.join(model_path, \"text_encoder.pt\"))\n",
    "        unet = torch.load(os.path.join(model_path, \"unet.pt\"))\n",
    "    \n",
    "    # å†»ç»“ VAE å‚æ•°\n",
    "    vae.requires_grad_(False)\n",
    "\n",
    "    # ä½¿ç”¨ get_peft_model() åä¼šè‡ªåŠ¨å†»ç»“å…¶ä¸­çš„é LoRA å±‚ï¼Œæ‰€ä»¥å¯ä»¥æ³¨é‡Šæ‰\n",
    "    # # å†»ç»“ UNet å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­çš„é Lora å‚æ•°\n",
    "    # for name, param in unet.named_parameters():\n",
    "    #     if \"lora\" in name:\n",
    "    #         param.requires_grad_(True)\n",
    "    #     else:\n",
    "    #         param.requires_grad_(False)\n",
    "    # for name, param in text_encoder.named_parameters():\n",
    "    #     if \"lora\" in name:\n",
    "    #         param.requires_grad_(True)\n",
    "    #     else:\n",
    "    #         param.requires_grad_(False)\n",
    "\n",
    "    # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹\n",
    "    unet.to(DEVICE, dtype=weight_dtype)\n",
    "    vae.to(DEVICE, dtype=weight_dtype)\n",
    "    text_encoder.to(DEVICE, dtype=weight_dtype)\n",
    "    \n",
    "    return tokenizer, noise_scheduler, unet, vae, text_encoder\n",
    "\n",
    "\n",
    "def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°åˆ†åˆ«è®¾ç½®ä¼˜åŒ–å™¨ï¼Œå¹¶æŒ‡å®šä¸åŒçš„å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - unet: UNet2DConditionModel, Hugging Face çš„ UNet æ¨¡å‹\n",
    "        - text_encoder: CLIPTextModel, Hugging Face çš„æ–‡æœ¬ç¼–ç å™¨\n",
    "        - unet_learning_rate: float, UNet çš„å­¦ä¹ ç‡\n",
    "        - text_encoder_learning_rate: float, æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - è¾“å‡º: ä¼˜åŒ–å™¨ Optimizer\n",
    "    \"\"\"\n",
    "    # ç­›é€‰å‡º UNet ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°\n",
    "    unet_lora_layers = [p for p in unet.parameters() if p.requires_grad]\n",
    "    \n",
    "    # ç­›é€‰å‡ºæ–‡æœ¬ç¼–ç å™¨ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°\n",
    "    text_encoder_lora_layers = [p for p in text_encoder.parameters() if p.requires_grad]\n",
    "    \n",
    "    # å°†éœ€è¦è®­ç»ƒçš„å‚æ•°åˆ†ç»„å¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡\n",
    "    trainable_params = [\n",
    "        {\"params\": unet_lora_layers, \"lr\": unet_learning_rate},\n",
    "        {\"params\": text_encoder_lora_layers, \"lr\": text_encoder_learning_rate}\n",
    "    ]\n",
    "    \n",
    "    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨\n",
    "    optimizer = torch.optim.AdamW(trainable_params)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºåŠ è½½ç»™å®šè·¯å¾„ä¸­çš„ UNet å’Œæ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ï¼Œç”Ÿæˆå›¾åƒå¹¶è®¡ç®—é¢éƒ¨ç›¸ä¼¼æ€§ã€CLIP åˆ†æ•°å’Œæ— é¢éƒ¨å›¾åƒçš„æ•°é‡ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - pretrained_model_name_or_path: str, Hugging Face çš„æ¨¡å‹åç§°\n",
    "        - weight_dtype: torch.dtype, æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹\n",
    "        - seed: int, éšæœºæ•°ç§å­\n",
    "        - unet_path: str, UNet æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "        - text_encoder_path: str, æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "        - validation_prompt: list, å­˜å‚¨éªŒè¯æ–‡æœ¬çš„å­—ç¬¦ä¸²åˆ—è¡¨\n",
    "        - output_folder: str, ä¿å­˜ç”Ÿæˆå›¾åƒçš„æ–‡ä»¶å¤¹\n",
    "        - train_emb: tensor, è®­ç»ƒå›¾åƒçš„é¢éƒ¨ç‰¹å¾\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - face_score: float, é¢éƒ¨ç›¸ä¼¼æ€§è¯„åˆ†\n",
    "        - clip_score: float, CLIP åˆ†æ•°\n",
    "        - mis: int, æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾åƒæ•°é‡\n",
    "\n",
    "    \"\"\"\n",
    "    # åŠ è½½ DiffusionPipeline å¹¶è®¾ç½® UNet å’Œæ–‡æœ¬ç¼–ç å™¨\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        safety_checker=None,\n",
    "    )\n",
    "    pipeline.unet = torch.load(unet_path)  # åŠ è½½ UNet æ¨¡å‹\n",
    "    pipeline.text_encoder = torch.load(text_encoder_path)  # åŠ è½½æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹\n",
    "    pipeline = pipeline.to(DEVICE)\n",
    "\n",
    "    # åŠ è½½ CLIP æ¨¡å‹ç”¨äºè®¡ç®—å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼æ€§åˆ†æ•°\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    clip_model = AutoModel.from_pretrained(clip_model_name).to(DEVICE)\n",
    "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
    "    \n",
    "    # æ¨ç†ç”Ÿæˆå›¾åƒ\n",
    "    with torch.no_grad():\n",
    "        generator = torch.Generator(device=DEVICE)\n",
    "        generator.manual_seed(seed)\n",
    "        face_score = 0\n",
    "        clip_score = 0\n",
    "        mis = 0  # è®°å½•æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾åƒæ•°é‡\n",
    "        print(\"æ­£åœ¨ç”ŸæˆéªŒè¯å›¾åƒ......\")\n",
    "        images = []\n",
    "        for i in range(0, len(validation_prompt), 4):\n",
    "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
    "        \n",
    "        # è®¡ç®—é¢éƒ¨ç›¸ä¼¼æ€§å’Œ CLIP åˆ†æ•°\n",
    "        print(\"æ­£åœ¨è®¡ç®—éªŒè¯åˆ†æ•°......\")\n",
    "        valid_emb = []\n",
    "        for i, image in enumerate(tqdm(images)):\n",
    "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
    "            image.save(save_file)  # å°†ç”Ÿæˆçš„å›¾åƒä¿å­˜åˆ°æŒ‡å®šç›®å½•\n",
    "            opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # ä½¿ç”¨ DeepFace æ£€æµ‹é¢éƒ¨ç‰¹å¾\n",
    "            emb = DeepFace.represent(\n",
    "                opencvImage,\n",
    "                detector_backend=\"ssd\",\n",
    "                model_name=\"GhostFaceNet\",\n",
    "                enforce_detection=False,\n",
    "            )\n",
    "            # å¦‚æœæ— æ³•æ£€æµ‹åˆ°é¢éƒ¨ï¼Œè®¡å…¥ mis\n",
    "            if emb == [] or emb[0]['face_confidence'] == 0:\n",
    "                mis += 1\n",
    "                continue\n",
    "\n",
    "            # è®¡ç®— CLIP åˆ†æ•°ï¼Œè¡¡é‡å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼åº¦\n",
    "            emb = emb[0]\n",
    "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outputs = clip_model(**inputs)\n",
    "            sim = outputs.logits_per_image  # è·å–å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼æ€§å¾—åˆ†\n",
    "            clip_score += sim.item()\n",
    "            valid_emb.append(emb['embedding'])\n",
    "\n",
    "        # å¦‚æœç”Ÿæˆçš„å›¾ç‰‡éƒ½æ²¡æœ‰äººè„¸ï¼Œç›´æ¥ 0 åˆ†\n",
    "        if len(valid_emb) == 0:\n",
    "            return 0, 0, mis\n",
    "        \n",
    "        # è®¡ç®—é¢éƒ¨ç›¸ä¼¼æ€§åˆ†æ•°\n",
    "        valid_emb = torch.tensor(valid_emb)\n",
    "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).cuda()\n",
    "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).cuda()\n",
    "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
    "        clip_score /= len(validation_prompt) - mis\n",
    "    \n",
    "    return face_score, clip_score, mis   # è¿”å›é¢éƒ¨ç›¸ä¼¼æ€§ã€CLIP åˆ†æ•°å’Œæ— é¢éƒ¨å›¾åƒçš„æ•°é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b834a7-3f78-4d6f-9432-17444e7c29bd",
   "metadata": {},
   "source": [
    "## å‡†å¤‡å¾®è°ƒæ‰€éœ€çš„æ¨¡å‹å’Œä¼˜åŒ–å™¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b8290-6b15-4171-bc1b-403de3abe81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å†³å®šæ˜¯å¦ç»§ç»­ä¹‹å‰çš„è®­ç»ƒ\n",
    "resume = False\n",
    "\n",
    "# å‡†å¤‡å¾®è°ƒæ‰€éœ€çš„ tokenizer, å™ªå£°è°ƒåº¦å™¨, UNet, VAE å’Œæ–‡æœ¬ç¼–ç å™¨\n",
    "tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(lora_config, pretrained_model_name_or_path, model_path, resume)\n",
    "\n",
    "# å‡†å¤‡ä¼˜åŒ–å™¨ï¼Œç”¨äºæ›´æ–° UNet å’Œ æ–‡æœ¬ç¼–ç å™¨ä¸­ç»è¿‡ LoRA å±‚è°ƒæ•´çš„å¯è®­ç»ƒå‚æ•°\n",
    "optimizer = prepare_optimizer(unet, text_encoder, unet_learning_rate, text_encoder_learning_rate)\n",
    "\n",
    "# è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ§åˆ¶å­¦ä¹ ç‡çš„å˜åŒ–\n",
    "# get_scheduler æ ¹æ®æŒ‡å®šçš„è°ƒåº¦å™¨ç±»å‹ï¼ˆä¾‹å¦‚çº¿æ€§ã€ä½™å¼¦ç­‰ï¼‰ï¼Œä¸ºä¼˜åŒ–å™¨æä¾›ä¸€ä¸ªåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡çš„ç­–ç•¥\n",
    "# num_warmup_steps: åœ¨åˆå§‹é˜¶æ®µè¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°\n",
    "# num_training_steps: æ€»çš„è®­ç»ƒæ­¥æ•°\n",
    "# num_cycles: ç”¨äºä½™å¼¦è°ƒåº¦å™¨ï¼ŒæŒ‡å®šä½™å¼¦è¡°å‡å‘¨æœŸçš„æ•°é‡\n",
    "lr_scheduler = get_scheduler(\n",
    "    lr_scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    "    num_cycles=3\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ•°æ®é›†\n",
    "# Text2ImageDataset ä¼šæ ¹æ®æŒ‡å®šçš„å›¾åƒæ–‡ä»¶å¤¹å’Œæ–‡æœ¬æ ‡æ³¨æ–‡ä»¶å¤¹ï¼ŒåŠ è½½å¹¶å¤„ç†æ•°æ®\n",
    "# å…·ä½“ï¼šä½¿ç”¨æŒ‡å®šçš„ transform æ¥é¢„å¤„ç†å›¾åƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨ tokenizer æ¥å°†æ–‡æœ¬è½¬æ¢ä¸º tokens\n",
    "dataset = Text2ImageDataset(\n",
    "    images_folder=images_folder,\n",
    "    captions_folder=captions_folder,\n",
    "    transform=train_transform,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# è®¾ç½®æ•°æ®åŠ è½½å™¨ï¼Œè´Ÿè´£å°†æ•°æ®é›†åˆ†æ‰¹æ¬¡åŠ è½½åˆ°æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒ\n",
    "# collate_fn: è‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œå°†æ•°æ®é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨åˆå¹¶æˆæ‰¹æ¬¡\n",
    "def collate_fn(examples):\n",
    "    pixel_values = []\n",
    "    input_ids = []\n",
    "    \n",
    "    # å°†æ¯ä¸ªæ ·æœ¬ä¸­çš„å›¾åƒ tensor å’Œæ–‡æœ¬æ ‡æ³¨ input_ids åˆ†åˆ«æ”¶é›†åˆ°åˆ—è¡¨ä¸­\n",
    "    for tensor, input_id in examples:\n",
    "        pixel_values.append(tensor)  # å°†å›¾åƒå¼ é‡æ”¶é›†åˆ° pixel_values åˆ—è¡¨ä¸­\n",
    "        input_ids.append(input_id)  # å°†æ–‡æœ¬æ ‡æ³¨æ”¶é›†åˆ° input_ids åˆ—è¡¨ä¸­\n",
    "    \n",
    "    # å°†åˆ—è¡¨ä¸­çš„æ‰€æœ‰å›¾åƒå¼ é‡å †å æˆä¸€ä¸ªå¤§çš„ batch tensor\n",
    "    pixel_values = torch.stack(pixel_values, dim=0).float()  # å½¢çŠ¶ä¸º (batch_size, channels, height, width)\n",
    "    input_ids = torch.stack(input_ids, dim=0)  # å½¢çŠ¶ä¸º (batch_size, max_seq_length)\n",
    "    \n",
    "    # è¿”å›æ‰¹å¤„ç†åçš„å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "# ä½¿ç”¨ PyTorch DataLoader åŠ è½½æ•°æ®é›†ï¼Œè®¾ç½®æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°å’Œçº¿ç¨‹æ•°\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    shuffle=True,  # æ‰“ä¹±æ•°æ®\n",
    "    collate_fn=collate_fn,  # è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°\n",
    "    batch_size=train_batch_size,  # æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡\n",
    "    num_workers=8,  # ä½¿ç”¨ 8 ä¸ªçº¿ç¨‹å¹¶è¡ŒåŠ è½½æ•°æ®ï¼ŒåŠ å¿«æ•°æ®åŠ è½½é€Ÿåº¦\n",
    ")\n",
    "\n",
    "print(\"å‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27875-b3b2-4b03-a554-4675667e4415",
   "metadata": {},
   "source": [
    "## å¼€å§‹å¾®è°ƒ\n",
    "\n",
    "æœ€åæ˜¯å¼€å§‹è¿›è¡Œå¾®è°ƒçš„éƒ¨åˆ†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836a890-8e06-4aed-bd11-71df8fd5bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹å¾®è°ƒ Stable Diffusion æ¨¡å‹\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
    "progress_bar = tqdm(\n",
    "    range(0, max_train_steps),\n",
    "    initial=0,\n",
    "    desc=\"æ­¥éª¤\",\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–å…¨å±€æ­¥æ•°å’Œå…¶ä»–æ§åˆ¶å˜é‡\n",
    "global_step = 0\n",
    "num_epochs = math.ceil(max_train_steps / len(train_dataloader))  # æ ¹æ®æœ€å¤§è®­ç»ƒæ­¥æ•°è®¡ç®—æ€»å…±çš„ epoch æ•°é‡\n",
    "validation_step = int(max_train_steps * validation_step_ratio)  # æ ¹æ®è®¾ç½®çš„éªŒè¯æ­¥æ•°æ¯”ä¾‹è®¡ç®—æ¯éš”å¤šå°‘æ­¥è¿›è¡ŒéªŒè¯\n",
    "best_face_score = float(\"inf\")  # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§ï¼Œå­˜å‚¨æœ€ä½³é¢éƒ¨ç›¸ä¼¼åº¦åˆ†æ•°\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯ï¼Œéå†æ‰€æœ‰çš„ epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # å°†æ¨¡å‹åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
    "    unet.train()\n",
    "    text_encoder.train()\n",
    "    \n",
    "    # éå†æ•°æ®åŠ è½½å™¨ï¼Œè¿›è¡Œé€æ­¥è®­ç»ƒ\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if global_step >= max_train_steps:\n",
    "            break  # å½“è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•°æ—¶ï¼Œç»ˆæ­¢è®­ç»ƒ\n",
    "        \n",
    "        # ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰ï¼Œé€šè¿‡ VAE å¯¹å›¾åƒè¿›è¡Œç¼–ç \n",
    "        latents = vae.encode(batch[\"pixel_values\"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor  # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨ç©ºé—´\n",
    "\n",
    "        # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ\n",
    "        noise = torch.randn_like(latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°\n",
    "        bsz = latents.shape[0]  # æ‰¹æ¬¡å¤§å°\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()  # éšæœºé€‰æ‹©æ—¶é—´æ­¥\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)  # æ·»åŠ å™ªå£°åˆ°æ½œåœ¨è¡¨ç¤º\n",
    "\n",
    "        # è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º\n",
    "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(latents.device), return_dict=False)[0]\n",
    "        \n",
    "        # è®¡ç®—ç›®æ ‡å€¼ï¼ŒåŸºäºé¢„æµ‹ç±»å‹ï¼ˆepsilon æˆ– v_predictionï¼‰\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise  # é¢„æµ‹å™ªå£°\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡\n",
    "\n",
    "        # UNet æ¨¡å‹é¢„æµ‹ï¼Œè¾“å…¥å™ªå£°æ½œåœ¨ç©ºé—´ã€æ—¶é—´æ­¥å’Œæ–‡æœ¬åµŒå…¥\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±ï¼ŒåŸºäºæ˜¯å¦è®¾ç½®äº† snr_gammaï¼Œè¿™ä¸ªå‚æ•°åœ¨æœ€å¼€å§‹çš„éƒ¨åˆ†è¿›è¡Œäº†è®¾ç½®\n",
    "        if not snr_gamma:\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")  # ä½¿ç”¨å‡æ–¹è¯¯å·® (MSE) ä½œä¸ºæŸå¤±\n",
    "        else:\n",
    "            # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±\n",
    "            snr = compute_snr(noise_scheduler, timesteps)\n",
    "            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                mse_loss_weights = mse_loss_weights / snr\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                mse_loss_weights = mse_loss_weights / (snr + 1)\n",
    "            \n",
    "            # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()  # è®¡ç®—æ¢¯åº¦\n",
    "        optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "        lr_scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡\n",
    "        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\n",
    "        progress_bar.update(1)  # æ›´æ–°è¿›åº¦æ¡\n",
    "        global_step += 1  # æ›´æ–°å…¨å±€æ­¥æ•°\n",
    "\n",
    "\n",
    "        # éªŒè¯æ¨¡å‹æ€§èƒ½\n",
    "        if global_step % validation_step == 0 or global_step == max_train_steps:\n",
    "            # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹ï¼ˆåŒ…å« UNet å’Œ æ–‡æœ¬ç¼–ç å™¨çš„æ¨¡å‹å‚æ•°ï¼‰\n",
    "            save_path = os.path.join(output_folder, f\"checkpoint-last\")\n",
    "            unet_path = os.path.join(save_path, \"unet.pt\")\n",
    "            text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
    "            print(f\"æ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {save_path} ......\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            torch.save(unet, unet_path)\n",
    "            torch.save(text_encoder, text_encoder_path)\n",
    "            \n",
    "            # è¿›è¡ŒéªŒè¯\n",
    "            save_path = os.path.join(output_folder, f\"checkpoint-{global_step}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            face_score, clip_score, mis = evaluate(\n",
    "                pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "                weight_dtype=weight_dtype,\n",
    "                seed=seed,\n",
    "                unet_path=unet_path,\n",
    "                text_encoder_path=text_encoder_path,\n",
    "                validation_prompt=validation_prompt[:validation_prompt_num],  # éªŒè¯æç¤ºæ–‡æœ¬\n",
    "                output_folder=save_path,  # ä¿å­˜éªŒè¯ç»“æœçš„æ–‡ä»¶å¤¹\n",
    "                train_emb=dataset.train_emb  # è®­ç»ƒæ•°æ®ä¸­çš„é¢éƒ¨ç‰¹å¾åµŒå…¥\n",
    "            )\n",
    "            print(\"æ­¥éª¤:\", global_step, \"é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ†:\", face_score, \"CLIPè¯„åˆ†:\", clip_score, \"æ— é¢éƒ¨å›¾åƒæ•°é‡:\", mis)\n",
    "            \n",
    "            # å¦‚æœå½“å‰é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ†ä¼˜äºä¹‹å‰çš„æœ€ä½³è®°å½•ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if face_score < best_face_score:\n",
    "                best_face_score = face_score\n",
    "                save_path = os.path.join(output_folder, f\"checkpoint-best\")\n",
    "                unet_path = os.path.join(save_path, \"unet.pt\")\n",
    "                text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                torch.save(unet, unet_path)  # ä¿å­˜æœ€ä½³çš„ UNet æ¨¡å‹\n",
    "                torch.save(text_encoder, text_encoder_path)  # ä¿å­˜æœ€ä½³çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹\n",
    "\n",
    "print(\"å¾®è°ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf90ba1-3511-425c-b278-a3c6da887ed5",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆå›¾åƒå’Œæµ‹è¯•\n",
    "\n",
    "å¾®è°ƒè¿‡ç¨‹å·²ç»å®Œæˆã€‚æ¥ä¸‹æ¥å°†æµ‹è¯•æ¨¡å‹ã€‚\n",
    "\n",
    "é¦–å…ˆåŠ è½½ä¹‹å‰ä¿å­˜çš„å¾®è°ƒæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œå¹¶è®¡ç®—é¢éƒ¨ç›¸ä¼¼åº¦ã€CLIP è¯„åˆ†ä»¥åŠæ²¡æœ‰é¢éƒ¨çš„å›¾åƒæ•°é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb883783-f35b-46b7-9279-2cf96fdd5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç”¨äºæ¨ç†çš„ checkpoint è·¯å¾„\n",
    "checkpoint_path = os.path.join(output_folder, f\"checkpoint-last\")  # è®¾ç½®æ¨ç†æ—¶ä½¿ç”¨æœ€åä¿å­˜çš„ checkpoint\n",
    "\n",
    "# è®¾ç½®è·¯å¾„\n",
    "unet_path = os.path.join(checkpoint_path, \"unet.pt\")\n",
    "text_encoder_path = os.path.join(checkpoint_path, \"text_encoder.pt\")\n",
    "inference_path = os.path.join(project_dir, \"inference\")\n",
    "os.makedirs(inference_path, exist_ok=True)\n",
    "\n",
    "# è·å–è®­ç»ƒå›¾åƒçš„è·¯å¾„\n",
    "train_image_paths = []  # åˆå§‹åŒ–å­˜å‚¨æ‰€æœ‰è®­ç»ƒå›¾åƒè·¯å¾„çš„åˆ—è¡¨\n",
    "for ext in IMAGE_EXTENSIONS:\n",
    "    # é€šè¿‡æ‰©å±•åéå†å›¾åƒæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒ\n",
    "    train_image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
    "train_image_paths = sorted(train_image_paths)  # å¯¹å›¾åƒè·¯å¾„è¿›è¡Œæ’åº\n",
    "\n",
    "# åˆå§‹åŒ–åˆ—è¡¨å­˜å‚¨é¢éƒ¨ç‰¹å¾åµŒå…¥\n",
    "train_emb_list = []\n",
    "\n",
    "# éå†è®­ç»ƒå›¾åƒè·¯å¾„å¹¶æå–é¢éƒ¨ç‰¹å¾åµŒå…¥\n",
    "for img_path in train_image_paths:\n",
    "    # ä½¿ç”¨ DeepFace ä»æ¯å¼ å›¾åƒä¸­æå–é¢éƒ¨ç‰¹å¾\n",
    "    face_representation = DeepFace.represent(\n",
    "        img_path, \n",
    "        detector_backend=\"ssd\",  # ä½¿ç”¨ ssd æ£€æµ‹å™¨\n",
    "        model_name=\"GhostFaceNet\",  # ä½¿ç”¨ GhostFaceNet æ¨¡å‹\n",
    "        enforce_detection=False  # å…³é—­å¼ºåˆ¶æ£€æµ‹\n",
    "    )\n",
    "    \n",
    "    # å¦‚æœæå–åˆ°çš„ç‰¹å¾éç©ºï¼Œåˆ™è·å–åµŒå…¥å‘é‡\n",
    "    if face_representation:\n",
    "        embedding = face_representation[0]['embedding']\n",
    "        train_emb_list.append(embedding)\n",
    "\n",
    "# å°†æ‰€æœ‰é¢éƒ¨åµŒå…¥è½¬æ¢ä¸º tensor\n",
    "train_emb = torch.tensor(train_emb_list)\n",
    "\n",
    "# è°ƒç”¨ evaluate å‡½æ•°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½\n",
    "face_score, clip_score, mis = evaluate(\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,  # é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„\n",
    "    weight_dtype=weight_dtype,  # æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹\n",
    "    seed=seed,  # éšæœºæ•°ç§å­ï¼Œç¡®ä¿å¯é‡å¤çš„æ¨ç†ç»“æœ\n",
    "    unet_path=unet_path,  # è®­ç»ƒä¿å­˜çš„ UNet æ¨¡å‹çš„æƒé‡æ–‡ä»¶è·¯å¾„\n",
    "    text_encoder_path=text_encoder_path,  # è®­ç»ƒä¿å­˜çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹çš„æƒé‡æ–‡ä»¶è·¯å¾„\n",
    "    validation_prompt=validation_prompt,  # éªŒè¯æ—¶ä½¿ç”¨çš„æ–‡æœ¬æç¤ºï¼Œè¾“å…¥æ¨¡å‹ç”Ÿæˆå›¾åƒ\n",
    "    output_folder=inference_path,  # æ¨ç†æ—¶ç”Ÿæˆçš„å›¾åƒä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶å¤¹\n",
    "    train_emb=train_emb  # ä½¿ç”¨çš„è®­ç»ƒå›¾åƒçš„é¢éƒ¨ç‰¹å¾åµŒå…¥ï¼Œç”¨äºè¯„ä¼°é¢éƒ¨ç›¸ä¼¼åº¦\n",
    ")\n",
    "\n",
    "# æ‰“å°è¯„ä¼°ç»“æœ\n",
    "print(f\"é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ† (å¹³å‡æ¬§æ°è·ç¦»): {face_score:.4f} (è¶Šä½è¶Šå¥½ï¼Œè¡¨ç¤ºç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒæ›´ç›¸ä¼¼)\")\n",
    "print(f\"CLIP è¯„åˆ† (å¹³å‡ç›¸ä¼¼åº¦): {clip_score:.4f} (è¶Šé«˜è¶Šå¥½ï¼Œè¡¨ç¤ºç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„ç›¸å…³æ€§æ›´å¼º)\")\n",
    "print(f\"æ— é¢éƒ¨å›¾åƒæ•°é‡: {mis} (æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„ç”Ÿæˆå›¾åƒæ•°é‡)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cea9f-314a-49ed-9b7c-20a590b298bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
