{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3e66d1-137d-4acd-8b3c-1c8047c5fd9a",
   "metadata": {},
   "source": [
    "# b. å°è¯•ä½¿ç”¨ LoRA å¾®è°ƒ Stable Diffusion æ¨¡å‹ï¼ˆæ–‡ç”Ÿå›¾ï¼‰- ç²¾ç®€ç‰ˆ\n",
    "\n",
    "> æŒ‡å¯¼æ–‡ç« ï¼š[16. ç”¨ LoRA å¾®è°ƒ Stable Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡ AI ç»˜ç”»](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/16.%20ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡%20AI%20ç»˜ç”».md)\n",
    "\n",
    "å½“å‰ç‰ˆæœ¬åªä¿ç•™äº†æ ¸å¿ƒä»£ç ï¼Œå¹¶é‡æ–°ç»„ç»‡äº†å‡½æ•°çš„é¡ºåºï¼Œè¿™æ¬¡ä½ å¯ä»¥ä¸¤ä¸ªç‰ˆæœ¬éƒ½ç²—ç•¥è¿è¡Œï¼Œå¹¶é€‰æ‹©å…¶ä¸­ä¸€ä¸ªç‰ˆæœ¬æ·±å…¥å­¦ä¹ ã€‚ä¹Ÿå› ä¸ºç²¾ç®€ç‰ˆä¿®æ”¹çš„ä¸œè¥¿æ¯”è¾ƒå¤šï¼Œæ‰€ä»¥ä¸å»ºè®®åŒæ—¶ä»£å…¥ä¸¤ä¸ªç‰ˆæœ¬ã€‚æœ€ç»ˆè®­ç»ƒæ•ˆæœä¸€è‡´ã€‚\n",
    "\n",
    "æ³¨æ„ï¼Œ[ç‰ˆæœ¬ a](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/14a.%20å°è¯•ä½¿ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusion%20æ¨¡å‹.ipynb) å¯¹äºä¿å­˜å’ŒåŠ è½½ä½¿ç”¨çš„æ–¹æ³•ä¸ç‰ˆæœ¬ b ä¸åŒï¼Œç²¾ç®€ç‰ˆä½¿ç”¨ PEFT åº“ç›´æ¥å®Œæˆï¼Œä¸ä¹‹å‰çš„çŸ¥è¯†[ã€Š14. PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRAã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/14.%20PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨%20LoRA.md)å¯¹é½ã€‚\n",
    "\n",
    "è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç®€å•çš„ [ğŸ¡ SD LoRA è„šæœ¬](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/sd_lora.py)ä¾›ä½ å°è¯•ï¼Œè¯¦è§ï¼š[CodePlayground](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/README.md#å½“å‰çš„ç©å…·)ï¼Œç‚¹å‡» `â–º` æˆ–å¯¹åº”çš„æ–‡æœ¬å±•å¼€ã€‚\n",
    "\n",
    "åœ¨çº¿é“¾æ¥ï¼ˆç²¾ç®€ç‰ˆï¼‰ï¼š[Kaggle](https://www.kaggle.com/code/aidemos/14b-lora-stable-diffusion) | [Colab](https://colab.research.google.com/drive/1idmnaQZwRhjUPw7ToEXlVo82Mihfl_aA?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33364619-4768-4837-ad1c-2b72edc0f957",
   "metadata": {},
   "source": [
    "## å®‰è£…å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e7d63-f337-4b40-840e-77678f29ef1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "!pip install fairscale\n",
    "!pip install transformers\n",
    "!pip install requests\n",
    "!pip install accelerate\n",
    "!pip install diffusers\n",
    "!pip install einop\n",
    "!pip install safetensors\n",
    "!pip install voluptuous\n",
    "!pip install jax\n",
    "!pip install jaxlib\n",
    "!pip install peft\n",
    "!pip install deepface==0.0.90\n",
    "!pip install tensorflow==2.9.0  # ä¸ºäº†é¿å…æœ€åè¯„ä¼°é˜¶æ®µä½¿ç”¨deepfaceæ—¶çš„é”™è¯¯ï¼Œè¿™é‡Œé€‰æ‹©é™çº§ç‰ˆæœ¬\n",
    "!pip install keras\n",
    "!pip install opencv-python\n",
    "#!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09608f50-24e2-4860-b68a-320b25207f98",
   "metadata": {},
   "source": [
    "## å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e43e2c-c6f9-4981-8ac8-a8f89de845e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ ‡å‡†åº“æ¨¡å— ==========\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# ========== ç¬¬ä¸‰æ–¹åº“ ==========\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ========== æ·±åº¦å­¦ä¹ ç›¸å…³åº“ ==========\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPProcessor\n",
    "\n",
    "# Diffusers (Hugging Face)\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    DiffusionPipeline\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import compute_snr\n",
    "\n",
    "# ========== LoRA æ¨¡å‹åº“ ==========\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# ========== é¢éƒ¨æ£€æµ‹åº“ ==========\n",
    "from deepface import DeepFace\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf9948-94b5-463b-a8e7-f48d999762e3",
   "metadata": {},
   "source": [
    "## å‡†å¤‡é¡¹ç›®\n",
    "\n",
    "### è®¾ç½®è·¯å¾„\n",
    "è¿™é‡Œçš„å‚æ•°ä¸éœ€è¦ä¿®æ”¹ï¼Œå¦‚æœä½ æƒ³è‡ªå®šä¹‰æ–‡ä»¶å¤¹ï¼Œåç»­æˆ‘ä¼šå‡ºä¸€ä¸ªæ›´é€šç”¨çš„è„šæœ¬ä¾›ä½ å­¦ä¹ ã€‚\n",
    "\n",
    "å½“ä½ çœ‹è§âœ…æ—¶ï¼Œä»£è¡¨é¡¹ç›®å·²ç»å‡†å¤‡å¥½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae237bf-6d30-4efa-b6a9-7a6d22827238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¹ç›®åç§°å’Œæ•°æ®é›†åç§°\n",
    "project_name = \"Brad\"\n",
    "dataset_name = \"Brad\"\n",
    "\n",
    "# æ ¹ç›®å½•å’Œä¸»è¦ç›®å½•\n",
    "root_dir = \"./\"  # å½“å‰ç›®å½•\n",
    "main_dir = os.path.join(root_dir, \"SD\")  # ä¸»ç›®å½•\n",
    "\n",
    "# é¡¹ç›®ç›®å½•\n",
    "project_dir = os.path.join(main_dir, project_name)  # é¡¹ç›®ç›®å½•\n",
    "\n",
    "# æ•°æ®é›†å’Œæ¨¡å‹è·¯å¾„\n",
    "images_folder = os.path.join(main_dir, \"Datasets\", dataset_name)\n",
    "prompts_folder = os.path.join(main_dir, \"Datasets\", \"prompts\")\n",
    "captions_folder = images_folder  # ä¸åŸå§‹ä»£ç ä¸€è‡´\n",
    "output_folder = os.path.join(project_dir, \"logs\")  # å­˜æ”¾ model checkpoints å’Œ validation çš„æ–‡ä»¶å¤¹\n",
    "\n",
    "# prompt æ–‡ä»¶è·¯å¾„\n",
    "validation_prompt_name = \"validation_prompt.txt\"\n",
    "validation_prompt_path = os.path.join(prompts_folder, validation_prompt_name)\n",
    "\n",
    "# æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "model_path = os.path.join(project_dir, \"logs\", \"checkpoint-last\")\n",
    "\n",
    "# å…¶ä»–è·¯å¾„è®¾ç½®\n",
    "zip_file = os.path.join(\"./\", \"data/14/Datasets.zip\")\n",
    "inference_path = os.path.join(project_dir, \"inference\")  # ä¿å­˜æ¨ç†ç»“æœçš„æ–‡ä»¶å¤¹\n",
    "\n",
    "os.makedirs(images_folder, exist_ok=True)\n",
    "os.makedirs(prompts_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(inference_path, exist_ok=True)\n",
    "\n",
    "# æ£€æŸ¥å¹¶è§£å‹æ•°æ®é›†\n",
    "print(\"ğŸ“‚ æ­£åœ¨æ£€æŸ¥å¹¶è§£å‹æ ·ä¾‹æ•°æ®é›†...\")\n",
    "\n",
    "if not os.path.exists(zip_file):\n",
    "    print(\"âŒ æœªæ‰¾åˆ°æ•°æ®é›†å‹ç¼©æ–‡ä»¶ Datasets.zipï¼\")\n",
    "    print(\"è¯·ä¸‹è½½æ•°æ®é›†:\\nhttps://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/data/14/Datasets.zip\\nå¹¶æ”¾åœ¨ ./data/14 æ–‡ä»¶å¤¹ä¸‹\")\n",
    "else:\n",
    "    subprocess.run(f\"unzip -q -o {zip_file} -d {main_dir}\", shell=True)\n",
    "    print(f\"âœ… é¡¹ç›® {project_name} å·²å‡†å¤‡å¥½ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdaa70a-5e3a-4b67-8743-3780d360d1a4",
   "metadata": {},
   "source": [
    "## å®šä¹‰ä¸€äº›æœ‰ç”¨çš„å‡½æ•°å’Œç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0040ab7-0da7-4458-b627-af55dd701e67",
   "metadata": {},
   "source": [
    "### æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7d433-bb97-4cc1-a3b6-7c4c2ea22832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›¾ç‰‡åç¼€ï¼Œä¸ç”¨å…³æ³¨\n",
    "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\", \".BMP\"]\n",
    "\n",
    "class Text2ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºæ„å»ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¾®è°ƒæ•°æ®é›†\n",
    "    \"\"\"\n",
    "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
    "        \"\"\"\n",
    "        (2) å‚æ•°:\n",
    "            - images_folder: str, å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„\n",
    "            - captions_folder: str, æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„\n",
    "            - transform: function, å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º torch.Tensor\n",
    "            - tokenizer: CLIPTokenizer, å°†æ–‡æœ¬æ ‡æ³¨è½¬ä¸º word ids\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–å›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ‰©å±•åæ‰¾åˆ°æ‰€æœ‰å›¾åƒæ–‡ä»¶\n",
    "        self.image_paths = []\n",
    "        for ext in IMAGE_EXTENSIONS:\n",
    "            self.image_paths.extend(glob.glob(os.path.join(images_folder, f\"*{ext}\")))\n",
    "        self.image_paths = sorted(self.image_paths)\n",
    "\n",
    "        # åŠ è½½å¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨ï¼Œä¾æ¬¡è¯»å–æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­çš„å†…å®¹\n",
    "        caption_paths = sorted(glob.glob(os.path.join(captions_folder, \"*.txt\")))\n",
    "        captions = []\n",
    "        for p in caption_paths:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                captions.append(f.readline().strip())\n",
    "\n",
    "        # ç¡®ä¿å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸€è‡´\n",
    "        if len(captions) != len(self.image_paths):\n",
    "            raise ValueError(\"å›¾åƒæ•°é‡ä¸æ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ã€‚\")\n",
    "\n",
    "        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º word ids\n",
    "        inputs = tokenizer(\n",
    "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        self.input_ids = inputs.input_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        input_id = self.input_ids[idx]\n",
    "        try:\n",
    "            # åŠ è½½å›¾åƒå¹¶å°†å…¶è½¬æ¢ä¸º RGB æ¨¡å¼ï¼Œç„¶ååº”ç”¨æ•°æ®å¢å¼º\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            tensor = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ æ— æ³•åŠ è½½å›¾åƒè·¯å¾„: {img_path}, é”™è¯¯: {e}\")\n",
    "            # è¿”å›ä¸€ä¸ªå…¨é›¶çš„å¼ é‡å’Œç©ºçš„è¾“å…¥ ID ä»¥é¿å…å´©æºƒ\n",
    "            tensor = torch.zeros((3, resolution, resolution))\n",
    "            input_id = torch.zeros_like(input_id)\n",
    "        \n",
    "        return tensor, input_id  # è¿”å›å¤„ç†åçš„å›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æ ‡æ³¨\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18b966-f7e0-415d-b49a-70773a33fb7b",
   "metadata": {},
   "source": [
    "### åŠ è½½ LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413064d-c177-4857-944f-0030ff51089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lora_model(lora_config, pretrained_model_name_or_path, model_path=None, resume=False, merge_lora=False):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - åŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ LoRA å±‚ï¼Œå¹¶æ ¹æ®éœ€è¦åˆå¹¶ LoRA æƒé‡ã€‚è¿™åŒ…æ‹¬ Tokenizerã€å™ªå£°è°ƒåº¦å™¨ã€UNetã€VAE å’Œæ–‡æœ¬ç¼–ç å™¨ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - lora_config: LoraConfig, LoRA çš„é…ç½®å¯¹è±¡\n",
    "        - pretrained_model_name_or_path: str, Hugging Face ä¸Šçš„æ¨¡å‹åç§°æˆ–è·¯å¾„\n",
    "        - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„\n",
    "        - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤\n",
    "        - merge_lora: bool, æ˜¯å¦åœ¨æ¨ç†æ—¶åˆå¹¶ LoRA æƒé‡\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - tokenizer: CLIPTokenizer\n",
    "        - noise_scheduler: DDPMScheduler\n",
    "        - unet: UNet2DConditionModel\n",
    "        - vae: AutoencoderKL\n",
    "        - text_encoder: CLIPTextModel\n",
    "    \"\"\"\n",
    "    # åŠ è½½å™ªå£°è°ƒåº¦å™¨ï¼Œç”¨äºæ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å™ªå£°æ·»åŠ å’Œç§»é™¤è¿‡ç¨‹\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "    # åŠ è½½ Tokenizerï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º tokens\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸ºç‰¹å¾å‘é‡\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        subfolder=\"text_encoder\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ VAE æ¨¡å‹ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¤„ç†å›¾åƒçš„æ½œåœ¨è¡¨ç¤º\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"vae\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ UNet æ¨¡å‹ï¼Œè´Ÿè´£å¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒç”Ÿæˆå’Œæ¨ç†è¿‡ç¨‹\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        subfolder=\"unet\"\n",
    "    )\n",
    "    \n",
    "    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡\n",
    "    if resume:\n",
    "        if model_path is None or not os.path.exists(model_path):\n",
    "            raise ValueError(\"å½“ resume è®¾ç½®ä¸º True æ—¶ï¼Œå¿…é¡»æä¾›æœ‰æ•ˆçš„ model_path\")\n",
    "        # ä½¿ç”¨ PEFT çš„ from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹\n",
    "        text_encoder = PeftModel.from_pretrained(text_encoder, os.path.join(model_path, \"text_encoder\"))\n",
    "        unet = PeftModel.from_pretrained(unet, os.path.join(model_path, \"unet\"))\n",
    "\n",
    "        # ç¡®ä¿ UNet çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True\n",
    "        for param in unet.parameters():\n",
    "            if param.requires_grad is False:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # ç¡®ä¿æ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True\n",
    "        for param in text_encoder.parameters():\n",
    "            if param.requires_grad is False:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        print(f\"âœ… å·²ä» {model_path} æ¢å¤æ¨¡å‹æƒé‡\")\n",
    "\n",
    "    else:\n",
    "        # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet\n",
    "        text_encoder = get_peft_model(text_encoder, lora_config)\n",
    "        unet = get_peft_model(unet, lora_config)\n",
    "\n",
    "        # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡\n",
    "        print(\"ğŸ“Š Text Encoder å¯è®­ç»ƒå‚æ•°:\")\n",
    "        text_encoder.print_trainable_parameters()\n",
    "        print(\"ğŸ“Š UNet å¯è®­ç»ƒå‚æ•°:\")\n",
    "        unet.print_trainable_parameters()\n",
    "    \n",
    "    if merge_lora:\n",
    "        # åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼Œä»…åœ¨æ¨ç†æ—¶è°ƒç”¨\n",
    "        text_encoder = text_encoder.merge_and_unload()\n",
    "        unet = unet.merge_and_unload()\n",
    "\n",
    "        # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "        text_encoder.eval()\n",
    "        unet.eval()\n",
    "\n",
    "    # å†»ç»“ VAE å‚æ•°\n",
    "    vae.requires_grad_(False)\n",
    "\n",
    "    # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹\n",
    "    unet.to(DEVICE, dtype=weight_dtype)\n",
    "    vae.to(DEVICE, dtype=weight_dtype)\n",
    "    text_encoder.to(DEVICE, dtype=weight_dtype)\n",
    "    \n",
    "    return tokenizer, noise_scheduler, unet, vae, text_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f62fe6-8b07-4210-84ed-7c4f4368c179",
   "metadata": {},
   "source": [
    "### å‡†å¤‡ä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cd8f9-678f-464b-a386-3dd3eca3b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°åˆ†åˆ«è®¾ç½®ä¼˜åŒ–å™¨ï¼Œå¹¶æŒ‡å®šä¸åŒçš„å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - unet: UNet2DConditionModel, Hugging Face çš„ UNet æ¨¡å‹\n",
    "        - text_encoder: CLIPTextModel, Hugging Face çš„æ–‡æœ¬ç¼–ç å™¨\n",
    "        - unet_learning_rate: float, UNet çš„å­¦ä¹ ç‡\n",
    "        - text_encoder_learning_rate: float, æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - è¾“å‡º: ä¼˜åŒ–å™¨ Optimizer\n",
    "    \"\"\"\n",
    "    # ç­›é€‰å‡º UNet ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°\n",
    "    unet_lora_layers = [p for p in unet.parameters() if p.requires_grad]\n",
    "    \n",
    "    # ç­›é€‰å‡ºæ–‡æœ¬ç¼–ç å™¨ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°\n",
    "    text_encoder_lora_layers = [p for p in text_encoder.parameters() if p.requires_grad]\n",
    "    \n",
    "    # å°†éœ€è¦è®­ç»ƒçš„å‚æ•°åˆ†ç»„å¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡\n",
    "    trainable_params = [\n",
    "        {\"params\": unet_lora_layers, \"lr\": unet_learning_rate},\n",
    "        {\"params\": text_encoder_lora_layers, \"lr\": text_encoder_learning_rate}\n",
    "    ]\n",
    "    \n",
    "    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨\n",
    "    optimizer = torch.optim.AdamW(trainable_params)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c7c21-5dc3-484a-8fa1-5689be53684f",
   "metadata": {},
   "source": [
    "### å®šä¹‰ collate_fn å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bec1c8-4535-4cd4-9c97-e571da1dc861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = []\n",
    "    input_ids = []\n",
    "    \n",
    "    for tensor, input_id in examples:\n",
    "        pixel_values.append(tensor)\n",
    "        input_ids.append(input_id)\n",
    "    \n",
    "    pixel_values = torch.stack(pixel_values, dim=0).float()\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    \n",
    "    # å¦‚æœä½ å–œæ¬¢åˆ—è¡¨æ¨å¯¼å¼çš„è¯ï¼Œä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•\n",
    "    #pixel_values = torch.stack([example[0] for example in examples], dim=0).float()\n",
    "    #input_ids = torch.stack([example[1] for example in examples], dim=0)\n",
    "    \n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f6274-fb4b-4b7c-9936-06252f45f42e",
   "metadata": {},
   "source": [
    "## å‚æ•°è®¾ç½®\n",
    "\n",
    "### 1. è®¾å¤‡é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe24c04-4638-466a-a909-8e44e7b43560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾å¤‡é…ç½®\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# For Mac M1, M2...\n",
    "# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "print(f\"ğŸ–¥ å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a41d7-f426-406c-9fbd-9ba3afe5ed7d",
   "metadata": {},
   "source": [
    "### 2. å›¾åƒé¢„å¤„ç†ä¸æ•°æ®å¢å¼º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967ac77-7b2d-410d-975e-af9433145e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡\n",
    "resolution = 512\n",
    "\n",
    "# æ•°æ®å¢å¼ºæ“ä½œ\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°\n",
    "        transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ\n",
    "        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬\n",
    "        transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9aec3-6c15-4e98-bf83-3b423c5dab40",
   "metadata": {},
   "source": [
    "### 3. æ¨¡å‹ä¸è®­ç»ƒå‚æ•°é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a050558-444c-4d7c-bccd-4b001829fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒç›¸å…³å‚æ•°\n",
    "train_batch_size = 2  # è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œå³æ¯æ¬¡è®­ç»ƒä¸­å¤„ç†çš„æ ·æœ¬æ•°é‡\n",
    "weight_dtype = torch.bfloat16  # æƒé‡æ•°æ®ç±»å‹ï¼Œä½¿ç”¨ bfloat16 ä»¥èŠ‚çœå†…å­˜å¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦\n",
    "snr_gamma = 5  # SNR å‚æ•°ï¼Œç”¨äºä¿¡å™ªæ¯”åŠ æƒæŸå¤±çš„è°ƒèŠ‚ç³»æ•°\n",
    "\n",
    "# è®¾ç½®éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§\n",
    "seed = 1126  # éšæœºæ•°ç§å­\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# Stable Diffusion LoRA çš„å¾®è°ƒå‚æ•°\n",
    "\n",
    "# ä¼˜åŒ–å™¨å‚æ•°\n",
    "unet_learning_rate = 1e-4  # UNet çš„å­¦ä¹ ç‡ï¼Œæ§åˆ¶ UNet å‚æ•°æ›´æ–°çš„æ­¥é•¿\n",
    "text_encoder_learning_rate = 1e-4  # æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ–‡æœ¬åµŒå…¥å±‚çš„å‚æ•°æ›´æ–°æ­¥é•¿\n",
    "\n",
    "# å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°\n",
    "lr_scheduler_name = \"cosine_with_restarts\"  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ä¸º Cosine annealing with restartsï¼Œé€æ¸å‡å°‘å­¦ä¹ ç‡å¹¶å®šæœŸé‡å¯\n",
    "lr_warmup_steps = 100  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œåœ¨æœ€åˆçš„ 100 æ­¥ä¸­é€æ¸å¢åŠ å­¦ä¹ ç‡åˆ°æœ€å¤§å€¼\n",
    "max_train_steps = 200  # æ€»è®­ç»ƒæ­¥æ•°ï¼Œå†³å®šäº†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„è¿­ä»£æ¬¡æ•°\n",
    "num_cycles = 3  # Cosine è°ƒåº¦å™¨çš„å‘¨æœŸæ•°é‡ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¼šé‡å¤ 3 æ¬¡å­¦ä¹ ç‡å‘¨æœŸæ€§é€’å‡å¹¶é‡å¯\n",
    "\n",
    "# é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹è·¯å¾„ï¼Œç”¨äºåŠ è½½æ¨¡å‹è¿›è¡Œå¾®è°ƒ\n",
    "pretrained_model_name_or_path = \"stablediffusionapi/cyberrealistic-41\"  \n",
    "\n",
    "# LoRA é…ç½®\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # LoRA çš„ç§©ï¼Œå³ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œå†³å®šäº†å‚æ•°è°ƒæ•´çš„è‡ªç”±åº¦\n",
    "    lora_alpha=16,  # ç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶ LoRA æƒé‡å¯¹æ¨¡å‹çš„å½±å“\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\",  # æŒ‡å®š Text encoder çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŠ•å½±çŸ©é˜µï¼‰\n",
    "        \"to_k\", \"to_q\", \"to_v\", \"to_out.0\"  # æŒ‡å®š UNet çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´ UNet ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰\n",
    "    ],\n",
    "    lora_dropout=0  # LoRA dropout æ¦‚ç‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨ dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdeedb5-b8ce-4590-9dcc-559729edebd2",
   "metadata": {},
   "source": [
    "## å¾®è°ƒå‰çš„å‡†å¤‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46cd03-d4bd-4094-88f3-036fcf16b872",
   "metadata": {},
   "source": [
    "### 1. æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d335cfc-007d-4d47-878b-432cf8d1f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ– tokenizerï¼Œç”¨äºåŠ è½½æ•°æ®é›†\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\"\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ•°æ®é›†\n",
    "dataset = Text2ImageDataset(\n",
    "    images_folder=images_folder,\n",
    "    captions_folder=captions_folder,\n",
    "    transform=train_transform,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "print(\"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db606893-440b-431b-bdcc-f81695d3d679",
   "metadata": {},
   "source": [
    "### 2. æ¨¡å‹å’Œä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00a8bf-4103-4e22-93e5-14e749a84376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡æ¨¡å‹\n",
    "tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(\n",
    "    lora_config,\n",
    "    pretrained_model_name_or_path,\n",
    "    model_path,\n",
    "    resume=False,  # æ ¹æ®éœ€è¦è®¾ç½®ä¸º True ä»¥ä» checkpoint æ¢å¤\n",
    "    merge_lora=False  # æ˜¯å¦åˆå¹¶ LoRA æƒé‡\n",
    ")\n",
    "\n",
    "# å‡†å¤‡ä¼˜åŒ–å™¨\n",
    "optimizer = prepare_optimizer(\n",
    "    unet, \n",
    "    text_encoder, \n",
    "    unet_learning_rate=unet_learning_rate, \n",
    "    text_encoder_learning_rate=text_encoder_learning_rate\n",
    ")\n",
    "\n",
    "# è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "lr_scheduler = get_scheduler(\n",
    "    lr_scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    "    num_cycles=num_cycles\n",
    ")\n",
    "\n",
    "print(\"âœ… æ¨¡å‹å’Œä¼˜åŒ–å™¨å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3f649-bc32-441a-93b0-4fc784e0fa3b",
   "metadata": {},
   "source": [
    "## å¼€å§‹å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117f19a-fd07-4828-9c0a-1b91ae658986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¦ç”¨å¹¶è¡ŒåŒ–ï¼Œé¿å…è­¦å‘Š\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "global_step = 0\n",
    "best_face_score = float(\"inf\")  # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§ï¼Œå­˜å‚¨æœ€ä½³é¢éƒ¨ç›¸ä¼¼åº¦åˆ†æ•°\n",
    "\n",
    "# è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
    "progress_bar = tqdm(\n",
    "    range(max_train_steps),  # æ ¹æ® num_training_steps è®¾ç½®\n",
    "    desc=\"è®­ç»ƒæ­¥éª¤\",\n",
    ")\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(math.ceil(max_train_steps / len(train_dataloader))):\n",
    "    # å¦‚æœä½ æƒ³åœ¨è®­ç»ƒä¸­å¢åŠ è¯„ä¼°ï¼Œé‚£åœ¨å¾ªç¯ä¸­å¢åŠ  train() æ˜¯æœ‰å¿…è¦çš„\n",
    "    unet.train()\n",
    "    text_encoder.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "        \n",
    "        # ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰\n",
    "        latents = vae.encode(batch[\"pixel_values\"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor  # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨ç©ºé—´\n",
    "\n",
    "        # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ\n",
    "        noise = torch.randn_like(latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º\n",
    "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(DEVICE))[0]\n",
    "\n",
    "        # è®¡ç®—ç›®æ ‡å€¼\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise  # é¢„æµ‹å™ªå£°\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡\n",
    "\n",
    "        # UNet æ¨¡å‹é¢„æµ‹\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states)[0]\n",
    "\n",
    "        # è®¡ç®—æŸå¤±\n",
    "        if not snr_gamma:\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        else:\n",
    "            # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±\n",
    "            snr = compute_snr(noise_scheduler, timesteps)\n",
    "            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                mse_loss_weights = mse_loss_weights / snr\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                mse_loss_weights = mse_loss_weights / (snr + 1)\n",
    "            \n",
    "            # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "\n",
    "        # æ‰“å°è®­ç»ƒæŸå¤±\n",
    "        if global_step % 100 == 0 or global_step == max_train_steps:\n",
    "            print(f\"ğŸ”¥ æ­¥éª¤ {global_step}, æŸå¤±: {loss.item()}\")\n",
    "\n",
    "        # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œå½“å‰ç®€å•è®¾ç½®ä¸ºæ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡\n",
    "        if global_step % 500 == 0:\n",
    "            save_path = os.path.join(output_folder, f\"checkpoint-{global_step}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "            # ä½¿ç”¨ save_pretrained ä¿å­˜ PeftModel\n",
    "            unet.save_pretrained(os.path.join(save_path, \"unet\"))\n",
    "            text_encoder.save_pretrained(os.path.join(save_path, \"text_encoder\"))\n",
    "            print(f\"ğŸ’¾ å·²ä¿å­˜ä¸­é—´æ¨¡å‹åˆ° {save_path}\")\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° checkpoint-last\n",
    "save_path = os.path.join(output_folder, \"checkpoint-last\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "unet.save_pretrained(os.path.join(save_path, \"unet\"))\n",
    "text_encoder.save_pretrained(os.path.join(save_path, \"text_encoder\"))\n",
    "print(f\"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {save_path}\")\n",
    "\n",
    "print(\"ğŸ‰ å¾®è°ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83c69b-bb4a-430b-8580-f897874a69eb",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆå›¾åƒå’Œæµ‹è¯•\n",
    "\n",
    "### åŠ è½½ç”¨äºéªŒè¯çš„ prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cce6c3-d106-4317-8f28-4dc7be4c7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_prompts(validation_prompt_path):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - åŠ è½½éªŒè¯æç¤ºæ–‡æœ¬ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - validation_prompt_path: str, éªŒè¯æç¤ºæ–‡ä»¶çš„è·¯å¾„\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - validation_prompt: list, éªŒè¯æç¤ºçš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªprompt\n",
    "    \"\"\"\n",
    "    with open(validation_prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        validation_prompt = [line.strip() for line in f.readlines()]\n",
    "    return validation_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908312c-d673-4f23-9039-e42b01f05f45",
   "metadata": {},
   "source": [
    "### å®šä¹‰ç”Ÿæˆå›¾åƒçš„å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41e53b-7222-4cff-97de-5d8a7bef482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(pipeline, prompts, num_inference_steps=50, guidance_scale=7.5, output_folder=\"inference\", generator=None):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ä½¿ç”¨ DiffusionPipeline ç”Ÿæˆå›¾åƒï¼Œä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹å¹¶è¿”å›ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - pipeline: DiffusionPipeline, å·²åŠ è½½å¹¶é…ç½®å¥½çš„ Pipeline\n",
    "        - prompts: list, æ–‡æœ¬æç¤ºåˆ—è¡¨\n",
    "        - num_inference_steps: int, æ¨ç†æ­¥éª¤æ•°ï¼Œè¶Šé«˜å›¾åƒè´¨é‡è¶Šå¥½ï¼Œä½†æ¨ç†æ—¶é—´ä¹Ÿä¼šå¢åŠ \n",
    "        - guidance_scale: float, å†³å®šæ–‡æœ¬æç¤ºå¯¹ç”Ÿæˆå›¾åƒçš„å½±å“ç¨‹åº¦\n",
    "        - output_folder: str, ä¿å­˜ç”Ÿæˆå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        - generator: torch.Generator, æ§åˆ¶ç”Ÿæˆéšæœºæ•°çš„ç§å­ï¼Œç¡®ä¿å›¾åƒç”Ÿæˆçš„ä¸€è‡´æ€§ã€‚å¦‚æœä¸æä¾›ï¼Œç”Ÿæˆçš„å›¾åƒæ¯æ¬¡å¯èƒ½ä¸åŒ\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ï¼ŒåŒæ—¶å›¾åƒä¹Ÿä¼šä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ã€‚\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒ...\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    generated_images = []\n",
    "    \n",
    "    for i, prompt in enumerate(tqdm(prompts, desc=\"ç”Ÿæˆå›¾åƒä¸­\")):\n",
    "        # ä½¿ç”¨ pipeline ç”Ÿæˆå›¾åƒ\n",
    "        image = pipeline(prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=generator).images[0]\n",
    "        \n",
    "        # ä¿å­˜å›¾åƒåˆ°æŒ‡å®šæ–‡ä»¶å¤¹\n",
    "        save_file = os.path.join(output_folder, f\"generated_{i+1}.png\")\n",
    "        image.save(save_file)\n",
    "        \n",
    "        # å°†å›¾åƒä¿å­˜åˆ°åˆ—è¡¨ä¸­ï¼Œç¨åè¿”å›\n",
    "        generated_images.append(image)\n",
    "    \n",
    "    print(f\"âœ… å·²ç”Ÿæˆå¹¶ä¿å­˜ {len(prompts)} å¼ å›¾åƒåˆ° {output_folder}\")\n",
    "    \n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ff0b1-f183-4f45-b333-405577ce5958",
   "metadata": {},
   "source": [
    "### å®šä¹‰è¯„ä¼°å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5cfa8-7186-431f-923c-ca40d7a120db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(lora_config):\n",
    "    \"\"\"\n",
    "    åŠ è½½æ¨¡å‹ã€ç”Ÿæˆå›¾åƒå¹¶è¯„ä¼°ã€‚\n",
    "    \n",
    "    ä¸»è¦æ­¥éª¤ï¼š\n",
    "    1. åŠ è½½éªŒè¯æ–‡æœ¬æç¤ºï¼ˆpromptsï¼‰ç”¨äºç”Ÿæˆå›¾åƒã€‚\n",
    "    2. åŠ è½½å’Œå‡†å¤‡ LoRA å¾®è°ƒåçš„æ¨¡å‹ã€‚\n",
    "    3. ä½¿ç”¨ DiffusionPipeline ç”Ÿæˆå›¾åƒã€‚\n",
    "    4. è¯„ä¼°ç”Ÿæˆå›¾åƒçš„äººè„¸ç›¸ä¼¼åº¦ã€CLIP è¯„åˆ†å’Œæ— é¢éƒ¨å›¾åƒæ•°é‡ã€‚\n",
    "    5. æ‰“å°è¯„ä¼°ç»“æœã€‚\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“‚ åŠ è½½éªŒè¯æç¤º...\")\n",
    "    validation_prompts = load_validation_prompts(validation_prompt_path)\n",
    "\n",
    "    print(\"ğŸ”§ å‡†å¤‡ LoRA æ¨¡å‹...\")\n",
    "    # å‡†å¤‡ LoRA æ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼Œåˆå¹¶æƒé‡ï¼‰\n",
    "    tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(\n",
    "        lora_config,\n",
    "        pretrained_model_name_or_path,\n",
    "        model_path=model_path,\n",
    "        resume=True,  # ä»æ£€æŸ¥ç‚¹æ¢å¤\n",
    "        merge_lora=True  # åˆå¹¶ LoRA æƒé‡\n",
    "    )\n",
    "\n",
    "    # åˆ›å»º DiffusionPipeline å¹¶æ›´æ–°å…¶ç»„ä»¶\n",
    "    print(\"ğŸ”„ åˆ›å»º DiffusionPipeline...\")\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        unet=unet,  # ä¼ é€’åŸºç¡€æ¨¡å‹\n",
    "        text_encoder=text_encoder,  # ä¼ é€’åŸºç¡€æ¨¡å‹\n",
    "        torch_dtype=weight_dtype,\n",
    "        safety_checker=None,\n",
    "    )\n",
    "    pipeline = pipeline.to(DEVICE)\n",
    "\n",
    "    # åŠ è½½ CLIP æ¨¡å‹å’Œå¤„ç†å™¨\n",
    "    print(\"ğŸ¯ åŠ è½½ CLIP æ¨¡å‹...\")\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    clip_model = CLIPModel.from_pretrained(clip_model_name).to(DEVICE)\n",
    "    clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "\n",
    "    # CLIP æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    clip_model.eval()\n",
    "\n",
    "    # è®¾ç½®éšæœºæ•°ç§å­\n",
    "    generator = torch.Generator(device=DEVICE)\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    # åŠ è½½è®­ç»ƒå›¾åƒçš„é¢éƒ¨åµŒå…¥\n",
    "    print(\"ğŸ“‚ åŠ è½½è®­ç»ƒå›¾åƒçš„é¢éƒ¨åµŒå…¥...\")\n",
    "    train_image_paths = sorted([\n",
    "        p for p in glob.glob(os.path.join(images_folder, \"*\")) \n",
    "        if any(p.endswith(ext) for ext in IMAGE_EXTENSIONS)\n",
    "    ])\n",
    "    train_emb_list = []\n",
    "    for img_path in tqdm(train_image_paths, desc=\"æå–è®­ç»ƒå›¾åƒé¢éƒ¨åµŒå…¥\"):\n",
    "        face_representation = DeepFace.represent(\n",
    "            img_path, \n",
    "            detector_backend=\"ssd\",\n",
    "            model_name=\"GhostFaceNet\",\n",
    "            enforce_detection=False\n",
    "        )\n",
    "        if face_representation:\n",
    "            embedding = face_representation[0]['embedding']\n",
    "            train_emb_list.append(embedding)\n",
    "\n",
    "    if len(train_emb_list) == 0:\n",
    "        print(\"âš ï¸ æœªèƒ½æå–åˆ°ä»»ä½•è®­ç»ƒå›¾åƒçš„é¢éƒ¨åµŒå…¥ã€‚\")\n",
    "        train_emb = torch.tensor([]).to(DEVICE)\n",
    "    else:\n",
    "        train_emb = torch.tensor(train_emb_list).to(DEVICE)\n",
    "\n",
    "    # ç”Ÿæˆå›¾åƒ\n",
    "    generated_images = generate_images(\n",
    "        pipeline=pipeline,\n",
    "        prompts=validation_prompts,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        output_folder=inference_path,\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    # è¯„ä¼°ç”Ÿæˆçš„å›¾åƒï¼Œmisè®°å½•æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾åƒæ•°é‡\n",
    "    face_score, clip_score, mis = 0, 0, 0  # åˆå§‹åŒ–è¯„ä¼°åˆ†æ•°å’Œè®¡æ•°\n",
    "    valid_emb = []\n",
    "    print(\"ğŸ“Š æ­£åœ¨è®¡ç®—è¯„ä¼°åˆ†æ•°...\")\n",
    "\n",
    "    for i, image in enumerate(tqdm(generated_images, desc=\"è¯„ä¼°å›¾åƒä¸­\")):\n",
    "        # ä½¿ç”¨ DeepFace æ£€æµ‹é¢éƒ¨ç‰¹å¾\n",
    "        opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        emb = DeepFace.represent(\n",
    "            opencvImage,\n",
    "            detector_backend=\"ssd\",\n",
    "            model_name=\"GhostFaceNet\",\n",
    "            enforce_detection=False,\n",
    "        )\n",
    "        if not emb or emb[0].get('face_confidence', 0) == 0:\n",
    "            mis += 1  # æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾åƒæ•°é‡\n",
    "            continue\n",
    "\n",
    "        # è®¡ç®— CLIP åˆ†æ•°\n",
    "        current_prompt = validation_prompts[i]\n",
    "        inputs = clip_processor(text=current_prompt, images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "        sim = outputs.logits_per_image\n",
    "        clip_score += sim.item()\n",
    "\n",
    "        # æ”¶é›†æœ‰æ•ˆçš„é¢éƒ¨åµŒå…¥\n",
    "        valid_emb.append(emb[0]['embedding'])\n",
    "\n",
    "    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„é¢éƒ¨åµŒå…¥ï¼Œåˆ™è¿”å›é»˜è®¤åˆ†æ•°\n",
    "    if len(valid_emb) == 0:\n",
    "        print(\"âš ï¸ æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨åµŒå…¥ï¼\")\n",
    "        return 0, 0, mis\n",
    "\n",
    "    # è®¡ç®—é¢éƒ¨ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä½¿ç”¨æ¬§æ°è·ç¦»ï¼‰\n",
    "    valid_emb = torch.tensor(valid_emb).to(DEVICE)\n",
    "    valid_emb = valid_emb / valid_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "    train_emb = train_emb / train_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "    face_distance = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
    "    face_score = face_distance  # å¹³å‡æ¬§æ°è·ç¦»ä½œä¸ºé¢éƒ¨ç›¸ä¼¼æ€§åˆ†æ•°\n",
    "    clip_score /= (len(validation_prompts) - mis) if (len(validation_prompts) - mis) > 0 else 1\n",
    "    print(\"ğŸ“ˆ è¯„ä¼°å®Œæˆï¼\")\n",
    "\n",
    "    # æ‰“å°è¯„ä¼°ç»“æœ\n",
    "    print(f\"âœ… é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ† (å¹³å‡æ¬§æ°è·ç¦»): {face_score:.4f} (è¶Šä½è¶Šå¥½ï¼Œè¡¨ç¤ºç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒæ›´ç›¸ä¼¼)\")\n",
    "    print(f\"âœ… CLIP è¯„åˆ† (å¹³å‡ç›¸ä¼¼åº¦): {clip_score:.4f} (è¶Šé«˜è¶Šå¥½ï¼Œè¡¨ç¤ºç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„ç›¸å…³æ€§æ›´å¼º)\")\n",
    "    print(f\"âœ… æ— é¢éƒ¨å›¾åƒæ•°é‡: {mis} (æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„ç”Ÿæˆå›¾åƒæ•°é‡)\")\n",
    "\n",
    "# è°ƒç”¨å‡½æ•°æ‰§è¡Œ\n",
    "evaluate(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21673bd9-ac20-47c6-9195-456fd05c7df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
