{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdab2a6-be55-492e-b563-5f62ccc534af",
   "metadata": {},
   "source": [
    "# a. ä½¿ç”¨ Transformers åŠ è½½é‡åŒ–åçš„ LLM å¤§æ¨¡å‹ï¼ˆGPTQ & AWQï¼‰ \n",
    "\n",
    "> å¼•å¯¼æ–‡ç« ï¼š[19a. ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨ Transformers æœ¬åœ°è¿è¡Œé‡åŒ– LLM å¤§æ¨¡å‹ï¼ˆGPTQ & AWQï¼‰](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19a.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Transformers%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGPTQ%20%26%20AWQï¼‰.md)ã€‚\n",
    "\n",
    "ä»£ç æ–‡ä»¶æ²¡æœ‰æ˜¾å¡è¦æ±‚ï¼Œåœ¨ä¸ªäººè®¡ç®—æœºä¸Šå‡å¯è¿›è¡Œå¯¹è¯ã€‚\n",
    "\n",
    "**æ¨¡å‹æ–‡ä»¶çº¦ä¸º 4 GB**ã€‚\n",
    "\n",
    "è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç®€å•çš„ [ğŸ¡ AI Chat è„šæœ¬](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/chat.py)ä¾›ä½ å°è¯•ï¼Œè¯¦è§ï¼š[CodePlayground](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/README.md#å½“å‰çš„ç©å…·)ï¼Œç‚¹å‡» `â–º` æˆ–å¯¹åº”çš„æ–‡æœ¬å±•å¼€ã€‚\n",
    "\n",
    "Llama-cpp-python å…³äº GGUF æ–‡ä»¶åŠ è½½çš„ç›¸å…³é“¾æ¥ï¼š[æ–‡ç«  19b](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19b.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Llama-cpp-python%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.md) | [ä»£ç æ–‡ä»¶ 16b](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/16b.%20ä½¿ç”¨%20Llama-cpp-python%20åŠ è½½é‡åŒ–åçš„%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.ipynb)ã€‚\n",
    "\n",
    "åœ¨çº¿é“¾æ¥ï¼š[Kaggle - a](https://www.kaggle.com/code/aidemos/16a-transformers-llm-gptq) | [Colab - a](https://colab.research.google.com/drive/1cmIDjHriW8aQ5mIsV6ZeTqdnqYe6PoOv?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c340d-1694-4a80-bc61-3e97e00e9e42",
   "metadata": {},
   "source": [
    "## æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆæ¨èï¼‰\n",
    "\n",
    "æ¥è¯•è¯•å¤šçº¿ç¨‹æŒ‡å®šæ–‡ä»¶ä¸‹è½½ï¼Œå¯¹äº Linuxï¼Œè¿™é‡Œç»™å‡ºé…ç½®å‘½ä»¤ï¼Œå…¶ä½™ç³»ç»Ÿå¯ä»¥å‚ç…§[ã€Ša. ä½¿ç”¨ HFD åŠ å¿« Hugging Face æ¨¡å‹å’Œæ•°æ®é›†çš„ä¸‹è½½ã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/a.%20ä½¿ç”¨%20HFD%20åŠ å¿«%20Hugging%20Face%20æ¨¡å‹å’Œæ•°æ®é›†çš„ä¸‹è½½.md)å…ˆè¿›è¡Œç¯å¢ƒé…ç½®ã€‚ä½ ä¹Ÿå¯ä»¥è·³è¿‡è¿™éƒ¨åˆ†ï¼Œåé¢ä¼šä»‹ç»è‡ªåŠ¨ä¸‹è½½ã€‚\n",
    "\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install git git-lfs wget aria2\n",
    "git lfs install\n",
    "```\n",
    "\n",
    "ä¸‹è½½å¹¶é…ç½® HFD è„šæœ¬ï¼š\n",
    "\n",
    "```bash\n",
    "wget https://huggingface.co/hfd/hfd.sh\n",
    "chmod a+x hfd.sh\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "```\n",
    "\n",
    "ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½æŒ‡å®šæ¨¡å‹ã€‚\n",
    "\n",
    "### GPTQ\n",
    "\n",
    "å‘½ä»¤éµå¾ª `./hfd.sh <model_path> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š\n",
    "\n",
    "```bash\n",
    "./hfd.sh neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit --tool aria2c -x 8\n",
    "```\n",
    "\n",
    "### AWQ\n",
    "\n",
    "å‘½ä»¤éµå¾ª `./hfd.sh <model_path> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š\n",
    "\n",
    "```python\n",
    "./hfd.sh solidrust/Mistral-7B-Instruct-v0.3-AWQ --tool aria2c -x 8\n",
    "```\n",
    "\n",
    "### GGUF\n",
    "\n",
    "ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½æŒ‡å®šæ¨¡å‹ï¼Œå‘½ä»¤éµå¾ª `./hfd.sh <model_path> --include <file_name> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š\n",
    "\n",
    "```python\n",
    "./hfd.sh bartowski/Mistral-7B-Instruct-v0.3-GGUF --include Mistral-7B-Instruct-v0.3-Q4_K_M.gguf --tool aria2c -x 8\n",
    "```\n",
    "\n",
    "ä¸‹è½½å®Œæˆä½ åº”è¯¥å¯ä»¥çœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š\n",
    "\n",
    "```\n",
    "Download Results:\n",
    "gid   |stat|avg speed  |path/URI\n",
    "======+====+===========+=======================================================\n",
    "145eba|OK  |   6.8MiB/s|./Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\n",
    "\n",
    "Status Legend:\n",
    "(OK):download completed.\n",
    "Downloaded https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf successfully.\n",
    "Download completed successfully.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dea4a5-9f06-40c1-b443-8d51b2be20d0",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f9a99-176d-429c-af1d-63a2cb46aee9",
   "metadata": {},
   "source": [
    "### ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085c448-afe2-451b-897c-68ed7ad131d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!uv add numpy\n",
    "!uv add pandas\n",
    "!uv add transformers\n",
    "!uv add optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e7ad-9ee7-43ed-827d-654d2cb95506",
   "metadata": {},
   "source": [
    "### å…³äº autogptq çš„è¿ç§»\n",
    "\n",
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒAutoGPTQ çš„å¼€å‘å›¢é˜Ÿåœ¨ [24 å¹´ 12 æœˆ](https://github.com/AutoGPTQ/AutoGPTQ/discussions/758)æ˜ç¡®å»ºè®®æ‰€æœ‰ç”¨æˆ·è¿ç§»åˆ° GPTQModelã€‚ä¸”ä»“åº“çš„ [setup.py](https://github.com/AutoGPTQ/AutoGPTQ/blob/9f7d37072917ab3a7545835f23e808294a542153/setup.py#L11) æ–‡ä»¶ä¸­åŒ…å«å¼ƒç”¨è­¦å‘Šï¼š\"AutoGPTQ å·²åœæ­¢å¼€å‘ï¼Œè¯·è¿‡æ¸¡åˆ° GPTQModel...è®¡åˆ’åœ¨è¿‘æœŸå®Œå…¨ä» HuggingFace æ¡†æ¶ä¸­å¼ƒç”¨ AutoGPTQ\"ã€‚\n",
    "\n",
    "æ‰€ä»¥è¿™é‡Œæ–°å¢ä¸€ä¸ªå°æ¨¡å—è¿›è¡Œè¯´æ˜ã€‚\n",
    "\n",
    "åœ¨ç”¨æ³•æ–¹é¢ï¼Œæ—¢å¯ä»¥ä½¿ç”¨[å®˜æ–¹ä»“åº“](https://github.com/ModelCloud/GPTQModel)æ‰€å™è¿°çš„ GPTQModel.load(...)ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ transformersï¼Œè¯¦è§ [GPTQ - Hugging Face](https://huggingface.co/docs/transformers/quantization/gptq?install=GPTQmodel)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619732b-590d-4119-a968-64983be6c19f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!uv pip install -v gptqmodel --no-build-isolation  # å®‰è£…æ—¶é—´è¾ƒé•¿\n",
    "!uv pip install logbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e82aff-a3a9-4ea5-a510-f935628ac045",
   "metadata": {},
   "source": [
    "### GPTQ\n",
    "\n",
    "#### å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf71d3-8ea9-4cb7-b5f9-25bd1c2bc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# è®¾ç½®æ¨¡å‹ä¸‹è½½é•œåƒï¼ˆæ³¨æ„ï¼Œéœ€è¦åœ¨å¯¼å…¥ transformers ç­‰æ¨¡å—å‰è¿›è¡Œè®¾ç½®æ‰èƒ½èµ·æ•ˆï¼‰\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import torch\n",
    "from gptqmodel import GPTQModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74c5e0-8d78-4800-af07-290302b054fd",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä»‹ç»ä¸¤ç§å¯¼å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œå®é™…æ‰§è¡Œæ—¶æœ¬åœ°/è‡ªåŠ¨å¯¼å…¥äºŒé€‰ä¸€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d16b3-1d23-486f-bd32-a60d245d7d87",
   "metadata": {},
   "source": [
    "#### è®¾ç½®æ¨¡å‹è·¯å¾„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45027d9a-0f4a-4bf0-8416-9f20fb010825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœä½ å·²ç»é…ç½®è¿‡äº†ï¼Œå¯ä»¥ç›´æ¥åœ¨ Notebook ä¸­æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ä¸‹è½½ã€‚\n",
    "# !./hfd.sh neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit --tool aria2c -x 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ae2a7-fd89-4b8a-b7d9-b47a44c613be",
   "metadata": {},
   "source": [
    "å¦‚æœå·²ç»åœ¨æœ¬åœ°ä¸‹è½½äº†æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ¨¡å‹è·¯å¾„æ¥åŠ è½½æ¨¡å‹ã€‚ä»¥ä¸‹ç¤ºä¾‹å‡è®¾æ¨¡å‹ä½äºå½“å‰ç›®å½•çš„ `Mistral-7B-Instruct-v0.3-GPTQ-4bit` æ–‡ä»¶å¤¹ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7636b-e20d-4a38-880d-8915dd0b0216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„\n",
    "# model_path = \"./Mistral-7B-Instruct-v0.3-GPTQ-4bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97d93f-4c5a-4f6d-b90c-9640e74a3a17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œè®¾ç½®è¿œç¨‹è·¯å¾„ï¼ˆ`id` + `/` + `model_name`ï¼‰ï¼Œå¯¼å…¥çš„æ—¶å€™ä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0d8bc-3948-4f99-be76-41c8858a7eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# æŒ‡å®šè¿œç¨‹æ¨¡å‹çš„è·¯å¾„\n",
    "model_path = \"neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5510a5-4d7a-40a5-8859-54bbf2557a4e",
   "metadata": {},
   "source": [
    "#### åŠ è½½æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f05f48-b9a7-48f5-996c-153227b93822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹\n",
    "model = GPTQModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f31a6f-94e4-48bb-a8fd-78eac7b2f3af",
   "metadata": {},
   "source": [
    "#### æ¨ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25eaaa6-59e6-4a6f-a0ed-aa9db261cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¾“å…¥æ–‡æœ¬\n",
    "input_text = \"Hello, World!\"\n",
    "\n",
    "# å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼ï¼ˆåŒ…å«attention_maskï¼‰\n",
    "inputs = tokenizer(\n",
    "    input_text, \n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True  # æ˜ç¡®è¿”å›attention_mask\n",
    ").to(model.device)\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡º\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,  # ä¼ é€’æ•´ä¸ªinputså­—å…¸ï¼ŒåŒ…å«input_idså’Œattention_mask\n",
    "        max_length=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# è§£ç ç”Ÿæˆçš„è¾“å‡º\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3f269-22f0-485f-b5fd-2d0ebf3a662c",
   "metadata": {},
   "source": [
    "### å…³äº autoawq çš„è¿ç§»\n",
    "\n",
    "ç›®å‰ AutoAWQ å·²ç»å¼ƒç”¨ï¼Œå¯ä½¿ç”¨å¦ä¸€ä¸ªé¡¹ç›® vLLM è¿›è¡Œæ›¿ä»£ï¼šhttps://github.com/vllm-project/llm-compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1945e5-4d79-4a47-97b0-08dd23ddd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv pip install vllm\n",
    "# å¦‚æœè¿˜æƒ³å®‰è£… autoawqï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š\n",
    "# uv pip install autoawq 'autoawq[kernels]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093ae9a-0049-4eb7-995d-78958aaa4004",
   "metadata": {},
   "source": [
    "### AWQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a036ce-ce60-4efe-aa9b-86ef1c1d9d61",
   "metadata": {},
   "source": [
    "#### å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97c440-7083-4a09-9203-4d4a4bec7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# è®¾ç½®æ¨¡å‹ä¸‹è½½é•œåƒï¼ˆæ³¨æ„ï¼Œéœ€è¦åœ¨å¯¼å…¥ transformers ç­‰æ¨¡å—å‰è¿›è¡Œè®¾ç½®æ‰èƒ½èµ·æ•ˆï¼‰\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c534d8-3792-41a5-97ee-e7859e2aa094",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä»‹ç»ä¸¤ç§å¯¼å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œå®é™…æ‰§è¡Œæ—¶æœ¬åœ°/è‡ªåŠ¨å¯¼å…¥äºŒé€‰ä¸€ã€‚\n",
    "\n",
    "#### è®¾ç½®æ¨¡å‹è·¯å¾„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e43cbe-4282-4c05-bf6c-881d14e572a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # å¦‚æœä½ å·²ç»é…ç½®è¿‡äº†ï¼Œå¯ä»¥ç›´æ¥åœ¨ Notebook ä¸­æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ä¸‹è½½ã€‚\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com\n",
    "# !./hfd.sh solidrust/Mistral-7B-Instruct-v0.3-AWQ --tool aria2c -x 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cbbd2-03a1-4246-8b89-ae4b5e7d5363",
   "metadata": {},
   "source": [
    "å¦‚æœå·²ç»åœ¨æœ¬åœ°ä¸‹è½½äº†æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ¨¡å‹è·¯å¾„æ¥åŠ è½½æ¨¡å‹ã€‚ä»¥ä¸‹ç¤ºä¾‹å‡è®¾æ¨¡å‹ä½äºå½“å‰ç›®å½•çš„ `Mistral-7B-Instruct-v0.3-AWQ` æ–‡ä»¶å¤¹ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362c6e6-4a72-45a8-a51f-539550798f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„\n",
    "# model_path = \"./Mistral-7B-Instruct-v0.3-AWQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136eca1-3f7f-4731-bf79-0ac659b5566d",
   "metadata": {},
   "source": [
    "å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œè®¾ç½®è¿œç¨‹è·¯å¾„ï¼ˆ`id` + `/` + `model_name`ï¼‰ï¼Œå¯¼å…¥çš„æ—¶å€™ä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c9108-fa94-4cf1-a26e-3b9e24ce29f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# æŒ‡å®šè¿œç¨‹æ¨¡å‹çš„è·¯å¾„\n",
    "model_path = \"solidrust/Mistral-7B-Instruct-v0.3-AWQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a752722-eb3f-4e15-aafa-b89ec8cb27d7",
   "metadata": {},
   "source": [
    "#### åŠ è½½æ¨¡å‹\n",
    "\n",
    "ä¸€äº›æƒé‡ä¸ä¼šè¢«åŠ è½½ï¼Œå¯¹äºå½“å‰ä»»åŠ¡æ¥è¯´è¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e052c-d8bb-41d2-bd0a-8113ec1597b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ˆvLLMä¼šè‡ªåŠ¨å¤„ç†tokenizerï¼‰\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909e888-42a1-4a3a-954a-c090bf1afdcf",
   "metadata": {},
   "source": [
    "#### æ¨ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf0b23-4328-439f-8265-c48f82ae2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=50,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# è¾“å…¥æ–‡æœ¬\n",
    "input_text = \"Hello, World!\"\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡ºï¼ˆvLLMè‡ªåŠ¨å¤„ç†ç¼–ç å’Œè§£ç ï¼‰\n",
    "outputs = model.generate([input_text], sampling_params)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "output_text = outputs[0].outputs[0].text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f1f2d-6ed3-4caa-b3e4-04e8ac1cd452",
   "metadata": {},
   "source": [
    "### ç»Ÿä¸€æ–¹å¼åŠ è½½\n",
    "\n",
    "â€œè¦æ˜¯æ‰€æœ‰çš„æ¨¡å‹éƒ½èƒ½ç»Ÿä¸€å°±å¥½äº†ï¼Œè¿™æ ·å°±ä¸ç”¨æŸ¥é˜…å…¶ä»–åº“çš„æ–‡æ¡£äº†ã€‚â€\n",
    "\n",
    "äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä½¿ç”¨ transformers è¿›è¡ŒäºŒè€…çš„åŠ è½½ï¼ˆå¦‚æœä½ å¯¹å‚æ•°è®¾ç½®æ²¡æœ‰æ›´ç»†è‡´çš„è¦æ±‚ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c81a4b-f979-4549-8dbf-86e9126f27e7",
   "metadata": {},
   "source": [
    "#### ä¸‹è½½ autoawq åº“ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœæƒ³ç”¨ transformers åŠ è½½ï¼Œåˆ™éœ€è¦ä¸‹è½½ autoawq åº“ã€‚\n",
    "\n",
    "ä» `v0.2.7.post3` å¼€å§‹ awq ä¼šä½¿ç”¨ `flash-attn` åº“ï¼Œä½†éƒ¨åˆ† GPU æ¯”å¦‚åœ¨çº¿å¹³å° Kaggle å’Œ Colab æä¾›çš„ T4 ä¸æ”¯æŒï¼Œè¿è¡Œåç»­ä»£ç ä¼šæŠ¥é”™ï¼š`RuntimeError: FlashAttention only supports Ampere GPUs or newer.`\n",
    "\n",
    "æ•…é€‰æ‹©ä¸‹è½½å›ºå®šç‰ˆæœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107fe54-520c-4530-a96f-3f8a8143d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install autoawq==0.2.7.post2 'autoawq[kernels]==0.2.7.post2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23672cc5-e408-49c6-b0d9-12be2e06fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# è®¾ç½®æ¨¡å‹ä¸‹è½½é•œåƒ\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_quantized_model(model_path):\n",
    "    \"\"\"ç»Ÿä¸€åŠ è½½GPTQæˆ–AWQæ¨¡å‹çš„å‡½æ•°\"\"\"\n",
    "    \n",
    "    # åŠ è½½åˆ†è¯å™¨\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ä¿®å¤pad_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.unk_token or tokenizer.eos_token\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹ - transformersä¼šè‡ªåŠ¨æ£€æµ‹é‡åŒ–æ ¼å¼\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        # attn_implementation=\"flash_attention_2\"  # å¯é€‰ï¼šä½¿ç”¨flash attention\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# æµ‹è¯•ä¸åŒçš„æ¨¡å‹\n",
    "models = {\n",
    "    \"GPTQ\": \"neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit\",\n",
    "    \"AWQ\": \"solidrust/Mistral-7B-Instruct-v0.3-AWQ\"\n",
    "}\n",
    "\n",
    "# é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹\n",
    "model_type = \"GPTQ\"  # æˆ– \"AWQ\"\n",
    "model_path = models[model_type]\n",
    "\n",
    "print(f\"åŠ è½½ {model_type} æ¨¡å‹: {model_path}\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model, tokenizer = load_quantized_model(model_path)\n",
    "\n",
    "print(f\"æ¨¡å‹è®¾å¤‡: {model.device}\")\n",
    "print(f\"æ¨¡å‹æ•°æ®ç±»å‹: {model.dtype}\")\n",
    "\n",
    "# è¾“å…¥æ–‡æœ¬\n",
    "input_text = \"Hello, World!\"\n",
    "\n",
    "# å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼\n",
    "inputs = tokenizer(\n",
    "    input_text, \n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    ").to(model.device)\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡º\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,  # max_new_tokens ç›´æ¥ç­‰ä»·äºç”Ÿæˆçš„ tokens æ•°é‡ï¼Œå‚æ•° max_length = len(input) + max_new_tokens\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "input_length = inputs['input_ids'].shape[1]\n",
    "generated_text = tokenizer.decode(output_ids[0][input_length:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"è¾“å…¥: {input_text}\")\n",
    "print(f\"ç”Ÿæˆ: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ea31a-a5d8-47c3-9e7c-5fe6ee5c290f",
   "metadata": {},
   "source": [
    "### äº†è§£æç¤ºè¯æ¨¡ç‰ˆï¼ˆprompt_templateï¼‰\n",
    "\n",
    "å…¶å®éå¸¸ç®€å•ï¼Œå°±æ˜¯æ›¾ç»æåˆ°çš„å ä½ç¬¦ï¼ˆä¸‹å›¾å¯¹äº `{{question}}` çš„åº”ç”¨ï¼‰ã€‚\n",
    "\n",
    "![å ä½ç¬¦](../Guide/assets/%E5%8D%A0%E4%BD%8D%E7%AC%A6-6055722.png)\n",
    "\n",
    "ä¸¾ä¸ªç›´è§‚çš„ä¾‹å­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a83d1-e986-4dae-a7b7-962e0444a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ Prompt Template\n",
    "prompt_template = \"é—®ï¼š{question}\\nç­”ï¼š\"\n",
    "\n",
    "# å®šä¹‰é—®é¢˜\n",
    "question = \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "# ä½¿ç”¨ Prompt Template ç”Ÿæˆå®Œæ•´çš„æç¤º\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(prompt)\n",
    "# print(\"\\n\")\n",
    "# print(f\"é—®ï¼š{question}\\nç­”ï¼š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05446cce-b0fa-4c49-a602-bcae318fd314",
   "metadata": {},
   "source": [
    "#### tokenizer.chat_template\n",
    "\n",
    "æŸ¥çœ‹æ¨¡å‹çš„ `chat_template`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc212d22-5316-4bf9-bc2d-58212e533b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å° chat_template ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰\n",
    "if hasattr(tokenizer, 'chat_template'):\n",
    "    print(tokenizer.chat_template)\n",
    "else:\n",
    "    print(\"Tokenizer æ²¡æœ‰ 'chat_template' å±æ€§ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9a3a9-cac7-47ea-890d-430b1ba20fd4",
   "metadata": {},
   "source": [
    "### æµå¼è¾“å‡º\n",
    "\n",
    "åœ¨é¡¹ç›®åˆæœŸè®¤è¯† API çš„æ—¶å€™ï¼Œæ–‡ç« [ã€Š01. åˆè¯† LLM APIï¼šç¯å¢ƒé…ç½®ä¸å¤šè½®å¯¹è¯æ¼”ç¤ºã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/01.%20åˆè¯†%20LLM%20APIï¼šç¯å¢ƒé…ç½®ä¸å¤šè½®å¯¹è¯æ¼”ç¤º.md#æµå¼è¾“å‡º)æœ‰æåˆ°è¿‡æµå¼è¾“å‡ºï¼Œè¿™æ˜¯æˆ‘ä»¬ä¸€ç›´ä»¥æ¥è§åˆ°çš„å¤§æ¨¡å‹è¾“å‡ºæ–¹å¼ï¼šé€å­—ï¼ˆtokenï¼‰æ‰“å°è€Œéç­‰å…¨éƒ¨ç”Ÿæˆå®Œæ‰“å°ã€‚\n",
    "\n",
    "æ‰§è¡Œä¸‹é¢çš„ä»£ç è¯•è¯•ï¼ˆæ— è®ºä¹‹å‰å¯¼å…¥çš„æ˜¯å“ªç§æ¨¡å‹ï¼Œéƒ½å¯ä»¥ç»§ç»­ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b6478-3937-46e7-9235-7f91c02bfedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# åˆ›å»º TextStreamer å®ä¾‹\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬\n",
    "    skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰\n",
    ")\n",
    "\n",
    "# å°†æç¤ºç¼–ç ä¸ºæ¨¡å‹è¾“å…¥\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "generation_kwargs = {\n",
    "    \"input_ids\": input_ids,  # æ¨¡å‹çš„è¾“å…¥ IDï¼Œæ³¨æ„ï¼Œè¿™ä¸æ˜¯ Embedding\n",
    "    \"max_length\": 200,       # ç”Ÿæˆçš„æœ€å¤§ token æ•°\n",
    "    \"streamer\": streamer,    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬\n",
    "    \"pad_token_id\": tokenizer.eos_token_id  # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤ open-end è­¦å‘Š\n",
    "}\n",
    "\n",
    "# å¼€å§‹ç”Ÿæˆæ–‡æœ¬\n",
    "with torch.no_grad():\n",
    "    # ** æ˜¯ Python ä¸­çš„è§£åŒ…æ“ä½œç¬¦ï¼Œå®ƒå°†å­—å…¸ä¸­çš„é”®å€¼å¯¹è§£åŒ…ä¸ºå‡½æ•°çš„å…³é”®å­—å‚æ•°ã€‚\n",
    "    # åœ¨è¿™é‡Œï¼Œ**generation_kwargs å°†å­—å…¸ä¸­çš„å‚æ•°é€ä¸€ä¼ é€’ç»™ model.generate() æ–¹æ³•ï¼Œ\n",
    "    # ç­‰æ•ˆäºç›´æ¥å†™å‡ºæ‰€æœ‰å‚æ•°ï¼š\n",
    "    # model.generate(input_ids=input_ids, max_length=200, do_sample=True, ...)\n",
    "    # ä½ éœ€è¦æ³¨æ„åˆ°ï¼Œè¿™å’Œä¹‹å‰é‡‡ç”¨äº†ä¸åŒçš„ä¼ å‚æ–¹å¼ï¼Œä½†æœ¬è´¨æ˜¯ä¸€æ ·çš„ã€‚\n",
    "    # åœ¨åç»­çš„æ•™ç¨‹ä¸­ï¼Œä¼šè¾ƒå°‘åœ°ä½¿ç”¨è¿™ç§æ–¹å¼è¿›è¡Œä¼ å‚ã€‚\n",
    "    # å› ä¸ºè¿™å¾ˆå¥½çš„åˆ†ç¦»äº†å‚æ•°ï¼Œæ‰€ä»¥ä¹Ÿå¢åŠ äº†ä¹ä¸€çœ‹ä¹‹ä¸‹çš„æŠ½è±¡åº¦ï¼Œä¸ºäº†åˆè§çš„ç›´è§‚ï¼Œå°†å‡å°‘ä½¿ç”¨ã€‚\n",
    "    model.generate(**generation_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b235a01-1780-4573-b49e-3d889b0d4b76",
   "metadata": {},
   "source": [
    "### å•è½®å¯¹è¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aae13-8ba2-4654-8825-db9e97dd17cc",
   "metadata": {},
   "source": [
    "ï¼ˆå¦‚æœé‡æ–°å¯åŠ¨å†…æ ¸çš„è¯ï¼Œéµå¾ª `å¯¼å…¥åº“`-> `å¯¼å…¥æ¨¡å‹` -> `å½“å‰ä»£ç å—` çš„é¡ºåºæ‰§è¡Œã€‚ï¼‰\n",
    "\n",
    "è®©æˆ‘ä»¬ç›´æ¥è®¾è®¡ `messages`ï¼Œå¹¶åº”ç”¨ `chat_template` è¿›è¡Œå¯¹è¯ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da6253-3280-4761-aa07-1b6b39336a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# å®šä¹‰è¾“å…¥\n",
    "prompt = input(\"User: \")\n",
    "\n",
    "# å®šä¹‰æ¶ˆæ¯åˆ—è¡¨\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# ä½¿ç”¨ tokenizer.apply_chat_template() ç”Ÿæˆæ¨¡å‹è¾“å…¥\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# åˆ›å»º TextStreamer å®ä¾‹\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬\n",
    "    skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰\n",
    ")\n",
    "\n",
    "# è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "generation_kwargs = {\n",
    "    \"input_ids\": input_ids,  # æ¨¡å‹çš„è¾“å…¥ IDï¼Œæ³¨æ„ï¼Œè¿™ä¸æ˜¯ Embedding\n",
    "    \"max_length\": 500,      # ç”Ÿæˆçš„æœ€å¤§ token æ•°\n",
    "    \"streamer\": streamer,    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬\n",
    "    \"pad_token_id\": tokenizer.eos_token_id  # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤ open-end è­¦å‘Š\n",
    "}\n",
    "\n",
    "# å¼€å§‹ç”Ÿæˆæ–‡æœ¬\n",
    "with torch.no_grad():\n",
    "    model.generate(**generation_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bbef9-d9c7-4840-b6f0-8b042a811a2c",
   "metadata": {},
   "source": [
    "### å¤šè½®å¯¹è¯\n",
    "\n",
    "å¦‚æœé‡æ–°å¯åŠ¨å†…æ ¸çš„è¯ï¼Œéµå¾ª `å¯¼å…¥åº“`-> `å¯¼å…¥æ¨¡å‹` -> `å½“å‰ä»£ç å—` çš„é¡ºåºæ‰§è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea07ea5-e61d-43a9-abee-6aab5322cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# åˆå§‹åŒ–å¯¹è¯å†å²\n",
    "messages = []\n",
    "\n",
    "# å¼€å§‹å¤šè½®å¯¹è¯\n",
    "while True:\n",
    "    # è·å–è¾“å…¥\n",
    "    prompt = input(\"User: \")\n",
    "    \n",
    "    # é€€å‡ºå¯¹è¯æ¡ä»¶ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ç»ˆæ­¢ä»£ç å—ï¼‰\n",
    "    if prompt.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # å°†è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # ä½¿ç”¨ tokenizer.apply_chat_template() ç”Ÿæˆæ¨¡å‹è¾“å…¥\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # åˆ›å»º TextStreamer å®ä¾‹\n",
    "    streamer = TextStreamer(\n",
    "        tokenizer, \n",
    "        skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬\n",
    "        skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰\n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids,                  # æ¨¡å‹çš„è¾“å…¥ ID\n",
    "        \"max_length\": input_ids.shape[1] + 500,  # ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼Œinput_ids.shape[1] å³è¾“å…¥å¯¹åº”çš„ tokens æ•°é‡\n",
    "        \"streamer\": streamer,                    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬\n",
    "        \"pad_token_id\": tokenizer.eos_token_id   # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤è­¦å‘Š\n",
    "    }\n",
    "    \n",
    "    # å¼€å§‹ç”Ÿæˆå›å¤\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**generation_kwargs)\n",
    "    \n",
    "    # è·å–ç”Ÿæˆçš„å›å¤æ–‡æœ¬\n",
    "    assistant_reply = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # å°†æ¨¡å‹çš„å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_reply})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a66a9f-776d-4d6e-81d5-77c6a60525c0",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå°å‘ï¼Œä½ ä¸èƒ½ç®€å•ä½¿ç”¨ `output_ids[0]` æ¥ä¿å­˜å›å¤ï¼Œå› ä¸º`output_ids` ä¸­å®é™…ä¸ŠåŒ…å«äº† `input_ids`ï¼Œæ‰“å°å®ƒä»¬ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f7cc7-1c07-4715-bbed-386d06ad77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c70b9-bf14-46ad-acc8-8bd493c31a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
