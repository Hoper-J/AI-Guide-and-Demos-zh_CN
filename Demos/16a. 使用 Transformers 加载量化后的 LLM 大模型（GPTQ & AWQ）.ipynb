{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdab2a6-be55-492e-b563-5f62ccc534af",
   "metadata": {},
   "source": [
    "# a. ä½¿ç”¨ Transformers åŠ è½½é‡åŒ–åçš„ LLM å¤§æ¨¡å‹ï¼ˆGPTQ & AWQï¼‰ \n",
    "\n",
    "> å¼•å¯¼æ–‡ç« ï¼š[19a. ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨ Transformers æœ¬åœ°è¿è¡Œé‡åŒ– LLM å¤§æ¨¡å‹ï¼ˆGPTQ & AWQï¼‰](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19a.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Transformers%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGPTQ%20%26%20AWQï¼‰.md)ã€‚\n",
    "\n",
    "ä»£ç æ–‡ä»¶æ²¡æœ‰æ˜¾å¡è¦æ±‚ï¼Œåœ¨ä¸ªäººè®¡ç®—æœºä¸Šå‡å¯è¿›è¡Œå¯¹è¯ã€‚\n",
    "\n",
    "**æ¨¡å‹æ–‡ä»¶çº¦ä¸º 4 GB**ã€‚\n",
    "\n",
    "è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç®€å•çš„ [ğŸ¡ AI Chat è„šæœ¬](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/chat.py)ä¾›ä½ å°è¯•ï¼Œè¯¦è§ï¼š[CodePlayground](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/README.md#å½“å‰çš„ç©å…·)ï¼Œç‚¹å‡» `â–º` æˆ–å¯¹åº”çš„æ–‡æœ¬å±•å¼€ã€‚\n",
    "\n",
    "Llama-cpp-python å…³äº GGUF æ–‡ä»¶åŠ è½½çš„ç›¸å…³é“¾æ¥ï¼š[æ–‡ç«  19b](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19b.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Llama-cpp-python%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.md) | [ä»£ç æ–‡ä»¶ 16b](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/16b.%20ä½¿ç”¨%20Llama-cpp-python%20åŠ è½½é‡åŒ–åçš„%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.ipynb)ã€‚\n",
    "\n",
    "åœ¨çº¿é“¾æ¥ï¼š[Kaggle - a](https://www.kaggle.com/code/aidemos/16a-transformers-llm-gptq) | [Colab - a](https://colab.research.google.com/drive/1cmIDjHriW8aQ5mIsV6ZeTqdnqYe6PoOv?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c340d-1694-4a80-bc61-3e97e00e9e42",
   "metadata": {},
   "source": [
    "## æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆæ¨èï¼‰\n",
    "\n",
    "æ¥è¯•è¯•å¤šçº¿ç¨‹æŒ‡å®šæ–‡ä»¶ä¸‹è½½ï¼Œå¯¹äº Linuxï¼Œè¿™é‡Œç»™å‡ºé…ç½®å‘½ä»¤ï¼Œå…¶ä½™ç³»ç»Ÿå¯ä»¥å‚ç…§[ã€Ša. ä½¿ç”¨ HFD åŠ å¿« Hugging Face æ¨¡å‹å’Œæ•°æ®é›†çš„ä¸‹è½½ã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/a.%20ä½¿ç”¨%20HFD%20åŠ å¿«%20Hugging%20Face%20æ¨¡å‹å’Œæ•°æ®é›†çš„ä¸‹è½½.md)å…ˆè¿›è¡Œç¯å¢ƒé…ç½®ã€‚ä½ ä¹Ÿå¯ä»¥è·³è¿‡è¿™éƒ¨åˆ†ï¼Œåé¢ä¼šä»‹ç»è‡ªåŠ¨ä¸‹è½½ã€‚\n",
    "\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install git git-lfs wget aria2\n",
    "git lfs install\n",
    "```\n",
    "\n",
    "ä¸‹è½½å¹¶é…ç½® HFD è„šæœ¬ï¼š\n",
    "\n",
    "```bash\n",
    "wget https://huggingface.co/hfd/hfd.sh\n",
    "chmod a+x hfd.sh\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "```\n",
    "\n",
    "ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½æŒ‡å®šæ¨¡å‹ã€‚\n",
    "\n",
    "### GPTQ\n",
    "\n",
    "å‘½ä»¤éµå¾ª `./hfd.sh <model_path> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š\n",
    "\n",
    "```bash\n",
    "./hfd.sh neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit --tool aria2c -x 16\n",
    "```\n",
    "\n",
    "### AWQ\n",
    "\n",
    "å‘½ä»¤éµå¾ª `./hfd.sh <model_path> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š\n",
    "\n",
    "```python\n",
    "./hfd.sh solidrust/Mistral-7B-Instruct-v0.3-AWQ --tool aria2c -x 16\n",
    "```\n",
    "\n",
    "### GGUF\n",
    "\n",
    "ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½æŒ‡å®šæ¨¡å‹ï¼Œå‘½ä»¤éµå¾ª `./hfd.sh <model_path> --include <file_name> --tool aria2c -x <çº¿ç¨‹æ•°>`çš„æ ¼å¼ï¼š\n",
    "\n",
    "```python\n",
    "./hfd.sh bartowski/Mistral-7B-Instruct-v0.3-GGUF --include Mistral-7B-Instruct-v0.3-Q4_K_M.gguf --tool aria2c -x 16\n",
    "```\n",
    "\n",
    "ä¸‹è½½å®Œæˆä½ åº”è¯¥å¯ä»¥çœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š\n",
    "\n",
    "```\n",
    "Download Results:\n",
    "gid   |stat|avg speed  |path/URI\n",
    "======+====+===========+=======================================================\n",
    "145eba|OK  |   6.8MiB/s|./Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\n",
    "\n",
    "Status Legend:\n",
    "(OK):download completed.\n",
    "Downloaded https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf successfully.\n",
    "Download completed successfully.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dea4a5-9f06-40c1-b443-8d51b2be20d0",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f9a99-176d-429c-af1d-63a2cb46aee9",
   "metadata": {},
   "source": [
    "### ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085c448-afe2-451b-897c-68ed7ad131d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.4\n",
    "!pip install --upgrade transformers\n",
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3eb61-65f5-4043-9a37-41f1a7e9da1f",
   "metadata": {},
   "source": [
    "ä½ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœå®‰è£…ä¸æ­£ç¡®ï¼ŒGPTQ å°†æ— æ³•æ­£ç¡®ä½¿ç”¨ GPU è¿›è¡Œæ¨ç†ï¼Œä¹Ÿå°±æ˜¯è¯´æ— æ³•è¿›è¡ŒåŠ é€Ÿï¼Œå³ä¾¿ print(model.device) æ˜¾ç¤ºä¸º \"cuda\"ã€‚ç±»ä¼¼çš„é—®é¢˜è§ [Is This Inference Speed Slow?  #130](https://github.com/AutoGPTQ/AutoGPTQ/issues/130) å’Œ [CUDA extension not installed #694](https://github.com/AutoGPTQ/AutoGPTQ/issues/694)ã€‚\n",
    "\n",
    "è¿™ä¸ªé—®é¢˜æ˜¯æ™®éå­˜åœ¨çš„ï¼Œå½“ä½ ç›´æ¥ä½¿ç”¨ `pip install auto-gptq` è¿›è¡Œå®‰è£…æ—¶ï¼Œå¯èƒ½å°±ä¼šå‡ºç°ã€‚\n",
    "\n",
    "ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥å·²å®‰è£…çš„ç‰ˆæœ¬ï¼š\n",
    "\n",
    "```bash\n",
    "pip list | grep auto-gptq\n",
    "```\n",
    "\n",
    "å¦‚æœå‘ç°ä¹‹å‰å®‰è£…çš„ç‰ˆæœ¬ä¸å¸¦ cuda æ ‡è¯†ï¼Œå¸è½½å®ƒï¼Œä»æºç é‡æ–°è¿›è¡Œå®‰è£…ï¼ˆæ¨ç†é€Ÿåº¦å°†æå‡ä¸ºåŸæ¥çš„ 15 å€ä»¥ä¸Šï¼‰ã€‚\n",
    "\n",
    "```bash\n",
    "pip uninstall auto-gptq\n",
    "git clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ\n",
    "```\n",
    "\n",
    "```bash\n",
    "# ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä»»é€‰ä¸€ç§è¿›è¡Œå®‰è£…å³å¯ï¼Œç»æµ‹è¯•å‡æœ‰æ•ˆ\n",
    "pip install -vvv --no-build-isolation -e .\n",
    "# >> Successfully installed auto-gptq-0.8.0.dev0+cu121\n",
    "\n",
    "python setup.py install\n",
    "# >> Finished processing dependencies for auto-gptq==0.8.0.dev0+cu121\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e82aff-a3a9-4ea5-a510-f935628ac045",
   "metadata": {},
   "source": [
    "### GPTQ\n",
    "\n",
    "#### å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf71d3-8ea9-4cb7-b5f9-25bd1c2bc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74c5e0-8d78-4800-af07-290302b054fd",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä»‹ç»ä¸¤ç§å¯¼å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œå®é™…æ‰§è¡Œæ—¶æœ¬åœ°/è‡ªåŠ¨å¯¼å…¥äºŒé€‰ä¸€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d16b3-1d23-486f-bd32-a60d245d7d87",
   "metadata": {},
   "source": [
    "#### è®¾ç½®æ¨¡å‹è·¯å¾„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45027d9a-0f4a-4bf0-8416-9f20fb010825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœä½ å·²ç»é…ç½®è¿‡äº†ï¼Œå¯ä»¥ç›´æ¥åœ¨ Notebook ä¸­æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ä¸‹è½½ã€‚\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "!./hfd.sh neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit --tool aria2c -x 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ae2a7-fd89-4b8a-b7d9-b47a44c613be",
   "metadata": {},
   "source": [
    "å¦‚æœå·²ç»åœ¨æœ¬åœ°ä¸‹è½½äº†æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ¨¡å‹è·¯å¾„æ¥åŠ è½½æ¨¡å‹ã€‚ä»¥ä¸‹ç¤ºä¾‹å‡è®¾æ¨¡å‹ä½äºå½“å‰ç›®å½•çš„ `Mistral-7B-Instruct-v0.3-GPTQ-4bit` æ–‡ä»¶å¤¹ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7636b-e20d-4a38-880d-8915dd0b0216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„\n",
    "model_path = \"./Mistral-7B-Instruct-v0.3-GPTQ-4bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97d93f-4c5a-4f6d-b90c-9640e74a3a17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œè®¾ç½®è¿œç¨‹è·¯å¾„ï¼ˆ`id` + `/` + `model_name`ï¼‰ï¼Œå¯¼å…¥çš„æ—¶å€™ä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0d8bc-3948-4f99-be76-41c8858a7eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# æŒ‡å®šè¿œç¨‹æ¨¡å‹çš„è·¯å¾„\n",
    "model_path = \"neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5510a5-4d7a-40a5-8859-54bbf2557a4e",
   "metadata": {},
   "source": [
    "#### åŠ è½½æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f05f48-b9a7-48f5-996c-153227b93822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",  # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹\n",
    "    device_map=\"auto\"    # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„è®¾å¤‡ï¼ˆCPU/GPUï¼‰\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f31a6f-94e4-48bb-a8fd-78eac7b2f3af",
   "metadata": {},
   "source": [
    "#### æ¨ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25eaaa6-59e6-4a6f-a0ed-aa9db261cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¾“å…¥æ–‡æœ¬\n",
    "input_text = \"Hello, World!\"\n",
    "\n",
    "# å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡º\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=50,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# è§£ç ç”Ÿæˆçš„è¾“å‡º\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093ae9a-0049-4eb7-995d-78958aaa4004",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec1b61-708a-440e-9eee-91d360676bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autoawq autoawq-kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a036ce-ce60-4efe-aa9b-86ef1c1d9d61",
   "metadata": {},
   "source": [
    "#### å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97c440-7083-4a09-9203-4d4a4bec7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer, TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c534d8-3792-41a5-97ee-e7859e2aa094",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä»‹ç»ä¸¤ç§å¯¼å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œå®é™…æ‰§è¡Œæ—¶æœ¬åœ°/è‡ªåŠ¨å¯¼å…¥äºŒé€‰ä¸€ã€‚\n",
    "\n",
    "#### è®¾ç½®æ¨¡å‹è·¯å¾„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e43cbe-4282-4c05-bf6c-881d14e572a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å¦‚æœä½ å·²ç»é…ç½®è¿‡äº†ï¼Œå¯ä»¥ç›´æ¥åœ¨ Notebook ä¸­æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ä¸‹è½½ã€‚\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "!./hfd.sh solidrust/Mistral-7B-Instruct-v0.3-AWQ --tool aria2c -x 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cbbd2-03a1-4246-8b89-ae4b5e7d5363",
   "metadata": {},
   "source": [
    "å¦‚æœå·²ç»åœ¨æœ¬åœ°ä¸‹è½½äº†æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ¨¡å‹è·¯å¾„æ¥åŠ è½½æ¨¡å‹ã€‚ä»¥ä¸‹ç¤ºä¾‹å‡è®¾æ¨¡å‹ä½äºå½“å‰ç›®å½•çš„ `Mistral-7B-Instruct-v0.3-AWQ` æ–‡ä»¶å¤¹ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362c6e6-4a72-45a8-a51f-539550798f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„\n",
    "model_path = \"./Mistral-7B-Instruct-v0.3-AWQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136eca1-3f7f-4731-bf79-0ac659b5566d",
   "metadata": {},
   "source": [
    "å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œè®¾ç½®è¿œç¨‹è·¯å¾„ï¼ˆ`id` + `/` + `model_name`ï¼‰ï¼Œå¯¼å…¥çš„æ—¶å€™ä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c9108-fa94-4cf1-a26e-3b9e24ce29f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# æŒ‡å®šè¿œç¨‹æ¨¡å‹çš„è·¯å¾„\n",
    "model_path = \"solidrust/Mistral-7B-Instruct-v0.3-AWQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a752722-eb3f-4e15-aafa-b89ec8cb27d7",
   "metadata": {},
   "source": [
    "#### åŠ è½½æ¨¡å‹\n",
    "\n",
    "ä¸€äº›æƒé‡ä¸ä¼šè¢«åŠ è½½ï¼Œå¯¹äºå½“å‰ä»»åŠ¡æ¥è¯´è¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e052c-d8bb-41d2-bd0a-8113ec1597b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    model_path,\n",
    "    fuse_layers=True  # èåˆéƒ¨åˆ†æ¨¡å‹å±‚ä»¥æé«˜æ¨ç†é€Ÿåº¦\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909e888-42a1-4a3a-954a-c090bf1afdcf",
   "metadata": {},
   "source": [
    "#### æ¨ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf0b23-4328-439f-8265-c48f82ae2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è®¾å¤‡\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# è¾“å…¥æ–‡æœ¬\n",
    "input_text = \"Hello, World!\"\n",
    "\n",
    "# å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡º\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=50,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# è§£ç ç”Ÿæˆçš„è¾“å‡º\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f1f2d-6ed3-4caa-b3e4-04e8ac1cd452",
   "metadata": {},
   "source": [
    "### ç»Ÿä¸€æ–¹å¼åŠ è½½\n",
    "\n",
    "å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°äº† GPTQ å¹¶ä¸æ˜¯ä½¿ç”¨ Auto-GPTQ åº“ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨ Transformersï¼ˆè™½ç„¶èƒŒåä»ç„¶ä¾èµ–äº Auto-GPTQï¼‰ã€‚\n",
    "\n",
    "â€œè¦æ˜¯æ‰€æœ‰çš„æ¨¡å‹éƒ½èƒ½ç»Ÿä¸€å°±å¥½äº†ï¼Œè¿™æ ·å°±ä¸ç”¨æŸ¥é˜…å…¶ä»–åº“çš„æ–‡æ¡£äº†ã€‚â€\n",
    "\n",
    "äº‹å®ä¸Šï¼ŒAWQ ä¹Ÿå¯ä»¥é€šè¿‡ Transformers æ¥åŠ è½½ï¼ˆå¦‚æœä½ å¯¹å‚æ•°è®¾ç½®æ²¡æœ‰æ›´ç»†è‡´çš„è¦æ±‚ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23672cc5-e408-49c6-b0d9-12be2e06fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# æŒ‡å®šæ¨¡å‹ï¼Œåˆ‡æ¢ä¸ºå¯¹åº”çš„ GPTQ æˆ– AWQ æ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯è¿œç¨‹è·¯å¾„ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½\n",
    "model_path = \"neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit\"\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",  # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹\n",
    "    device_map=\"auto\"    # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„è®¾å¤‡ï¼ˆCPU/GPUï¼‰\n",
    ")\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# è¾“å…¥æ–‡æœ¬\n",
    "input_text = \"Hello, World!\"\n",
    "\n",
    "# å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡º\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=50,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# è§£ç ç”Ÿæˆçš„è¾“å‡º\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ea31a-a5d8-47c3-9e7c-5fe6ee5c290f",
   "metadata": {},
   "source": [
    "### äº†è§£æç¤ºè¯æ¨¡ç‰ˆï¼ˆprompt_templateï¼‰\n",
    "\n",
    "å…¶å®éå¸¸ç®€å•ï¼Œå°±æ˜¯æ›¾ç»æåˆ°çš„å ä½ç¬¦ï¼ˆä¸‹å›¾å¯¹äº `{{question}}` çš„åº”ç”¨ï¼‰ã€‚\n",
    "\n",
    "![å ä½ç¬¦](../Guide/assets/%E5%8D%A0%E4%BD%8D%E7%AC%A6-6055722.png)\n",
    "\n",
    "ä¸¾ä¸ªç›´è§‚çš„ä¾‹å­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a83d1-e986-4dae-a7b7-962e0444a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ Prompt Template\n",
    "prompt_template = \"é—®ï¼š{question}\\nç­”ï¼š\"\n",
    "\n",
    "# å®šä¹‰é—®é¢˜\n",
    "question = \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "# ä½¿ç”¨ Prompt Template ç”Ÿæˆå®Œæ•´çš„æç¤º\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(prompt)\n",
    "# print(\"\\n\")\n",
    "# print(f\"é—®ï¼š{question}\\nç­”ï¼š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05446cce-b0fa-4c49-a602-bcae318fd314",
   "metadata": {},
   "source": [
    "#### tokenizer.chat_template\n",
    "\n",
    "æŸ¥çœ‹æ¨¡å‹çš„ `chat_template`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc212d22-5316-4bf9-bc2d-58212e533b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å° chat_template ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰\n",
    "if hasattr(tokenizer, 'chat_template'):\n",
    "    print(tokenizer.chat_template)\n",
    "else:\n",
    "    print(\"Tokenizer æ²¡æœ‰ 'chat_template' å±æ€§ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9a3a9-cac7-47ea-890d-430b1ba20fd4",
   "metadata": {},
   "source": [
    "### æµå¼è¾“å‡º\n",
    "\n",
    "åœ¨é¡¹ç›®åˆæœŸè®¤è¯† API çš„æ—¶å€™ï¼Œæ–‡ç« [ã€Š01. åˆè¯† LLM APIï¼šç¯å¢ƒé…ç½®ä¸å¤šè½®å¯¹è¯æ¼”ç¤ºã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/01.%20åˆè¯†%20LLM%20APIï¼šç¯å¢ƒé…ç½®ä¸å¤šè½®å¯¹è¯æ¼”ç¤º.md#æµå¼è¾“å‡º)æœ‰æåˆ°è¿‡æµå¼è¾“å‡ºï¼Œè¿™æ˜¯æˆ‘ä»¬ä¸€ç›´ä»¥æ¥è§åˆ°çš„å¤§æ¨¡å‹è¾“å‡ºæ–¹å¼ï¼šé€å­—ï¼ˆtokenï¼‰æ‰“å°è€Œéç­‰å…¨éƒ¨ç”Ÿæˆå®Œæ‰“å°ã€‚\n",
    "\n",
    "æ‰§è¡Œä¸‹é¢çš„ä»£ç è¯•è¯•ï¼ˆæ— è®ºä¹‹å‰å¯¼å…¥çš„æ˜¯å“ªç§æ¨¡å‹ï¼Œéƒ½å¯ä»¥ç»§ç»­ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b6478-3937-46e7-9235-7f91c02bfedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# åˆ›å»º TextStreamer å®ä¾‹\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬\n",
    "    skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰\n",
    ")\n",
    "\n",
    "# å°†æç¤ºç¼–ç ä¸ºæ¨¡å‹è¾“å…¥\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "generation_kwargs = {\n",
    "    \"input_ids\": input_ids,  # æ¨¡å‹çš„è¾“å…¥ IDï¼Œæ³¨æ„ï¼Œè¿™ä¸æ˜¯ Embedding\n",
    "    \"max_length\": 200,       # ç”Ÿæˆçš„æœ€å¤§ token æ•°\n",
    "    \"streamer\": streamer,    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬\n",
    "    \"pad_token_id\": tokenizer.eos_token_id  # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤ open-end è­¦å‘Š\n",
    "}\n",
    "\n",
    "# å¼€å§‹ç”Ÿæˆæ–‡æœ¬\n",
    "with torch.no_grad():\n",
    "    # ** æ˜¯ Python ä¸­çš„è§£åŒ…æ“ä½œç¬¦ï¼Œå®ƒå°†å­—å…¸ä¸­çš„é”®å€¼å¯¹è§£åŒ…ä¸ºå‡½æ•°çš„å…³é”®å­—å‚æ•°ã€‚\n",
    "    # åœ¨è¿™é‡Œï¼Œ**generation_kwargs å°†å­—å…¸ä¸­çš„å‚æ•°é€ä¸€ä¼ é€’ç»™ model.generate() æ–¹æ³•ï¼Œ\n",
    "    # ç­‰æ•ˆäºç›´æ¥å†™å‡ºæ‰€æœ‰å‚æ•°ï¼š\n",
    "    # model.generate(input_ids=input_ids, max_length=200, do_sample=True, ...)\n",
    "    # ä½ éœ€è¦æ³¨æ„åˆ°ï¼Œè¿™å’Œä¹‹å‰é‡‡ç”¨äº†ä¸åŒçš„ä¼ å‚æ–¹å¼ï¼Œä½†æœ¬è´¨æ˜¯ä¸€æ ·çš„ã€‚\n",
    "    # åœ¨åç»­çš„æ•™ç¨‹ä¸­ï¼Œä¼šè¾ƒå°‘åœ°ä½¿ç”¨è¿™ç§æ–¹å¼è¿›è¡Œä¼ å‚ã€‚\n",
    "    # å› ä¸ºè¿™å¾ˆå¥½çš„åˆ†ç¦»äº†å‚æ•°ï¼Œæ‰€ä»¥ä¹Ÿå¢åŠ äº†ä¹ä¸€çœ‹ä¹‹ä¸‹çš„æŠ½è±¡åº¦ï¼Œä¸ºäº†åˆè§çš„ç›´è§‚ï¼Œå°†å‡å°‘ä½¿ç”¨ã€‚\n",
    "    model.generate(**generation_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b235a01-1780-4573-b49e-3d889b0d4b76",
   "metadata": {},
   "source": [
    "### å•è½®å¯¹è¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aae13-8ba2-4654-8825-db9e97dd17cc",
   "metadata": {},
   "source": [
    "ï¼ˆå¦‚æœé‡æ–°å¯åŠ¨å†…æ ¸çš„è¯ï¼Œéµå¾ª `å¯¼å…¥åº“`-> `å¯¼å…¥æ¨¡å‹` -> `å½“å‰ä»£ç å—` çš„é¡ºåºæ‰§è¡Œã€‚ï¼‰\n",
    "\n",
    "è®©æˆ‘ä»¬ç›´æ¥è®¾è®¡ `messages`ï¼Œå¹¶åº”ç”¨ `chat_template` è¿›è¡Œå¯¹è¯ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da6253-3280-4761-aa07-1b6b39336a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# å®šä¹‰è¾“å…¥\n",
    "prompt = input(\"User: \")\n",
    "\n",
    "# å®šä¹‰æ¶ˆæ¯åˆ—è¡¨\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# ä½¿ç”¨ tokenizer.apply_chat_template() ç”Ÿæˆæ¨¡å‹è¾“å…¥\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# åˆ›å»º TextStreamer å®ä¾‹\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬\n",
    "    skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰\n",
    ")\n",
    "\n",
    "# è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "generation_kwargs = {\n",
    "    \"input_ids\": input_ids,  # æ¨¡å‹çš„è¾“å…¥ IDï¼Œæ³¨æ„ï¼Œè¿™ä¸æ˜¯ Embedding\n",
    "    \"max_length\": 500,      # ç”Ÿæˆçš„æœ€å¤§ token æ•°\n",
    "    \"streamer\": streamer,    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬\n",
    "    \"pad_token_id\": tokenizer.eos_token_id  # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤ open-end è­¦å‘Š\n",
    "}\n",
    "\n",
    "# å¼€å§‹ç”Ÿæˆæ–‡æœ¬\n",
    "with torch.no_grad():\n",
    "    model.generate(**generation_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bbef9-d9c7-4840-b6f0-8b042a811a2c",
   "metadata": {},
   "source": [
    "### å¤šè½®å¯¹è¯\n",
    "\n",
    "å¦‚æœé‡æ–°å¯åŠ¨å†…æ ¸çš„è¯ï¼Œéµå¾ª `å¯¼å…¥åº“`-> `å¯¼å…¥æ¨¡å‹` -> `å½“å‰ä»£ç å—` çš„é¡ºåºæ‰§è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea07ea5-e61d-43a9-abee-6aab5322cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# åˆå§‹åŒ–å¯¹è¯å†å²\n",
    "messages = []\n",
    "\n",
    "# å¼€å§‹å¤šè½®å¯¹è¯\n",
    "while True:\n",
    "    # è·å–è¾“å…¥\n",
    "    prompt = input(\"User: \")\n",
    "    \n",
    "    # é€€å‡ºå¯¹è¯æ¡ä»¶ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ç»ˆæ­¢ä»£ç å—ï¼‰\n",
    "    if prompt.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # å°†è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # ä½¿ç”¨ tokenizer.apply_chat_template() ç”Ÿæˆæ¨¡å‹è¾“å…¥\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # åˆ›å»º TextStreamer å®ä¾‹\n",
    "    streamer = TextStreamer(\n",
    "        tokenizer, \n",
    "        skip_prompt=True,         # åœ¨è¾“å‡ºæ—¶è·³è¿‡è¾“å…¥çš„æç¤ºéƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬\n",
    "        skip_special_tokens=True  # å¿½ç•¥ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ¯”å¦‚ <pad> / <eos> ...ï¼‰\n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids,                  # æ¨¡å‹çš„è¾“å…¥ ID\n",
    "        \"max_length\": input_ids.shape[1] + 500,  # ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼Œinput_ids.shape[1] å³è¾“å…¥å¯¹åº”çš„ tokens æ•°é‡\n",
    "        \"streamer\": streamer,                    # ä½¿ç”¨ TextStreamer å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥è¾“å‡ºæ–‡æœ¬\n",
    "        \"pad_token_id\": tokenizer.eos_token_id   # é»˜è®¤è¡Œä¸ºï¼Œæ¶ˆé™¤è­¦å‘Š\n",
    "    }\n",
    "    \n",
    "    # å¼€å§‹ç”Ÿæˆå›å¤\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**generation_kwargs)\n",
    "    \n",
    "    # è·å–ç”Ÿæˆçš„å›å¤æ–‡æœ¬\n",
    "    assistant_reply = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # å°†æ¨¡å‹çš„å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_reply})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a66a9f-776d-4d6e-81d5-77c6a60525c0",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå°å‘ï¼Œä½ ä¸èƒ½ç®€å•ä½¿ç”¨ `output_ids[0]` æ¥ä¿å­˜å›å¤ï¼Œå› ä¸º`output_ids` ä¸­å®é™…ä¸ŠåŒ…å«äº† `input_ids`ï¼Œæ‰“å°å®ƒä»¬ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f7cc7-1c07-4715-bbed-386d06ad77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c70b9-bf14-46ad-acc8-8bd493c31a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
