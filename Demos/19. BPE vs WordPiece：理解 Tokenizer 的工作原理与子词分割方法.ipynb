{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d293fb3-7b27-4c56-ad0e-1d681360265f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# BPE vs WordPieceï¼šç†è§£ Tokenizer çš„å·¥ä½œåŸç†ä¸å­è¯åˆ†å‰²æ–¹æ³•\n",
    "\n",
    "> å¼•å¯¼æ–‡ç« ï¼š[21. BPE vs WordPieceï¼šç†è§£ Tokenizer çš„å·¥ä½œåŸç†ä¸å­è¯åˆ†å‰²æ–¹æ³•](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/21.%20BPE%20vs%20WordPieceï¼šç†è§£%20Tokenizer%20çš„å·¥ä½œåŸç†ä¸å­è¯åˆ†å‰²æ–¹æ³•.md)\n",
    ">\n",
    "> å½“å‰ä»£ç æ–‡ä»¶å®Œå…¨é•œåƒäº†æ–‡ç« çš„å†…å®¹ï¼Œå¯ä»¥ä»…é˜…è¯»è¯¥æ–‡ä»¶ã€‚\n",
    "\n",
    "åœ¨çº¿é“¾æ¥ï¼š[Kaggle](https://www.kaggle.com/code/aidemos/19-bpe-vs-wordpiece-tokenizer) | [Colab](https://colab.research.google.com/drive/1J6QN0QbuoWBDIIrBe-TJ6Hi5rnzTSovM?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0eb806-1599-4d18-9bf5-ed75c7220bd8",
   "metadata": {},
   "source": [
    "> åœ¨åº”ç”¨çš„è·¯ä¸Šâ€œè’™ç€å¤´â€èµ°äº†ä¸€æ®µï¼Œæ˜¯æ—¶å€™å›è¿‡å¤´æ¥ç†è§£å…¶ä¸­çš„å·¥ä½œåŸç†äº†ã€‚\n",
    ">\n",
    "> æ–‡ç« å°†ä»¥æ–‡æœ¬å¤„ç†ä¸ºä¾‹ï¼Œä»‹ç»æ•°æ®é¢„å¤„ç†ä¸­çš„å…³é”®ç»„ä»¶â€”â€”**Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰**ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæ˜¯åæ¦‚å¿µæ€§çš„è®²è§£ï¼Œä¸ä¼šæ·±å…¥å…·ä½“å‡½æ•°çš„å‚æ•°ç»†èŠ‚ã€‚\n",
    ">\n",
    "> ã€Œæ„é€ è¯æ±‡è¡¨ã€éƒ¨åˆ†å°†ä»‹ç»ä¸¤ç§å¸¸è§çš„å­è¯åˆ†å‰²æ–¹æ³•ï¼š\n",
    ">\n",
    "> - **BPEï¼ˆByte-Pair Encodingï¼‰**ï¼šç”¨äº GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTa ç­‰æ¨¡å‹ã€‚\n",
    "> - **WordPiece**ï¼šç”¨äº DistilBERTã€MobileBERTã€Funnel Transformers å’Œ MPNET ç­‰æ¨¡å‹ã€‚\n",
    ">\n",
    "> ã€Œæ‹“å±•ã€éƒ¨åˆ†å°†æ¶‰åŠä¸¤ä¸ªé‡è¦æ¦‚å¿µï¼š\n",
    ">\n",
    "> - **æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰**\n",
    "> - **è¯å…ƒç±»å‹ ID ï¼ˆToken Type IDsï¼‰**\n",
    ">\n",
    "> å·¥å…·ï¼š[Tiktokenizerï¼ˆæ¨èï¼‰](https://tiktokenizer.vercel.app) | [The Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)\n",
    "\n",
    "\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯ Tokenizerï¼Ÿ\n",
    "\n",
    "**Tokenizer**ï¼ˆåˆ†è¯å™¨ï¼‰å¯ä»¥å°†åŸå§‹æ–‡æœ¬ï¼ˆraw textï¼‰è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—åºåˆ—ï¼Œåœ¨æ¨¡å‹è¾“å…¥å’Œè¾“å‡ºçš„ä¸¤ä¸ªä¸»è¦é˜¶æ®µä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼š\n",
    "\n",
    "### æ¨¡å‹è¾“å…¥ï¼ˆç¼–ç  Encodeï¼‰é˜¶æ®µ\n",
    "\n",
    "1. **åˆ†è¯ï¼ˆTokenizeï¼‰**\n",
    "\n",
    "   å°†æ–‡æœ¬æ‹†åˆ†ä¸ºè¯å…ƒï¼ˆTokenï¼‰ï¼Œå¸¸è§çš„åˆ†è¯æ–¹å¼åŒ…æ‹¬å­—çº§ã€è¯çº§ã€å­è¯çº§ï¼ˆå¦‚ BPEã€WordPieceï¼‰ã€ç©ºæ ¼åˆ†è¯ç­‰ã€‚\n",
    "\n",
    "   ```sql\n",
    "   è¾“å…¥: \"ä½ å¥½\"\n",
    "   åˆ†è¯: [\"ä½ \", \"å¥½\"]\n",
    "   ```\n",
    "\n",
    "2. **æ˜ å°„ï¼ˆMappingï¼‰**\n",
    "\n",
    "   å°†æ¯ä¸ªè¯å…ƒæ˜ å°„ä¸ºè¯æ±‡è¡¨ä¸­çš„å”¯ä¸€ IDï¼Œç”Ÿæˆçš„æ•°å­—åºåˆ—å³ä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚\n",
    "\n",
    "   ```sql\n",
    "   åˆ†è¯: [\"ä½ \", \"å¥½\"]\n",
    "   æ˜ å°„: [1001, 1002]\n",
    "   ```\n",
    "\n",
    "### æ¨¡å‹è¾“å‡ºï¼ˆè§£ç  Decodeï¼‰é˜¶æ®µ\n",
    "\n",
    "1. **åæ˜ å°„ï¼ˆDe-mappingï¼‰**\n",
    "\n",
    "   æ¨¡å‹è¾“å‡ºçš„æ•°å­—åºåˆ—é€šè¿‡è¯æ±‡è¡¨æ˜ å°„å›å¯¹åº”çš„è¯å…ƒï¼ŒäºŒè€…æ˜¯ä¸€ä¸€å¯¹åº”çš„å…³ç³»ã€‚\n",
    "\n",
    "   ```sql\n",
    "   è¾“å‡º: [1001, 1002]\n",
    "   åæ˜ å°„: [\"ä½ \", \"å¥½\"]\n",
    "   ```\n",
    "\n",
    "2. **æ–‡æœ¬é‡ç»„**\n",
    "\n",
    "   å°†è§£ç åçš„è¯å…ƒä»¥æŸç§è§„åˆ™é‡æ–°æ‹¼æ¥ä¸ºå®Œæ•´æ–‡æœ¬ã€‚\n",
    "\n",
    "   ```sql\n",
    "   åæ˜ å°„: [\"ä½ \", \"å¥½\"]\n",
    "   é‡ç»„: \"ä½ å¥½\"\n",
    "   ```\n",
    "\n",
    "### ç›´è§‚æ„Ÿå—\n",
    "\n",
    "è®¿é—® [Tiktokenizer](https://tiktokenizer.vercel.app)ï¼Œé€šè¿‡å³ä¸Šè§’é€‰å–ä¸åŒçš„ Tokenizer è¿›è¡Œå°è¯•ï¼š\n",
    "\n",
    "![image-20241022152315606](../Guide/assets/image-20241022152315606.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3a399-48ba-4f1f-aa7b-68e69a9b4e74",
   "metadata": {},
   "source": [
    "## å®é™…ä½¿ç”¨\n",
    "åœ¨è¿›ä¸€æ­¥è®²è§£ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡ **Transformers** åº“ä¸­çš„ `AutoTokenizer` ç±»æ¥ä½¿ç”¨ Tokenizerã€‚\n",
    "\n",
    "### å®‰è£…åº“\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf279482-ba17-48ef-9aaf-cd447e6249b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d1a72-a07e-4394-a448-746e74b0e4b0",
   "metadata": {},
   "source": [
    "### BPE åˆ†è¯å™¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81d62a7-97cc-43a4-8df2-2827fa3cfc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', ',', 'Ä world', '!']\n",
      "Token IDs: [15496, 11, 995, 0]\n",
      "Tokens: ['Hello', ',', 'Ä world', '!']\n",
      "Decoded Text: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ä½¿ç”¨ GPT-2 çš„åˆ†è¯å™¨ï¼ˆBPEï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# ç¼–ç \n",
    "# 1. å°†æ–‡æœ¬åˆ†è¯ä¸º Tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. å°† Tokens è½¬æ¢ä¸º Token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# è§£ç \n",
    "# 1. Token IDs è½¬æ¢ä¸º Tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. Tokens æ‹¼æ¥ä¸ºæ–‡æœ¬\n",
    "decoded_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86bd1e-875c-4949-8a35-547e85b02951",
   "metadata": {},
   "source": [
    "### WordPiece åˆ†è¯å™¨ç¤ºä¾‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfaafd8-12e2-48b7-8a60-c3e53d4d48f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', ',', 'world', '!']\n",
      "Token IDs: [7592, 1010, 2088, 999]\n",
      "Tokens: ['hello', ',', 'world', '!']\n",
      "Decoded Text: hello, world!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ä½¿ç”¨ BERT çš„åˆ†è¯å™¨ï¼ˆWordPieceï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# ç¼–ç \n",
    "# 1. å°†æ–‡æœ¬åˆ†è¯ä¸º Tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. å°† Tokens è½¬æ¢ä¸º Token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# è§£ç \n",
    "# 1. Token IDs è½¬æ¢ä¸º Tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. Tokens æ‹¼æ¥ä¸ºæ–‡æœ¬\n",
    "decoded_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9ae48-58e4-4e96-b7b4-947f00ec146a",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ `encode()` å’Œ `decode()` æ–¹æ³•\n",
    "\n",
    "æ›´ç®€æ´ä¸”å¸¸è§çš„ä½¿ç”¨æ–¹å¼æ˜¯ç›´æ¥ä½¿ç”¨ `encode()` å’Œ `decode()` æ–¹æ³•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cab755-c719-4742-a5d4-35e5dc4a7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [15496, 11, 995, 0]\n",
      "Decoded Text: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# å–æ¶ˆæ³¨é‡Šä»¥å¯¹æ¯”ä¸¤ç§åˆ†è¯å™¨çš„è¾“å‡ºå·®å¼‚\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# ä½¿ç”¨ encode() å°†æ–‡æœ¬ç›´æ¥è½¬æ¢ä¸º Token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# ä½¿ç”¨ decode() å°† Token IDs è½¬æ¢å›æ–‡æœ¬\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c9cd4-66cd-4601-b938-e48cc814d5f9",
   "metadata": {},
   "source": [
    "## äº†è§£ Tokenizer çš„åŸºç¡€å±æ€§\n",
    "\n",
    "å¯¼å…¥åˆ†è¯å™¨åï¼Œå¯ä»¥é€‰æ‹©æŸ¥çœ‹ä¸€äº›å±æ€§æ¥è·å¾—ç›´è§‚çš„ç†è§£ï¼Œä¾‹å¦‚æŸ¥çœ‹è¯æ±‡è¡¨ã€ç‰¹æ®Šæ ‡è®°ç­‰ï¼Œä»¥ GPT-2 ä¸ºä¾‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9345315-83db-484a-9945-360d26f8ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 50257\n"
     ]
    }
   ],
   "source": [
    "# è·å–è¯æ±‡è¡¨å¤§å°\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107665fe-51d3-4f65-8f5a-1aa3e4aeace5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ä ban': 3958,\n",
       " 'Ä sorely': 50103,\n",
       " 'Ä speaks': 9209,\n",
       " 'nih': 37373,\n",
       " 'Cong': 18649,\n",
       " 'usercontent': 43667,\n",
       " 'rax': 32040,\n",
       " 'Ä DPS': 25366,\n",
       " 'dies': 25990,\n",
       " 'Ä Government': 5070,\n",
       " 'Ang': 13450,\n",
       " 'annon': 8825,\n",
       " 'Ä Nathan': 18106,\n",
       " 'Ä Slash': 26616,\n",
       " 'ophical': 49256,\n",
       " 'Ä 450': 18523,\n",
       " 'Ä dystopian': 49483,\n",
       " 'edes': 37507,\n",
       " 'Ä api': 40391,\n",
       " 'Ä leans': 39416,\n",
       " 'Ä situations': 7445,\n",
       " 'Ä colorful': 20239,\n",
       " 'Ä upper': 6727,\n",
       " 'Ä vouchers': 41262,\n",
       " 'Ä wellbeing': 40013,\n",
       " 'Ä Ã°ÅÄ³': 50169,\n",
       " 'action': 2673,\n",
       " 'udeb': 46092,\n",
       " 'Ä Kak': 31250,\n",
       " 'Ä formations': 30648,\n",
       " 'Ä specifies': 26052,\n",
       " 'Ä Meredith': 40378,\n",
       " 'through': 9579,\n",
       " 'Ä Housing': 16797,\n",
       " 'Ä Marathon': 24828,\n",
       " 'Ä Recent': 22926,\n",
       " 'Ä Plain': 28847,\n",
       " 'Ä Prosecutor': 37478,\n",
       " 'Ä Romeo': 43989,\n",
       " 'Ä Ste': 2441,\n",
       " 'Ä arbitrary': 14977,\n",
       " 'Ä stabilized': 44945,\n",
       " 'LINE': 24027,\n",
       " 'Ä Ã‚Â«': 21110,\n",
       " 'Ä referral': 31413,\n",
       " 'ento': 50217,\n",
       " 'payer': 34987,\n",
       " 'af': 1878,\n",
       " 'Ä Hector': 42621,\n",
       " 'Ä NUM': 36871,\n",
       " 'Ä Spo': 49331,\n",
       " 'Ä installment': 25168,\n",
       " 'Chuck': 44324,\n",
       " 'Hide': 38518,\n",
       " 'ying': 1112,\n",
       " '73': 4790,\n",
       " 'Ä Liber': 5261,\n",
       " 'Ä 414': 45900,\n",
       " 'Awesome': 49061,\n",
       " 'UN': 4944,\n",
       " 'ption': 1159,\n",
       " 'Ä Rear': 30144,\n",
       " 'Ä goats': 35199,\n",
       " 'Ä Premier': 9952,\n",
       " 'Ä NS': 10896,\n",
       " 'Ä obsessive': 36681,\n",
       " 'has': 10134,\n",
       " 'Ä stretched': 19110,\n",
       " 'Ä waning': 50073,\n",
       " 'Ä Mits': 22424,\n",
       " 'Ä 253': 32056,\n",
       " 'launch': 35681,\n",
       " 'phasis': 28432,\n",
       " 'Ä Samson': 47977,\n",
       " 'ban': 3820,\n",
       " 'Boy': 26554,\n",
       " 'MS': 5653,\n",
       " 'Nazis': 44527,\n",
       " 'Ä Fn': 37481,\n",
       " 'TL': 14990,\n",
       " 'animous': 45873,\n",
       " 'istant': 10167,\n",
       " 'ERT': 17395,\n",
       " 'Ä Dome': 31390,\n",
       " 'Ä buses': 16893,\n",
       " '277': 27019,\n",
       " '=\"#': 25698,\n",
       " 'sche': 15952,\n",
       " 'Ä Said': 23885,\n",
       " 'Ä Greece': 10315,\n",
       " 'Ä communities': 5348,\n",
       " 'Ä infographic': 48033,\n",
       " 'Ä Horizons': 39519,\n",
       " 'Ä 51': 6885,\n",
       " 'fruit': 34711,\n",
       " 'Ä ------': 40103,\n",
       " 'Ä Age': 7129,\n",
       " 'Ä Kyr': 42748,\n",
       " 'Ä grass': 8701,\n",
       " 'Ä loaded': 9639,\n",
       " 'accept': 13635,\n",
       " 'WER': 45532,\n",
       " 'aching': 8103,\n",
       " 'olla': 33011,\n",
       " 'Ä marking': 18730,\n",
       " 'Ä sane': 33241,\n",
       " 'Ä stacked': 24167,\n",
       " 'Ä 1984': 12844,\n",
       " 'Ä Duncan': 18625,\n",
       " 'Ä Femin': 25832,\n",
       " 'Ä authorized': 10435,\n",
       " 'PLE': 16437,\n",
       " 'Ä canon': 18061,\n",
       " 'Event': 9237,\n",
       " 'meter': 27231,\n",
       " 'alter': 47653,\n",
       " 'imaru': 49551,\n",
       " 'Ã¤Â¸Ä«': 49011,\n",
       " 'Ä Bring': 24347,\n",
       " 'Ä leakage': 47988,\n",
       " 'Enhanced': 49026,\n",
       " 'cludes': 13955,\n",
       " 'Ä Approximately': 40453,\n",
       " 'illy': 6548,\n",
       " 'hig': 25196,\n",
       " 'inoa': 40564,\n",
       " 'within': 33479,\n",
       " 'btn': 46118,\n",
       " 'xd': 24954,\n",
       " '385': 27203,\n",
       " 'Ä INFO': 24890,\n",
       " 'Ä Easy': 16789,\n",
       " 'Ä Plat': 32715,\n",
       " 'Ä meet': 1826,\n",
       " 'Ä noisy': 31210,\n",
       " 'Ä phone': 3072,\n",
       " 'abling': 11716,\n",
       " 'modified': 41771,\n",
       " 'Ä BMW': 19339,\n",
       " 'Ä 1926': 38525,\n",
       " 'Ä Tue': 30030,\n",
       " 'Weak': 44898,\n",
       " 'Ä Cheng': 27692,\n",
       " 'hen': 831,\n",
       " 'Ä Engine': 7117,\n",
       " 'Ä Kurds': 23880,\n",
       " 'Ä Mac': 4100,\n",
       " 'Ä bottom': 4220,\n",
       " 'Ä coup': 12092,\n",
       " 'mitter': 37974,\n",
       " 'Ã¤Â½': 19526,\n",
       " 'Ä Ele': 15987,\n",
       " 'Ä juxtap': 45273,\n",
       " 'Ä strategist': 25651,\n",
       " 'Rock': 19665,\n",
       " 'Ä uterus': 41303,\n",
       " 'Ä EP': 14724,\n",
       " 'Ä omission': 35725,\n",
       " 'Ä Clone': 30698,\n",
       " 'Ä alcohol': 5548,\n",
       " 'urion': 40956,\n",
       " 'Ä failures': 15536,\n",
       " 'Ä turrets': 41104,\n",
       " 'Ä get': 651,\n",
       " 'for': 1640,\n",
       " 'Ä 320': 20959,\n",
       " 'Ä disabilities': 19358,\n",
       " '025': 36629,\n",
       " 'otions': 36083,\n",
       " 'Count': 12332,\n",
       " 'GAME': 47109,\n",
       " 'interested': 34339,\n",
       " 'Ä interestingly': 50226,\n",
       " 'Ä patrols': 34141,\n",
       " 'Ä subdued': 38759,\n",
       " 'Ä Flint': 21660,\n",
       " 'Ä Through': 9561,\n",
       " 'Ä shaky': 35335,\n",
       " 'Ä switches': 18225,\n",
       " 'ippy': 41214,\n",
       " 'Am': 5840,\n",
       " 'Ä convictions': 19131,\n",
       " 'Ä Carolina': 5913,\n",
       " 'Ä Founding': 44593,\n",
       " 'Ä Serpent': 30177,\n",
       " 'Ä Kel': 15150,\n",
       " 'Ä ICC': 32300,\n",
       " '???': 28358,\n",
       " 'Ä infamous': 16526,\n",
       " 'Ä roundup': 48390,\n",
       " '024': 40839,\n",
       " 'Ä boobs': 41050,\n",
       " 'Ä Rex': 17853,\n",
       " 'ifully': 17049,\n",
       " 'Ä Captain': 8599,\n",
       " 'Ä stopp': 43804,\n",
       " 'Fred': 30847,\n",
       " 'Ä quar': 36343,\n",
       " 'Ä Suzuki': 35807,\n",
       " 'iable': 3379,\n",
       " 'Ä nations': 7027,\n",
       " '...': 986,\n",
       " 'Ä Oblivion': 34181,\n",
       " 'Ä constructed': 12006,\n",
       " 'Ä Verge': 44499,\n",
       " 'Ä purchaser': 39122,\n",
       " 'Ä Bog': 21555,\n",
       " 'Ä Recap': 46585,\n",
       " 'athering': 25545,\n",
       " 'Admin': 46787,\n",
       " 'Ä Coal': 12896,\n",
       " 'Ä parental': 21694,\n",
       " 'Ä pirates': 27516,\n",
       " 'Studies': 45833,\n",
       " 'Ä sophisticated': 13767,\n",
       " 'event': 15596,\n",
       " 'rogram': 39529,\n",
       " 'Ä abl': 46624,\n",
       " 'Ä disl': 19621,\n",
       " 'Ä ubiquitous': 27888,\n",
       " 'Ä½': 249,\n",
       " 'Ä ACLU': 24381,\n",
       " 'Und': 31319,\n",
       " 'avier': 19492,\n",
       " 'Ä something': 1223,\n",
       " 'Ni': 34153,\n",
       " 'context': 22866,\n",
       " 'Ä Virus': 40584,\n",
       " 'Ä Loading': 12320,\n",
       " 'Ä Marvin': 35105,\n",
       " 'Brend': 48015,\n",
       " 'Ä exploited': 21514,\n",
       " 'Ä cube': 23441,\n",
       " 'Ä motivating': 46891,\n",
       " 'aturation': 36921,\n",
       " 'esm': 45798,\n",
       " 'ODUCT': 28644,\n",
       " 'Ä Gren': 19674,\n",
       " 'itchie': 48423,\n",
       " 'Ä Elections': 24473,\n",
       " 'Ã£Ä£Å‚': 46777,\n",
       " 'Ä athleticism': 35159,\n",
       " 'Ä bathing': 39153,\n",
       " 'Ä blocks': 7021,\n",
       " 'inguished': 46709,\n",
       " 'Ä Hass': 20300,\n",
       " 'Ä complaint': 8224,\n",
       " 'Ä disturbed': 24069,\n",
       " 'Ä imper': 11071,\n",
       " 'Ä sincere': 17082,\n",
       " 'Ä Interesting': 43580,\n",
       " 'Ä enabling': 15882,\n",
       " 'Ä yards': 5695,\n",
       " 'Ä¸Ä¼': 31204,\n",
       " 'VE': 6089,\n",
       " 'ibaba': 37541,\n",
       " 'Ä Has': 7875,\n",
       " 'itions': 1756,\n",
       " 'oyer': 35301,\n",
       " 'Ä Sah': 22982,\n",
       " 'Ä dedicated': 7256,\n",
       " 'Ä expand': 4292,\n",
       " 'Ä pitchers': 25259,\n",
       " 'arantine': 37996,\n",
       " 'ador': 7079,\n",
       " 'utt': 15318,\n",
       " 'Ä Darling': 49825,\n",
       " 'Ä Erie': 40911,\n",
       " 'Ä Hispanic': 16949,\n",
       " 'Ä Ox': 10736,\n",
       " 'neck': 27235,\n",
       " 'Ä Vanessa': 42100,\n",
       " 'Ä wind': 2344,\n",
       " 'Ä Wrestling': 29662,\n",
       " 'ether': 6750,\n",
       " 'Ä Imam': 38386,\n",
       " 'Ä Phillip': 29470,\n",
       " 'Ä Ride': 21640,\n",
       " 'Ä correlate': 39684,\n",
       " 'Ä tropical': 19690,\n",
       " 'trip': 39813,\n",
       " 'Ä Ars': 24230,\n",
       " 'Ä specialists': 22447,\n",
       " 'Ä unite': 24558,\n",
       " 'Ä Benef': 19899,\n",
       " 'Ä enacted': 17814,\n",
       " 'uded': 19289,\n",
       " 'Ä coroner': 39723,\n",
       " 'Ä practition': 17629,\n",
       " 'OST': 10892,\n",
       " 'independent': 34750,\n",
       " 'otic': 6210,\n",
       " 'Ä Topic': 47373,\n",
       " 'Ä crammed': 48384,\n",
       " 'Ä premature': 19905,\n",
       " 'ittees': 13263,\n",
       " 'Ä Ferr': 19130,\n",
       " 'Ä Sed': 22710,\n",
       " 'mot': 27926,\n",
       " 'Ä Autob': 41735,\n",
       " 'Ä Commentary': 45465,\n",
       " 'Ä Papa': 42328,\n",
       " 'Ä balcony': 29780,\n",
       " 'ilee': 40626,\n",
       " 'Ä Learns': 30667,\n",
       " 'Ä dances': 38207,\n",
       " 'Ä demonstrators': 25016,\n",
       " 'atan': 39036,\n",
       " 'Ä formerly': 15734,\n",
       " 'Ä Craft': 15745,\n",
       " 'otton': 11324,\n",
       " 'Ä professors': 20339,\n",
       " 'Ä raid': 9513,\n",
       " 'Ä thumbs': 32766,\n",
       " 'Ä underwent': 25289,\n",
       " '(': 7,\n",
       " 'inges': 26792,\n",
       " 'erick': 41556,\n",
       " 'Ä trooper': 41967,\n",
       " 'Sher': 28782,\n",
       " 'Ä rookie': 12302,\n",
       " 'Ä scheduled': 7530,\n",
       " 'Ä Calories': 45133,\n",
       " 'Ä pedest': 14238,\n",
       " 'backed': 17078,\n",
       " 'Ä intoxicated': 35344,\n",
       " 'Ä 284': 40654,\n",
       " 'Japanese': 25324,\n",
       " 'Ä Cambridge': 14457,\n",
       " 'Ä Nightmare': 23951,\n",
       " '********': 4557,\n",
       " 'Ä Rated': 49949,\n",
       " 'Ä continue': 2555,\n",
       " 'Ä Serial': 23283,\n",
       " 'Ä hijacked': 41554,\n",
       " 'Ä hog': 40476,\n",
       " 'Ä order': 1502,\n",
       " 'Ä urged': 11643,\n",
       " 'ulz': 37314,\n",
       " 'Ä all': 477,\n",
       " 'Ä fishing': 12478,\n",
       " 'Ä Bapt': 18226,\n",
       " 'Ä Cabin': 16804,\n",
       " 'aliation': 22885,\n",
       " 'creation': 38793,\n",
       " 'Fran': 38848,\n",
       " 'Ä Ethnic': 48021,\n",
       " 'Ä frying': 45366,\n",
       " 'Ä problematic': 15833,\n",
       " 'Ä Castle': 11312,\n",
       " 'Ä Neil': 15929,\n",
       " 'onut': 16478,\n",
       " 'Ä whopping': 27833,\n",
       " 'ompl': 6316,\n",
       " 'Ä mushrooms': 23452,\n",
       " 'yright': 4766,\n",
       " '\":-': 48219,\n",
       " 'effective': 16803,\n",
       " 'essler': 33730,\n",
       " 'Ä general': 2276,\n",
       " 'Ä Sexual': 19536,\n",
       " 'Ä Zan': 47022,\n",
       " 'Ä random': 4738,\n",
       " 'Ä segregated': 38135,\n",
       " 'ÃƒÂª': 25792,\n",
       " 'Education': 41183,\n",
       " 'Ä dormant': 41038,\n",
       " 'Ä praying': 26002,\n",
       " 'Ä signalling': 45829,\n",
       " 'Take': 12322,\n",
       " 'University': 21009,\n",
       " 'nsic': 19364,\n",
       " 'rupted': 31590,\n",
       " 'brates': 44835,\n",
       " 'Ä Sheet': 21616,\n",
       " 'source': 10459,\n",
       " 'Ä UNHCR': 49558,\n",
       " 'Ä revolution': 5854,\n",
       " 'Ä vacuum': 17076,\n",
       " 'Ä Optional': 32233,\n",
       " '205': 21261,\n",
       " 'Application': 23416,\n",
       " 'Ä SIGN': 36771,\n",
       " 'Ä democrat': 43268,\n",
       " 'Ä Mastery': 37799,\n",
       " 'Ä Morris': 14433,\n",
       " 'Ä discriminatory': 27200,\n",
       " 'Ä oscill': 24969,\n",
       " 'Ä appreciate': 9144,\n",
       " 'Ä Offensive': 26855,\n",
       " 'HC': 16045,\n",
       " 'gin': 1655,\n",
       " 'Ä Strike': 12282,\n",
       " 'Ä hesitation': 29592,\n",
       " 'Ä Hawaii': 13708,\n",
       " 'Cold': 34312,\n",
       " 'Ä chicken': 9015,\n",
       " 'Ä Britain': 5491,\n",
       " 'react': 45018,\n",
       " 'Ä fingerprint': 25338,\n",
       " 'Ä QU': 19604,\n",
       " 'Ä Patterns': 47020,\n",
       " 'Ä capacity': 5339,\n",
       " 'Ä speech': 4046,\n",
       " '!!\"': 37160,\n",
       " 'Ä Typ': 17134,\n",
       " 'Ä Pizza': 20952,\n",
       " 'Ä cocktail': 24554,\n",
       " 'Ä Officer': 10391,\n",
       " 'Ä conventions': 21396,\n",
       " 'illac': 40607,\n",
       " 'ATION': 6234,\n",
       " 'Cam': 21701,\n",
       " 'Ä ringing': 32333,\n",
       " 'Ä fert': 11093,\n",
       " 'Ä Yok': 45138,\n",
       " 'inter': 3849,\n",
       " 'Ä ranges': 16069,\n",
       " 'deg': 13500,\n",
       " 'Ä sensed': 39243,\n",
       " 'assembled': 46826,\n",
       " 'industrial': 31130,\n",
       " 'Ä consequences': 6948,\n",
       " 'Ä BAS': 29809,\n",
       " 'Ä While': 2893,\n",
       " 'Ä numeric': 35575,\n",
       " 'Ä socialist': 15889,\n",
       " 'Ä rule': 3896,\n",
       " 'Ä tournaments': 18130,\n",
       " '\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': 34604,\n",
       " 'ricane': 11551,\n",
       " 'Ä Kathryn': 48674,\n",
       " 'Ä mentioning': 20862,\n",
       " 'Ä realms': 35423,\n",
       " 'today': 40838,\n",
       " 'Ä¶': 242,\n",
       " 'Ä nation': 3277,\n",
       " 'Ä livestream': 49683,\n",
       " 'Ä Yelp': 44628,\n",
       " 'ced': 771,\n",
       " 'Ä commuting': 45309,\n",
       " 'electric': 31067,\n",
       " 'aida': 30546,\n",
       " 'Ä attorneys': 14449,\n",
       " 'STER': 41809,\n",
       " '_{': 23330,\n",
       " 'Ä Build': 10934,\n",
       " 'Ä McCabe': 45395,\n",
       " 'Clar': 48035,\n",
       " 'Ä Sach': 20678,\n",
       " 'iths': 47252,\n",
       " 'Ä Wik': 11145,\n",
       " 'Ã—Ä»Ã—': 33951,\n",
       " '999': 17032,\n",
       " 'Ä Florida': 4744,\n",
       " 'Ä Span': 49101,\n",
       " 'Ä Winchester': 40868,\n",
       " 'Ä auxiliary': 37419,\n",
       " 'Ä knot': 29654,\n",
       " 'Ä INS': 29194,\n",
       " 'Ä lush': 37408,\n",
       " 'Ä principles': 7811,\n",
       " 'eneg': 46495,\n",
       " 'Program': 15167,\n",
       " 'powers': 30132,\n",
       " 'Ä Tracks': 42259,\n",
       " 'ldom': 23826,\n",
       " 'Ä tags': 15940,\n",
       " 'Tex': 17005,\n",
       " 'Ä esteem': 42213,\n",
       " 'Ä mobility': 15873,\n",
       " 'isl': 3044,\n",
       " 'olis': 8506,\n",
       " 'ruction': 2762,\n",
       " 'Ä Fare': 35205,\n",
       " 'Les': 35882,\n",
       " 'Ä previous': 2180,\n",
       " 'Ä sociop': 41221,\n",
       " 'billion': 24540,\n",
       " 'Ä Prior': 14481,\n",
       " 'Ä contradiction': 25741,\n",
       " 'Ä deviation': 28833,\n",
       " 'Ä Played': 33101,\n",
       " 'Ä trauma': 14649,\n",
       " '558': 40486,\n",
       " 'Ä editorial': 13684,\n",
       " 'Ä entirety': 21818,\n",
       " 'Ã‚Â©': 16224,\n",
       " 'Ä GD': 27044,\n",
       " 'medi': 2379,\n",
       " 'Ä Garage': 45502,\n",
       " 'Lastly': 37511,\n",
       " 'inventory': 24807,\n",
       " 'Ä mediated': 36631,\n",
       " 'Ä receives': 11583,\n",
       " 'Gear': 38141,\n",
       " 'Ä Cla': 27166,\n",
       " 'Ä differed': 36337,\n",
       " 'itbart': 17868,\n",
       " 'onomy': 30565,\n",
       " 'Ä Regular': 23603,\n",
       " 'Ä bleach': 49024,\n",
       " 'Ä concepts': 10838,\n",
       " 'Ä courts': 8028,\n",
       " 'Ä forfeit': 46548,\n",
       " 'types': 19199,\n",
       " 'Ä Putting': 39956,\n",
       " 'Ä organis': 13867,\n",
       " 'Shop': 29917,\n",
       " 'Ä Hungarian': 27304,\n",
       " 'Ä screamed': 25421,\n",
       " 'WASHINGTON': 21793,\n",
       " 'Ä Latino': 20496,\n",
       " 'Ä extensive': 7667,\n",
       " 'igning': 38944,\n",
       " 'Ä ling': 18459,\n",
       " 'ublished': 33286,\n",
       " 'Ä Elk': 40151,\n",
       " 'Ä united': 16503,\n",
       " 'DK': 48510,\n",
       " 'raphic': 22262,\n",
       " 'Ä boycot': 46878,\n",
       " 'Ä dist': 1233,\n",
       " 'Ä interruption': 41728,\n",
       " 'iability': 12455,\n",
       " 'urst': 24962,\n",
       " 'Ä silhouette': 41834,\n",
       " 'OTT': 29089,\n",
       " 'Ä FOR': 7473,\n",
       " 'Ä resentment': 28888,\n",
       " 'Ä episodes': 8640,\n",
       " 'variable': 45286,\n",
       " 'zhen': 46732,\n",
       " 'Ä Grey': 13980,\n",
       " 'Small': 18712,\n",
       " 'Nevertheless': 29011,\n",
       " 'uned': 40881,\n",
       " 'Ä itching': 48140,\n",
       " 'Disable': 48893,\n",
       " 'Ä Newspaper': 49598,\n",
       " 'elta': 12514,\n",
       " 'Ã˜Â§': 12919,\n",
       " 'ached': 2317,\n",
       " 'Ä moderately': 32611,\n",
       " 'Ä Usage': 29566,\n",
       " 'Ä sued': 16334,\n",
       " 'Ä Origin': 19349,\n",
       " 'mund': 20125,\n",
       " 'Ä adhere': 26325,\n",
       " 'Ä Posted': 12918,\n",
       " 'Ä orig': 1796,\n",
       " 'Found': 21077,\n",
       " 'Ä conducive': 45645,\n",
       " 'Peter': 19727,\n",
       " 'Ä echo': 9809,\n",
       " 'Ä Four': 6675,\n",
       " 'anything': 49459,\n",
       " 'enses': 4541,\n",
       " 'Ä Gundam': 32467,\n",
       " 'Ä Arizona': 7943,\n",
       " 'Ä resisted': 26643,\n",
       " 'ashtra': 38535,\n",
       " 'ynski': 40008,\n",
       " 'Ä AIR': 31600,\n",
       " 'Ä sponsored': 15901,\n",
       " 'igion': 17035,\n",
       " 'Ä Illum': 39256,\n",
       " 'Director': 28702,\n",
       " 'Ä Lastly': 36778,\n",
       " 'Ä Pra': 21127,\n",
       " 'Ä partners': 4887,\n",
       " 'Ä insomnia': 47104,\n",
       " 'Ä Azure': 22134,\n",
       " 'count': 9127,\n",
       " 'omics': 31994,\n",
       " 'lost': 33224,\n",
       " 'cles': 5427,\n",
       " 'rador': 40368,\n",
       " 'Ä annually': 13844,\n",
       " 'Ä uninstall': 43194,\n",
       " 'Ä fare': 14505,\n",
       " 'Ä Knife': 32287,\n",
       " 'Ä clues': 20195,\n",
       " 'Ä shone': 44193,\n",
       " 'Ä trainers': 28514,\n",
       " 'Ä Labor': 7882,\n",
       " 'ael': 3010,\n",
       " 'ilateral': 14796,\n",
       " 'inside': 48787,\n",
       " 'lessness': 17587,\n",
       " 'neg': 12480,\n",
       " '/$': 32624,\n",
       " 'okingly': 48343,\n",
       " 'Ä Blue': 4518,\n",
       " 'UCK': 16696,\n",
       " 'Ä collaborator': 50160,\n",
       " 'Ä counties': 14683,\n",
       " 'Ä liqu': 14756,\n",
       " 'Ä mansion': 24141,\n",
       " 'Ä draped': 38425,\n",
       " 'donald': 40915,\n",
       " 'Ä strengthens': 49286,\n",
       " 'Going': 27404,\n",
       " 'Ä onlook': 47747,\n",
       " 'Ä Loan': 32314,\n",
       " 'Ä 188': 27778,\n",
       " 'Ä thorough': 9321,\n",
       " 'warm': 31975,\n",
       " 'apolis': 11174,\n",
       " 'ength': 3286,\n",
       " 'Ä Tuls': 33219,\n",
       " 'Ä sten': 45219,\n",
       " 'aniel': 6321,\n",
       " 'essed': 6676,\n",
       " 'Ä Vietnamese': 23618,\n",
       " 'Ä Came': 32653,\n",
       " 'Ä conceptions': 49849,\n",
       " 'Ä numb': 35519,\n",
       " 'HTTP': 40717,\n",
       " 'Ä Duterte': 25763,\n",
       " 'Ä Hick': 42441,\n",
       " 'Ä RM': 29820,\n",
       " 'Job': 33308,\n",
       " 'Ä patched': 39378,\n",
       " 'Ä socialism': 19803,\n",
       " 'o': 78,\n",
       " 'Ä 332': 41423,\n",
       " 'Ä stimulated': 40216,\n",
       " 'Ä kidney': 21919,\n",
       " '];': 11208,\n",
       " 'Ä opio': 18356,\n",
       " 'Ä Tire': 45942,\n",
       " 'Ä Corpor': 8422,\n",
       " 'Ä Its': 6363,\n",
       " 'Ä Wah': 35893,\n",
       " 'Ä Tavern': 32693,\n",
       " 'RS': 6998,\n",
       " 'Ä modesty': 48740,\n",
       " 'Ä activated': 13906,\n",
       " 'Ä became': 2627,\n",
       " 'Ä Ã¢Ä¸Å‚': 34252,\n",
       " 'Ä parks': 14860,\n",
       " 'Ä suggestive': 42789,\n",
       " '================': 4770,\n",
       " 'pose': 3455,\n",
       " 'Ä Diseases': 39988,\n",
       " 'Thu': 39902,\n",
       " 'sburgh': 11931,\n",
       " 'Ä Doc': 14432,\n",
       " 'Ä farmland': 45723,\n",
       " 'city': 19205,\n",
       " 'Ä shelter': 11772,\n",
       " 'Ä melanch': 40853,\n",
       " 'Ä Margaret': 19579,\n",
       " 'Ä VG': 34627,\n",
       " 'Ä function': 2163,\n",
       " 'igure': 7047,\n",
       " 'Ä Creat': 7921,\n",
       " 'bright': 29199,\n",
       " 'lar': 21681,\n",
       " 'roup': 3233,\n",
       " 'Ä Instead': 5455,\n",
       " '257': 28676,\n",
       " 'Ä Tycoon': 28222,\n",
       " 'Ä Zip': 38636,\n",
       " 'Ä contagious': 43944,\n",
       " 'Ä Mohammad': 29674,\n",
       " 'Ä decipher': 42790,\n",
       " 'Ä planned': 6027,\n",
       " 'Ä Anarch': 32229,\n",
       " 'han': 7637,\n",
       " 'Ä Carey': 31612,\n",
       " 'Ä MLA': 43265,\n",
       " 'Ä demise': 25403,\n",
       " 'Ä pretext': 35097,\n",
       " 'Ä slice': 16416,\n",
       " 'Ä valuable': 8119,\n",
       " 'reve': 36955,\n",
       " 'Ä 377': 42163,\n",
       " 'key': 2539,\n",
       " 'Ä influential': 14212,\n",
       " 'Ä shuff': 32299,\n",
       " 'Look': 8567,\n",
       " 'report': 13116,\n",
       " 'Ä articulated': 36877,\n",
       " 'Ä gasped': 45236,\n",
       " 'Ä moaning': 47644,\n",
       " 'onents': 3906,\n",
       " 'etically': 16877,\n",
       " 'Ä Lucifer': 27084,\n",
       " 'Ä dog': 3290,\n",
       " 'Ä counteract': 47578,\n",
       " 'card': 9517,\n",
       " 'Ä Strait': 41407,\n",
       " 'Ä historian': 18026,\n",
       " 'Ä mathemat': 11896,\n",
       " 'Ä chronic': 10726,\n",
       " 'Ä pack': 2353,\n",
       " 'Ä Coast': 8545,\n",
       " 'Ä Rih': 44502,\n",
       " 'Ä hysteria': 38893,\n",
       " 'Ä SolidGoldMagikarp': 43453,\n",
       " 'Ä membership': 9931,\n",
       " 'sim': 14323,\n",
       " 'Ä irrespective': 40611,\n",
       " 'Ä retired': 9880,\n",
       " 'oother': 31724,\n",
       " 'Ä expenditures': 22895,\n",
       " 'Ä optimizations': 41446,\n",
       " 'Ä Pry': 32500,\n",
       " 'iasis': 48455,\n",
       " 'found': 9275,\n",
       " 'Ä followed': 3940,\n",
       " 'endra': 48286,\n",
       " 'Ä ace': 31506,\n",
       " 'Ä Ann': 5506,\n",
       " 'ENDED': 49361,\n",
       " 'Ä affairs': 9674,\n",
       " 'Ä dictatorship': 26457,\n",
       " 'margin': 36153,\n",
       " 'ASHINGTON': 19436,\n",
       " 'Disc': 15642,\n",
       " 'emn': 37705,\n",
       " 'Ã‹Ä¾': 41185,\n",
       " 'Ä Sic': 28799,\n",
       " 'Ä committed': 5364,\n",
       " 'Ä employees': 4409,\n",
       " 'en': 268,\n",
       " 'ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤': 35496,\n",
       " 'Interest': 19302,\n",
       " 'uter': 11894,\n",
       " 'Ä Gaz': 21347,\n",
       " 'Ä account': 1848,\n",
       " 'inois': 8981,\n",
       " 'Ä burned': 11544,\n",
       " 'aces': 2114,\n",
       " 'Ä exploding': 30990,\n",
       " 'Ä fo': 11511,\n",
       " 'Ä glut': 25276,\n",
       " 'YD': 35755,\n",
       " 'Ä insofar': 44061,\n",
       " 'Moving': 33622,\n",
       " 'Ä Alliance': 10302,\n",
       " 'Ä Weekend': 30537,\n",
       " 'opt': 8738,\n",
       " 'Ä fumes': 47301,\n",
       " 'Ä lingu': 20280,\n",
       " 'Ä mail': 6920,\n",
       " 'Ä politic': 31723,\n",
       " 'Ä noteworthy': 30902,\n",
       " 'Ä Everett': 36815,\n",
       " 'Ä std': 14367,\n",
       " 'Ä introduce': 10400,\n",
       " 'Ä height': 6001,\n",
       " 'Ä declaration': 14305,\n",
       " 'Å': 253,\n",
       " 'Ä jewels': 42701,\n",
       " 'Ä hockey': 12217,\n",
       " '#$': 29953,\n",
       " 'Ä wall': 3355,\n",
       " 'Ä Ernst': 38129,\n",
       " 'Ä pot': 1787,\n",
       " 'Ä Nevada': 12087,\n",
       " 'Ä Nic': 8377,\n",
       " 'gh': 456,\n",
       " 'Ä Oo': 45801,\n",
       " 'Ä PLEASE': 37795,\n",
       " 'Ä PTS': 20907,\n",
       " 'Ä Vir': 16310,\n",
       " 'Ä plainly': 30723,\n",
       " \"'d\": 1549,\n",
       " 'GD': 45113,\n",
       " 'untary': 26468,\n",
       " 'Ä Operating': 24850,\n",
       " 'contract': 28484,\n",
       " 'Ä Barnes': 21335,\n",
       " 'ruly': 34715,\n",
       " '=#': 46249,\n",
       " 'Ä extraordinary': 11359,\n",
       " 'Solar': 38825,\n",
       " 'hu': 13415,\n",
       " 'Ä aesthetics': 35431,\n",
       " 'Ä slamming': 39603,\n",
       " 'odo': 24313,\n",
       " 'Ä Fac': 13585,\n",
       " 'Ä Korean': 6983,\n",
       " 'compl': 23855,\n",
       " 'Ä Checks': 47719,\n",
       " 'Ä Pentagon': 12651,\n",
       " 'Ä Recover': 49107,\n",
       " 'Ä clipping': 45013,\n",
       " 'angler': 49910,\n",
       " ':-': 21912,\n",
       " 'Ä rg': 48670,\n",
       " 'cycles': 32503,\n",
       " '464': 44578,\n",
       " 'Canada': 17940,\n",
       " 'Ä coupons': 45972,\n",
       " 'hair': 27108,\n",
       " \"?'\": 8348,\n",
       " 'powered': 12293,\n",
       " 'Ä Avalon': 39600,\n",
       " 'iscovery': 40821,\n",
       " 'Ä Innovation': 27724,\n",
       " 'Users': 14490,\n",
       " 'Ä Perth': 29913,\n",
       " 'Ä eager': 11069,\n",
       " 'elman': 32370,\n",
       " 'Ä Early': 12556,\n",
       " 'ochond': 22400,\n",
       " 'Ä filtered': 29083,\n",
       " 'Ä jab': 33896,\n",
       " 'Ä limb': 25035,\n",
       " 'Ä sal': 3664,\n",
       " 'Ä swap': 16075,\n",
       " 'Ä Ventura': 43204,\n",
       " 'Ä refund': 12929,\n",
       " 'Much': 20045,\n",
       " 'figured': 46296,\n",
       " 'sv': 21370,\n",
       " 'Ä checked': 10667,\n",
       " 'Ä closer': 5699,\n",
       " 'Ä evening': 6180,\n",
       " 'Ä Beer': 16971,\n",
       " 'Ä Assembly': 10006,\n",
       " 'Ä explore': 7301,\n",
       " '?\".': 43634,\n",
       " 'Ä Aristotle': 34067,\n",
       " 'Ä commenting': 26387,\n",
       " 'Ä OT': 21676,\n",
       " 'Ä abilities': 7883,\n",
       " 'Han': 29919,\n",
       " 'Ä KT': 42293,\n",
       " 'Ä ESC': 40251,\n",
       " 'pri': 3448,\n",
       " 'Ä Apex': 49440,\n",
       " 'loe': 24617,\n",
       " 'Ä Faust': 47411,\n",
       " 'Ä dun': 12574,\n",
       " 'Ä filling': 12591,\n",
       " 'Ä fragmented': 41630,\n",
       " 'illes': 21718,\n",
       " 'Ä halt': 17369,\n",
       " 'abwe': 27050,\n",
       " 'Any': 7149,\n",
       " 'Ã£Ä¥ÄºÃ£Ä¥Â©': 34473,\n",
       " 'Ä Besides': 16238,\n",
       " 'Ä Passive': 31652,\n",
       " 'Ä Roh': 32694,\n",
       " 'Ä img': 33705,\n",
       " 'Ä invoke': 26342,\n",
       " 'Ä resonate': 41523,\n",
       " 'Ä XI': 30554,\n",
       " 'Ä restore': 11169,\n",
       " 'Ä optimal': 16586,\n",
       " 'Ä shareholders': 19195,\n",
       " 'Ä tal': 3305,\n",
       " 'Ä triglycer': 47937,\n",
       " 'meal': 28208,\n",
       " '359': 30743,\n",
       " 'enei': 46009,\n",
       " 'Ä Ashe': 36318,\n",
       " 'Ä power': 1176,\n",
       " 'Ä shameless': 41564,\n",
       " 'zag': 50183,\n",
       " 'Ä aggrav': 20072,\n",
       " 'Ä disg': 13757,\n",
       " 'Ä more': 517,\n",
       " 'Ä rejects': 28317,\n",
       " 'snap': 45380,\n",
       " 'Ä junction': 35037,\n",
       " 'Ä rooting': 40105,\n",
       " 'mic': 9383,\n",
       " 'chen': 6607,\n",
       " 'Ä guessing': 25260,\n",
       " 'Ä videog': 36342,\n",
       " 'Ä volunteers': 11661,\n",
       " 'known': 4002,\n",
       " 'Ä whichever': 26204,\n",
       " 'Ä¡': 221,\n",
       " '806': 37988,\n",
       " 'Ä intrusion': 34396,\n",
       " 'Ä Legions': 48534,\n",
       " 'medical': 41693,\n",
       " 'Ä Mozilla': 29258,\n",
       " 'Ä preached': 38737,\n",
       " 'Ä psych': 3795,\n",
       " 'Ä maintains': 16047,\n",
       " 'oddy': 38553,\n",
       " 'Ä bir': 35122,\n",
       " 'shots': 20910,\n",
       " 'Ä runway': 23443,\n",
       " 'Ä opioid': 23039,\n",
       " 'rill': 20190,\n",
       " 'Ä Joe': 5689,\n",
       " 'Ä Calcul': 27131,\n",
       " 'Ä stakes': 21147,\n",
       " 'Such': 16678,\n",
       " 'iance': 3610,\n",
       " 'Ä illegally': 15572,\n",
       " 'Ä waits': 28364,\n",
       " 'guards': 33427,\n",
       " 'illegal': 47749,\n",
       " '->': 3784,\n",
       " 'livious': 35260,\n",
       " 'Ä worrisome': 48367,\n",
       " 'Ä transports': 45245,\n",
       " 'Ä London': 3576,\n",
       " 'arus': 20272,\n",
       " 'arat': 34174,\n",
       " 'Ä warships': 44304,\n",
       " 'TX': 29551,\n",
       " 'cms': 46406,\n",
       " 'ners': 2741,\n",
       " 'Ä Cyrus': 34305,\n",
       " 'Ä Orleans': 12255,\n",
       " 'ojure': 32511,\n",
       " 'Ä inev': 9026,\n",
       " 'Ä scraps': 44496,\n",
       " 'ÅƒÂ·': 48953,\n",
       " 'atile': 12610,\n",
       " 'Ä inflic': 30333,\n",
       " 'Ä kan': 43998,\n",
       " 'Ä incidentally': 42258,\n",
       " 'onto': 5957,\n",
       " 'Ä tyrant': 47167,\n",
       " 'Ä Malta': 35206,\n",
       " 'Ä autopsy': 30241,\n",
       " '005': 22544,\n",
       " 'opic': 16603,\n",
       " 'utterstock': 28819,\n",
       " 'Ä Examination': 50105,\n",
       " 'Ä Greenland': 30155,\n",
       " 'Ä incarcer': 18615,\n",
       " '////////////////': 27246,\n",
       " 'astery': 29310,\n",
       " 'itles': 30540,\n",
       " 'mbudsman': 47012,\n",
       " '1945': 41931,\n",
       " 'IRC': 49060,\n",
       " 'Ä Dragonbound': 17900,\n",
       " 'Ä Island': 5451,\n",
       " 'Ã¢Ä¢Äµ': 1906,\n",
       " 'Ä LC': 22228,\n",
       " 'node': 17440,\n",
       " 'Ä Dimensions': 41265,\n",
       " 'Bob': 18861,\n",
       " 'Ä Tra': 4759,\n",
       " 'Ä Baby': 14801,\n",
       " 'Ä Bond': 12812,\n",
       " 'group': 8094,\n",
       " 'Ä Pats': 47216,\n",
       " 'Ä chiefly': 36305,\n",
       " 'Ä crane': 41175,\n",
       " 'Ä parade': 16134,\n",
       " 'fall': 7207,\n",
       " '273': 27367,\n",
       " 'parable': 37064,\n",
       " 'peed': 39492,\n",
       " 'Ä outcome': 8055,\n",
       " 'Ä repeats': 29819,\n",
       " 'Channel': 29239,\n",
       " 'rs': 3808,\n",
       " 'leck': 40667,\n",
       " 'Ä Pis': 42021,\n",
       " 'Ä WA': 16400,\n",
       " 'Ä Trop': 25491,\n",
       " 'Ä neutron': 49810,\n",
       " 'Ä seeker': 45993,\n",
       " 'Ä args': 26498,\n",
       " 'father': 11358,\n",
       " 'atar': 9459,\n",
       " 'Ä 3000': 20343,\n",
       " 'Ä Keyboard': 31973,\n",
       " 'Ä Mississippi': 13797,\n",
       " '<<': 16791,\n",
       " 'Ä stainless': 25704,\n",
       " 'otle': 23556,\n",
       " 'Ä substantial': 8904,\n",
       " 'Ä totaled': 39398,\n",
       " 'afia': 22214,\n",
       " 'clude': 9152,\n",
       " 'Ä foam': 19828,\n",
       " 'Ä Xu': 33591,\n",
       " 'Ä Bhar': 33653,\n",
       " 'Ã£Ä¤Â¤Ã£Ä¥Äª': 42396,\n",
       " 'Ä pledge': 13995,\n",
       " 'Ä cod': 14873,\n",
       " 'Ä continuity': 24216,\n",
       " 'Ä Cato': 44509,\n",
       " 'Ä tuna': 38883,\n",
       " 'steen': 42580,\n",
       " 'Ä guided': 17455,\n",
       " 'ths': 9998,\n",
       " 'Ä starve': 47141,\n",
       " 'hearted': 20122,\n",
       " 'Ä Scriptures': 41622,\n",
       " 'Ä computing': 14492,\n",
       " 'Ä asteroid': 27460,\n",
       " 'Ä conspir': 29099,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹è¯æ±‡è¡¨\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8be0bd1-c9c0-4cb2-92fa-0b8c5d230d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for 'world': 6894\n",
      "Token for ID 995: Ä world\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ç‰¹å®š Token çš„ ID\n",
    "token_id = tokenizer.convert_tokens_to_ids('world')\n",
    "print(\"Token ID for 'world':\", token_id)\n",
    "\n",
    "# æŸ¥çœ‹ç‰¹å®š ID å¯¹åº”çš„ Token\n",
    "token = tokenizer.convert_ids_to_tokens(995)\n",
    "print(\"Token for ID 995:\", token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c3d36-3ac7-40ef-a8f7-48277b066825",
   "metadata": {},
   "source": [
    "> è¿™é‡Œçš„ `Ä ` ä»£è¡¨ä¸€ä¸ªç©ºæ ¼å­—ç¬¦ï¼š\n",
    ">\n",
    "> ```python\n",
    "> print(tokenizer.tokenize(' '))\n",
    "> ```\n",
    ">\n",
    "> è¾“å‡ºä¸º `['Ä ']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26fc1035-abf5-46de-923a-e082863b55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Special Tokens: ['<|endoftext|>']\n",
      "Special Token IDs: [50256]\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹æ‰€æœ‰ç‰¹æ®Šæ ‡è®°\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "print(\"All Special Tokens:\", special_tokens)\n",
    "\n",
    "# æŸ¥çœ‹ç‰¹æ®Šæ ‡è®°å¯¹åº”çš„ ID\n",
    "special_token_ids = tokenizer.all_special_ids\n",
    "print(\"Special Token IDs:\", special_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c39bd-fd98-46c4-9da7-06f08f327125",
   "metadata": {},
   "source": [
    "**ã€Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¢è®¨ Tokenizer çš„å…·ä½“ç»†èŠ‚ã€**\n",
    "\n",
    "> ä¸éœ€è¦æ·±å…¥æ¥ä¸‹æ¥çš„æ‰€æœ‰ä»£ç ç»†èŠ‚ï¼Œåªéœ€è¦æŸ¥çœ‹è¾“å‡ºä¸ç›¸åº”ã€Œæ­¥éª¤ã€çš„è¡¨è¿°ã€‚\n",
    "\n",
    "## åˆ†è¯ï¼ˆTokenizeï¼‰\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦å°†è¯­æ–™åº“ï¼ˆcorpusï¼‰çš„æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ï¼Œå‡è®¾å½“å‰è¯­æ–™åº“åŒ…å«çš„å•è¯å’Œå¯¹åº”é¢‘æ¬¡å¦‚ä¸‹ï¼š\n",
    "\n",
    "```sql\n",
    "(\"low\", 5), (\"lower\", 2), (\"newest\", 6), (\"widest\", 3)\n",
    "```\n",
    "\n",
    "æœ‰äº›è®ºæ–‡ä¹Ÿç”¨ `vocab` æ¥è¡¨è¿°ï¼ŒçŸ¥é“åé¢æ˜¯é¢‘æ¬¡å³å¯ï¼Œå‘½åä¸ç”¨çº ç»“ã€‚\n",
    "\n",
    "### æ„é€ è¯æ±‡è¡¨\n",
    "\n",
    "#### Byte-Pair Encoding (BPE)\n",
    "\n",
    "> **å‚è€ƒæ–‡çŒ®ï¼š**\n",
    ">\n",
    "> - [A new algorithm for data compression. 1994](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\n",
    "> - [Neural Machine Translation of Rare Words with Subword Units. 2015](https://arxiv.org/pdf/1508.07909v5)\n",
    ">\n",
    "> BPE æ˜¯ä¸€ç§åŸºäºæ•°æ®å‹ç¼©çš„æŠ€æœ¯ï¼Œæœ€æ—©ç”± Gage åœ¨ 1994 å¹´æå‡ºï¼Œåæ¥è¢«ç”¨äº GPT ç­‰æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§å­è¯åˆ†å‰²ç®—æ³•ï¼Œä»å­—ç¬¦çº§åˆ«å¼€å§‹ï¼Œé€šè¿‡è¿­ä»£åˆå¹¶é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ï¼ˆæˆ–å­—ç¬¦åºåˆ—ï¼‰æ¥æ„å»ºæ–°çš„ Tokenï¼Œä»è€Œå¯ä»¥å¤„ç†éƒ¨åˆ† OOVï¼ˆOut-Of-Vocabularyï¼‰æƒ…å†µã€‚\n",
    ">\n",
    "> **Q: ä»€ä¹ˆæ˜¯ OOV ï¼Ÿ**\n",
    ">\n",
    "> å…¶å®å°±æ˜¯ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯ï¼Œä¹Ÿç§°ä¹‹ä¸ºã€Œæœªç™»å½•è¯ã€ã€‚\n",
    "\n",
    "BPE æ¯æ¬¡çš„è¿­ä»£ç›®æ ‡æ˜¯æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„ç›¸é‚»å­—ç¬¦å¯¹ï¼Œå®šä¹‰ Score ä»¥ä¸ WordPiece ä½œå¯¹æ¯”ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Score}_{\\text{BPE}}(x, y) = \\text{freq}(x, y)\n",
    "$$\n",
    "å…¶ä¸­ï¼Œ$\\text{freq}(x, y)$ è¡¨ç¤ºå­—ç¬¦å¯¹ $(x, y)$ åœ¨è¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘æ¬¡ã€‚\n",
    "\n",
    "##### æ­¥éª¤\n",
    "\n",
    "1. **åˆå§‹åŒ–è¯æ±‡è¡¨ $V$**ï¼š\n",
    "   - $V$ åŒ…å«è¯­æ–™åº“ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ï¼Œå³å•è¯å­—ç¬¦çš„é›†åˆã€‚\n",
    "2. **ç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡**ï¼š\n",
    "   - å¯¹äºæ¯ä¸ªå•è¯çš„å­—ç¬¦åºåˆ—ï¼Œç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹çš„å‡ºç°é¢‘æ¬¡ã€‚\n",
    "3. **æ‰¾åˆ°é¢‘æ¬¡ï¼ˆScoreï¼‰æœ€é«˜çš„å­—ç¬¦å¯¹å¹¶åˆå¹¶**ï¼š\n",
    "   - é€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ $(x, y)$ï¼Œå°†å…¶åˆå¹¶ä¸ºæ–°ç¬¦å· $xy$ã€‚\n",
    "4. **æ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤æ­¥éª¤ 2 åˆ° 4**ï¼š\n",
    "   - å°†æ–°ç¬¦å·æ·»åŠ åˆ°è¯æ±‡è¡¨ $V = V \\cup \\{xy\\}$ã€‚\n",
    "   - æ›´æ–°è¯­æ–™åº“ä¸­çš„å•è¯è¡¨ç¤ºï¼Œé‡å¤ç»Ÿè®¡å’Œåˆå¹¶è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œè¯æ±‡è¡¨è¾¾åˆ°é¢„å®šå¤§å°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc09234-cef2-4898-8260-3ce47e0b2d50",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹\n",
    "\n",
    "**æ­¥éª¤ 1ï¼šåˆå§‹åŒ–è¯æ±‡è¡¨**\n",
    "\n",
    "- **å°†å•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åºåˆ—**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  (\"l\", \"o\", \"w\"), 5  \n",
    "  (\"l\", \"o\", \"w\", \"e\", \"r\"), 2  \n",
    "  (\"n\", \"e\", \"w\", \"e\", \"s\", \"t\"), 6  \n",
    "  (\"w\", \"i\", \"d\", \"e\", \"s\", \"t\"), 3\n",
    "  ```\n",
    "\n",
    "- **è¯æ±‡è¡¨ $V$**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd'}\n",
    "  ```\n",
    "\n",
    "**æ­¥éª¤ 2ï¼šç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡**\n",
    "\n",
    "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®ç»™å®šçš„å•è¯å’Œå…¶é¢‘æ¬¡ï¼Œè‡ªåŠ¨ç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39dfe4fe-c1fd-4c54-aa9b-3b21810b109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:\n",
      "('l', 'o'): 7\n",
      "('o', 'w'): 7\n",
      "('w', 'e'): 8\n",
      "('e', 'r'): 2\n",
      "('n', 'e'): 6\n",
      "('e', 'w'): 6\n",
      "('e', 's'): 9\n",
      "('s', 't'): 9\n",
      "('w', 'i'): 3\n",
      "('i', 'd'): 3\n",
      "('d', 'e'): 3\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_char_pairs(word_freq):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯å’Œå…¶é¢‘æ¬¡\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        chars = list(word)\n",
    "        for i in range(len(chars) - 1):\n",
    "            pair = (chars[i], chars[i + 1])\n",
    "            pair_freq[pair] += freq\n",
    "    return pair_freq\n",
    "\n",
    "# ç¤ºä¾‹è¯æ±‡è¡¨å’Œå•è¯é¢‘æ¬¡\n",
    "word_freq = [\n",
    "    (\"low\", 5),\n",
    "    (\"lower\", 2),\n",
    "    (\"newest\", 6),\n",
    "    (\"widest\", 3)\n",
    "]\n",
    "\n",
    "pair_freq = count_char_pairs(word_freq)\n",
    "print(\"å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:\")\n",
    "for pair, freq in pair_freq.items():\n",
    "    print(f\"{pair}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237d666-d360-44b3-95d8-54b56884cb5d",
   "metadata": {},
   "source": [
    "**æ­¥éª¤ 3ï¼šæ‰¾åˆ°é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹å¹¶åˆå¹¶**\n",
    "\n",
    "- **é€‰æ‹©é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹**ï¼š\n",
    "\n",
    "  - `(\"e\", \"s\")` å’Œ `(\"s\", \"t\")`ï¼Œé¢‘æ¬¡å‡ä¸º 9ã€‚å¯ä»¥ä»»é€‰å…¶ä¸€è¿›è¡Œåˆå¹¶ï¼Œå‡è®¾é€‰æ‹©æ’åºç¬¬ä¸€çš„ï¼š `(\"e\", \"s\")`ã€‚\n",
    "\n",
    "- **åˆå¹¶ `(\"e\", \"s\")` ä¸ºæ–°ç¬¦å· `es`**ã€‚\n",
    "\n",
    "- **è®°å½•åˆå¹¶æ“ä½œ**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  Merge 1: (\"e\", \"s\") -> \"es\"\n",
    "  ```\n",
    "\n",
    "**æ­¥éª¤ 4ï¼šæ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤**\n",
    "\n",
    "- **æ›´æ–°å•è¯åºåˆ—**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  (\"l\", \"o\", \"w\"), 5  \n",
    "  (\"l\", \"o\", \"w\", \"e\", \"r\"), 2  \n",
    "  (\"n\", \"e\", \"w\", \"es\", \"t\"), 6  \n",
    "  (\"w\", \"i\", \"d\", \"es\", \"t\"), 3\n",
    "  ```\n",
    "\n",
    "- **æ›´æ–°è¯æ±‡è¡¨ $V$**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es'}\n",
    "  ```\n",
    "\n",
    "- **é‡å¤æ­¥éª¤ 2 åˆ° 4ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è¯æ±‡è¡¨å¤§å°**ã€‚\n",
    "\n",
    "> ##### ğŸ“ ç»ƒä¹ é¢˜\n",
    ">\n",
    "> åœä¸‹æ¥æ€è€ƒä¸€ä¸‹ï¼Œç­”æ¡ˆå’Œä»£ç ä½äºå½“å‰æ¨¡å—æœ«å°¾ã€‚\n",
    ">\n",
    "> **Q1.** æœ€åˆçš„è¯æ±‡è¡¨å¤§å°ä¸º 10ï¼Œå‡è®¾é¢„å®šå¤§å°ä¸º 13ï¼Œé‚£ä¹ˆå½“å‰çš„è¯æ±‡è¡¨ $V$ ä¸ºå¤šå°‘ï¼Ÿåˆå¹¶è®°å½•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    ">\n",
    "> **Q2.** å¦‚æœä»¥ `</w>`ï¼ˆè¡¨ç¤ºå•è¯ç»“å°¾ï¼‰ä½œä¸ºæ¯ä¸ªè¯­æ–™åº“ä¸­å•è¯çš„ç»“å°¾ï¼Œæœ€åˆçš„è¯æ±‡è¡¨ä¼šå—åˆ°ä»€ä¹ˆå½±å“ï¼Œåç»­çš„è¿‡ç¨‹ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿå‡è®¾é¢„å®šå¤§å°ä¸º 14ï¼Œå½“å‰çš„åˆå¹¶è®°å½•æ˜¯ä»€ä¹ˆï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe352af-0f8f-4848-a8dd-b10136fbe205",
   "metadata": {},
   "source": [
    "#### WordPiece\n",
    "\n",
    "> **å‚è€ƒæ–‡çŒ®ï¼š**\n",
    ">\n",
    "> - [Japanese and Korean voice search. 2012](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37842.pdf)\n",
    "> - [Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. 2016](https://arxiv.org/pdf/1609.08144v2)\n",
    ">\n",
    "> WordPiece æ˜¯ä¸€ç§å­è¯åˆ†å‰²ç®—æ³•ï¼Œæœ€åˆç”¨äºå¤„ç†æ—¥è¯­å’ŒéŸ©è¯­çš„è¯­éŸ³æœç´¢ï¼Œåæ¥åœ¨ Google çš„ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¸­å¾—åˆ°åº”ç”¨ã€‚\n",
    "\n",
    "ä¸ BPE ä¸åŒï¼ŒWordPiece çš„ Score ç”±å­—ç¬¦å¯¹é¢‘æ¬¡ä¸å…¶ç»„æˆéƒ¨åˆ†é¢‘æ¬¡çš„æ¯”å€¼å†³å®šï¼Œå®šä¹‰ Scoreï¼š\n",
    "\n",
    "$$\n",
    "\\text{Score}_{\\text{WordPiece}}(x, y) = \\frac{\\text{freq}(xy)}{\\text{freq}(x) \\times \\text{freq}(y)}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼Œ$\\text{freq}(x)$ã€$\\text{freq}(y)$ å’Œ $\\text{freq}(xy)$ åˆ†åˆ«è¡¨ç¤ºç¬¦å· $x$ã€$y$ å’Œå®ƒä»¬åˆå¹¶åçš„ç¬¦å· $xy$ çš„é¢‘æ¬¡ã€‚\n",
    "\n",
    "##### æ­¥éª¤\n",
    "\n",
    "1. **åˆå§‹åŒ–è¯æ±‡è¡¨ $V$**ï¼š\n",
    "   - ä¸ BPE ç›¸åŒï¼Œ$V$ åŒ…å«è¯­æ–™åº“ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ï¼Œä½†å¤„ç†æ–¹å¼ç•¥æœ‰ä¸åŒï¼šå¯¹äºæ¯ä¸ªå•è¯ï¼Œé™¤äº†é¦–ä¸ªå­—ç¬¦å¤–ï¼Œå…¶ä»–å­—ç¬¦å‰éƒ½åŠ ä¸Š `##` å‰ç¼€ã€‚\n",
    "2. **ç»Ÿè®¡å­—ç¬¦å¯¹çš„é¢‘æ¬¡åŠ Score**ï¼š\n",
    "   - å¯¹äºæ¯ä¸ªå¯èƒ½çš„å­—ç¬¦å¯¹ $(x, y)$ï¼Œè®¡ç®— $\\text{freq}(x)$ã€$\\text{freq}(y)$ã€$\\text{freq}(xy)$ï¼Œå¹¶è®¡ç®— Scoreã€‚\n",
    "3. **æ‰¾åˆ° Score æœ€é«˜çš„å­—ç¬¦å¯¹å¹¶åˆå¹¶**ï¼š\n",
    "   - é€‰æ‹© Score æœ€é«˜çš„å­—ç¬¦å¯¹ $(x, y)$ï¼Œå°†å…¶åˆå¹¶ä¸ºæ–°ç¬¦å· $xy$ï¼Œæ³¨æ„ï¼š\n",
    "     - å¦‚æœç¬¬äºŒä¸ªç¬¦å·ä»¥ `##` å¼€å¤´ï¼Œåˆå¹¶æ—¶å»æ‰ `##` å‰ç¼€å†è¿›è¡Œè¿æ¥ã€‚\n",
    "     - æ–°ç¬¦å·æ˜¯å¦ä»¥ `##` å¼€å¤´ï¼Œå–å†³äºç¬¬ä¸€ä¸ªç¬¦å·æ˜¯å¦ä»¥ `##` å¼€å¤´ã€‚\n",
    "4. **æ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤æ­¥éª¤ 2 åˆ° 4**ï¼š\n",
    "   - å°†æ–°ç¬¦å·æ·»åŠ åˆ°è¯æ±‡è¡¨ $V = V \\cup \\{xy\\}$ã€‚\n",
    "   - æ›´æ–°è¯­æ–™åº“ä¸­çš„å•è¯è¡¨ç¤ºï¼Œé‡å¤ç»Ÿè®¡å’Œåˆå¹¶è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ã€‚\n",
    "\n",
    "##### ç¤ºä¾‹\n",
    "\n",
    "ä½¿ç”¨ä¸ BPE ç¤ºä¾‹ç›¸åŒçš„è¯­æ–™åº“ã€‚\n",
    "\n",
    "**æ­¥éª¤ 1ï¼šåˆå§‹åŒ–è¯æ±‡è¡¨**\n",
    "\n",
    "- **å°†å•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åºåˆ—**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  ('l', '##o', '##w'), 5                       # \"low\"\n",
    "  ('l', '##o', '##w', '##e', '##r'), 2         # \"lower\"\n",
    "  ('n', '##e', '##w', '##e', '##s', '##t'), 6  # \"newest\"\n",
    "  ('w', '##i', '##d', '##e', '##s', '##t'), 3  # \"widest\"\n",
    "  ```\n",
    "\n",
    "- **è¯æ±‡è¡¨ $V$**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', '##o', '##w', '##e', '##r', 'n', '##s', '##t', 'w', '##i', '##d'}\n",
    "  ```\n",
    "\n",
    "**æ­¥éª¤ 2ï¼šç»Ÿè®¡å­—ç¬¦å’Œå­—ç¬¦å¯¹çš„é¢‘æ¬¡ï¼Œè®¡ç®— Score**\n",
    "\n",
    "å¯ä»¥è®¾è®¡ä¸€ä¸ªå‡½æ•°å®Œæˆè¿™ä¸ªæ­¥éª¤ï¼ˆç›´æ¥è¿è¡ŒæŸ¥çœ‹è¾“å‡ºï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5910b8ee-7091-4cd6-803a-73802b7e5d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:\n",
      "('l', '##o'): 7\n",
      "('##o', '##w'): 7\n",
      "('##w', '##e'): 8\n",
      "('##e', '##r'): 2\n",
      "('n', '##e'): 6\n",
      "('##e', '##w'): 6\n",
      "('##e', '##s'): 9\n",
      "('##s', '##t'): 9\n",
      "('w', '##i'): 3\n",
      "('##i', '##d'): 3\n",
      "('##d', '##e'): 3\n",
      "\n",
      "å•ä¸ªå­—ç¬¦é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:\n",
      "l: 7\n",
      "##o: 7\n",
      "##w: 13\n",
      "##e: 17\n",
      "##r: 2\n",
      "n: 6\n",
      "##s: 9\n",
      "##t: 9\n",
      "w: 3\n",
      "##i: 3\n",
      "##d: 3\n",
      "\n",
      "å­—ç¬¦å¯¹ Score è®¡ç®—ç»“æœ:\n",
      "('l', '##o'): 0.1429\n",
      "('##o', '##w'): 0.0769\n",
      "('##w', '##e'): 0.0362\n",
      "('##e', '##r'): 0.0588\n",
      "('n', '##e'): 0.0588\n",
      "('##e', '##w'): 0.0271\n",
      "('##e', '##s'): 0.0588\n",
      "('##s', '##t'): 0.1111\n",
      "('w', '##i'): 0.3333\n",
      "('##i', '##d'): 0.3333\n",
      "('##d', '##e'): 0.0588\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_char_pairs_wordpiece(word_freq):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦çš„é¢‘æ¬¡ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        ä¸¤ä¸ªå­—å…¸ï¼Œåˆ†åˆ«ä¸ºå­—ç¬¦å¯¹é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦é¢‘æ¬¡\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    char_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        for i in range(len(word)):\n",
    "            char_freq[word[i]] += freq\n",
    "            if i < len(word) - 1:\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pair_freq[pair] += freq\n",
    "    return pair_freq, char_freq\n",
    "\n",
    "def compute_wordpiece_score(freq_xy, freq_x, freq_y):\n",
    "    \"\"\"\n",
    "    æ ¹æ® WordPiece çš„å®šä¹‰è®¡ç®— Scoreã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        freq_xy: ç¬¦å·å¯¹çš„é¢‘æ¬¡\n",
    "        freq_x: ç¬¦å· x çš„é¢‘æ¬¡\n",
    "        freq_y: ç¬¦å· y çš„é¢‘æ¬¡\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        è®¡ç®—å¾—åˆ°çš„ Score\n",
    "    \"\"\"\n",
    "    if freq_x == 0 or freq_y == 0:\n",
    "        return 0\n",
    "    return freq_xy / (freq_x * freq_y)\n",
    "\n",
    "# ç¤ºä¾‹è¯æ±‡è¡¨å’Œå•è¯é¢‘æ¬¡\n",
    "word_freq = [\n",
    "    (['l', '##o', '##w'], 5),\n",
    "    (['l', '##o', '##w', '##e', '##r'], 2),\n",
    "    (['n', '##e', '##w', '##e', '##s', '##t'], 6),\n",
    "    (['w', '##i', '##d', '##e', '##s', '##t'], 3)\n",
    "]\n",
    "\n",
    "# ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦é¢‘æ¬¡\n",
    "pair_freq, char_freq = count_char_pairs_wordpiece(word_freq)\n",
    "\n",
    "# è®¡ç®—æ¯å¯¹å­—ç¬¦çš„ Score\n",
    "scores = {}\n",
    "for pair in pair_freq:\n",
    "    freq_xy = pair_freq[pair]\n",
    "    freq_x = char_freq[pair[0]]\n",
    "    freq_y = char_freq[pair[1]]\n",
    "    score = compute_wordpiece_score(freq_xy, freq_x, freq_y)\n",
    "    scores[pair] = score\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(\"å­—ç¬¦å¯¹é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:\")\n",
    "for pair, freq in pair_freq.items():\n",
    "    print(f\"{pair}: {freq}\")\n",
    "\n",
    "print(\"\\nå•ä¸ªå­—ç¬¦é¢‘æ¬¡ç»Ÿè®¡ç»“æœ:\")\n",
    "for char, freq in char_freq.items():\n",
    "    print(f\"{char}: {freq}\")\n",
    "\n",
    "print(\"\\nå­—ç¬¦å¯¹ Score è®¡ç®—ç»“æœ:\")\n",
    "for pair, score in scores.items():\n",
    "    print(f\"{pair}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204f92b-08d1-42b5-a3b1-1b38a2fa2a3f",
   "metadata": {},
   "source": [
    "- **é€‰æ‹©é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹**ï¼š\n",
    "\n",
    "  -  `('w', '##i')` å’Œ `('##i', '##d')`ï¼ŒScore éƒ½ä¸º 0.3333ã€‚å¯ä»¥ä»»é€‰å…¶ä¸€è¿›è¡Œåˆå¹¶ï¼Œå‡è®¾é€‰æ‹©æ’åºç¬¬ä¸€çš„ï¼š `(\"w\", \"##i\")`ã€‚\n",
    "\n",
    "- **åˆå¹¶ `('w', '##i')` ä¸ºæ–°ç¬¦å· `wi`**\n",
    "\n",
    "  - æ³¨æ„ï¼šåˆå¹¶æ—¶ï¼Œè‹¥ç¬¬äºŒä¸ªç¬¦å·ä»¥ `##` å¼€å¤´ï¼Œåˆå¹¶åçš„æ–°ç¬¦å·ä¸ºç¬¬ä¸€ä¸ªç¬¦å·åŠ ä¸Šç¬¬äºŒä¸ªç¬¦å·å»æ‰ `##` å‰ç¼€çš„éƒ¨åˆ†ã€‚\n",
    "\n",
    "- **è®°å½•åˆå¹¶æ“ä½œï¼š**\n",
    "\n",
    "  ```plaintext\n",
    "  Merge 1: ('w', '##i') -> 'wi'\n",
    "  ```\n",
    "\n",
    "**æ­¥éª¤ 4ï¼šæ›´æ–°è¯æ±‡è¡¨å¹¶é‡å¤**\n",
    "\n",
    "- **æ›´æ–°è¯æ±‡è¡¨ $V$**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', '##o', '##w', '##e', '##r', 'n', '##s', '##t', 'w', '##i', '##d', 'wi'}\n",
    "  ```\n",
    "\n",
    "- **æ›´æ–°å•è¯åºåˆ—**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  ('l', '##o', '##w'), 5                       # \"low\"\n",
    "  ('l', '##o', '##w', '##e', '##r'), 2         # \"lower\"\n",
    "  ('n', '##e', '##w', '##e', '##s', '##t'), 6  # \"newest\"\n",
    "  ('wi', '##d', '##e', '##s', '##t'), 3        # \"widest\"\n",
    "  ```\n",
    "\n",
    "- **é‡å¤æ­¥éª¤ 2 åˆ° 4ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è¯æ±‡è¡¨å¤§å°**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125f661-a493-405f-8c2e-cee84129afbe",
   "metadata": {},
   "source": [
    "##### ä½¿ç”¨å‡½æ•°å®ç°ç®€å•çš„ WordPiece\n",
    "\n",
    "BPE çš„å®ç°åœ¨ã€Œç»ƒä¹ é¢˜ç­”æ¡ˆã€ä¸­ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b5a44a-72b1-46f5-997e-b497940186fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('w', '##i') -> wi, Score: 0.3333, è¯æ±‡è¡¨å¤§å°: 12\n",
      "Merge: ('wi', '##d') -> wid, Score: 0.3333, è¯æ±‡è¡¨å¤§å°: 13\n",
      "Merge: ('l', '##o') -> lo, Score: 0.1429, è¯æ±‡è¡¨å¤§å°: 14\n",
      "Merge: ('##s', '##t') -> ##st, Score: 0.1111, è¯æ±‡è¡¨å¤§å°: 15\n",
      "\n",
      "æœ€ç»ˆè¯æ±‡è¡¨ V:\n",
      "{'n', '##w', '##s', '##e', '##t', 'lo', 'wid', '##r', '##i', 'wi', 'w', '##o', '##d', 'l', '##st'}\n",
      "\n",
      "åˆå¹¶è®°å½•:\n",
      "Merge 1: ('w', '##i') -> wi\n",
      "Merge 2: ('wi', '##d') -> wid\n",
      "Merge 3: ('l', '##o') -> lo\n",
      "Merge 4: ('##s', '##t') -> ##st\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_new_symbol(x, y):\n",
    "    \"\"\"\n",
    "    æ ¹æ® WordPiece çš„è§„åˆ™åˆ›å»ºæ–°ç¬¦å·ã€‚\n",
    "\n",
    "    - å¦‚æœ y ä»¥ '##' å¼€å¤´ï¼Œåˆå¹¶æ—¶éœ€è¦å»æ‰ y çš„ '##' å‰ç¼€ã€‚\n",
    "    - æ–°ç¬¦å·æ˜¯å¦ä»¥ '##' å¼€å¤´ï¼Œå–å†³äº x æ˜¯å¦ä»¥ '##' å¼€å¤´ã€‚\n",
    "    \"\"\"\n",
    "    x_starts_hash = x.startswith('##')\n",
    "    x_without_hash = x[2:] if x_starts_hash else x\n",
    "    y_without_hash = y[2:] if y.startswith('##') else y\n",
    "    new_symbol = x_without_hash + y_without_hash\n",
    "    if x_starts_hash:\n",
    "        new_symbol = '##' + new_symbol\n",
    "    return new_symbol\n",
    "\n",
    "def count_char_pairs_wordpiece(word_freq):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦çš„é¢‘æ¬¡ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        ä¸¤ä¸ªå­—å…¸ï¼Œåˆ†åˆ«ä¸ºå­—ç¬¦å¯¹é¢‘æ¬¡å’Œå•ä¸ªå­—ç¬¦é¢‘æ¬¡\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    char_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        for i in range(len(word)):\n",
    "            char_freq[word[i]] += freq\n",
    "            if i < len(word) - 1:\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pair_freq[pair] += freq\n",
    "    return pair_freq, char_freq\n",
    "\n",
    "def compute_wordpiece_score(freq_xy, freq_x, freq_y):\n",
    "    \"\"\"\n",
    "    æ ¹æ® WordPiece çš„å®šä¹‰è®¡ç®— Scoreã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        freq_xy: ç¬¦å·å¯¹çš„é¢‘æ¬¡\n",
    "        freq_x: ç¬¦å· x çš„é¢‘æ¬¡\n",
    "        freq_y: ç¬¦å· y çš„é¢‘æ¬¡\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        è®¡ç®—å¾—åˆ°çš„ Score\n",
    "    \"\"\"\n",
    "    if freq_x == 0 or freq_y == 0:\n",
    "        return 0\n",
    "    return freq_xy / (freq_x * freq_y)\n",
    "\n",
    "def find_best_pair_wordpiece(pair_freq, char_freq):\n",
    "    \"\"\"\n",
    "    æ‰¾åˆ°å…·æœ‰æœ€é«˜ Score çš„å­—ç¬¦å¯¹ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        pair_freq: å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸\n",
    "        char_freq: å•ä¸ªå­—ç¬¦é¢‘æ¬¡çš„å­—å…¸\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        å…·æœ‰æœ€é«˜ Score çš„å­—ç¬¦å¯¹åŠå…¶ Score\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for pair, freq_xy in pair_freq.items():\n",
    "        x, y = pair\n",
    "        freq_x = char_freq.get(x, 0)\n",
    "        freq_y = char_freq.get(y, 0)\n",
    "        score = compute_wordpiece_score(freq_xy, freq_x, freq_y)\n",
    "        scores[pair] = score\n",
    "    if not scores:\n",
    "        return None, 0\n",
    "    best_pair = max(scores, key=scores.get)\n",
    "    return best_pair, scores[best_pair]\n",
    "\n",
    "def merge_pair_wordpiece(word_freq, pair_to_merge):\n",
    "    \"\"\"\n",
    "    åˆå¹¶æŒ‡å®šçš„å­—ç¬¦å¯¹åˆ°æ–°ç¬¦å·ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡\n",
    "        pair_to_merge: è¦åˆå¹¶çš„å­—ç¬¦å¯¹\n",
    "    è¿”å›ï¼š\n",
    "        æ›´æ–°åçš„å•è¯é¢‘æ¬¡åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    merged_word_freq = []\n",
    "    new_symbol = create_new_symbol(pair_to_merge[0], pair_to_merge[1])\n",
    "    for word, freq in word_freq:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            # æ£€æŸ¥å½“å‰å­—ç¬¦å’Œä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯è¦åˆå¹¶çš„å­—ç¬¦å¯¹\n",
    "            if (\n",
    "                i < len(word) - 1\n",
    "                and word[i] == pair_to_merge[0]\n",
    "                and word[i + 1] == pair_to_merge[1]\n",
    "            ):\n",
    "                new_word.append(new_symbol)\n",
    "                i += 2  # è·³è¿‡ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œå› ä¸ºå·²åˆå¹¶\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        merged_word_freq.append((new_word, freq))\n",
    "    return merged_word_freq\n",
    "\n",
    "def wordpiece_merge(word_freq, vocab_size):\n",
    "    \"\"\"\n",
    "    æ‰§è¡Œ WordPiece åˆå¹¶æ“ä½œï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°é¢„å®šå¤§å°ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡\n",
    "        vocab_size: é¢„å®šçš„è¯æ±‡è¡¨å¤§å°\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        æœ€ç»ˆè¯æ±‡è¡¨å’Œåˆå¹¶è®°å½•\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–è¯æ±‡è¡¨\n",
    "    vocab = set()\n",
    "    for word, _ in word_freq:\n",
    "        vocab.update(word)\n",
    "    merges = []\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_freq, char_freq = count_char_pairs_wordpiece(word_freq)\n",
    "        best_pair, best_score = find_best_pair_wordpiece(pair_freq, char_freq)\n",
    "        if not best_pair:\n",
    "            break\n",
    "        # åˆå¹¶æœ€ä½³å­—ç¬¦å¯¹\n",
    "        new_symbol = create_new_symbol(best_pair[0], best_pair[1])\n",
    "        word_freq = merge_pair_wordpiece(word_freq, best_pair)\n",
    "        vocab.add(new_symbol)\n",
    "        merges.append((best_pair, new_symbol))\n",
    "        print(\n",
    "            f\"Merge: {best_pair} -> {new_symbol}, Score: {best_score:.4f}, è¯æ±‡è¡¨å¤§å°: {len(vocab)}\"\n",
    "        )\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "word_freq = [\n",
    "    (['l', '##o', '##w'], 5),\n",
    "    (['l', '##o', '##w', '##e', '##r'], 2),\n",
    "    (['n', '##e', '##w', '##e', '##s', '##t'], 6),\n",
    "    (['w', '##i', '##d', '##e', '##s', '##t'], 3)\n",
    "]\n",
    "\n",
    "# é¢„å®šè¯æ±‡è¡¨å¤§å°ä¸º15\n",
    "final_vocab_wp, merge_records_wp = wordpiece_merge(word_freq, 15)\n",
    "\n",
    "print(\"\\næœ€ç»ˆè¯æ±‡è¡¨ V:\")\n",
    "print(final_vocab_wp)\n",
    "\n",
    "print(\"\\nåˆå¹¶è®°å½•:\")\n",
    "for idx, (pair, new_sym) in enumerate(merge_records_wp, 1):\n",
    "    print(f\"Merge {idx}: {pair} -> {new_sym}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a4da0-76fa-49b3-a327-525a47240d17",
   "metadata": {},
   "source": [
    "#### ğŸ“ ç»ƒä¹ é¢˜ç­”æ¡ˆ\n",
    "\n",
    "**Q1. æœ€åˆçš„è¯æ±‡è¡¨å¤§å°ä¸º 10ï¼Œå‡è®¾é¢„å®šå¤§å°ä¸º 13ï¼Œé‚£ä¹ˆå½“å‰çš„è¯æ±‡è¡¨ $V$ ä¸ºå¤šå°‘ï¼Ÿåˆå¹¶è®°å½•å‘¢ï¼Ÿ**\n",
    "\n",
    "- **åˆå§‹è¯æ±‡è¡¨ $V$**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd'}\n",
    "  ```\n",
    "\n",
    "  å¤§å°ä¸º 10ã€‚\n",
    "\n",
    "- **åˆå¹¶è®°å½•**ï¼š\n",
    "\n",
    "  1. åˆå¹¶ `(\"e\", \"s\")` -> `es`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 11ã€‚\n",
    "  2. åˆå¹¶ `(\"es\", \"t\")` -> `est`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 12ã€‚\n",
    "  3. åˆå¹¶ `(\"l\", \"o\")` -> `lo`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 13ã€‚\n",
    "\n",
    "- **æœ€ç»ˆè¯æ±‡è¡¨ $V$**ï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo'}\n",
    "  ```\n",
    "\n",
    "è¿è¡Œä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b1dd5a-2c25-467b-ba37-92ceed36df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('e', 's') -> es, è¯æ±‡è¡¨å¤§å°: 11\n",
      "Merge: ('es', 't') -> est, è¯æ±‡è¡¨å¤§å°: 12\n",
      "Merge: ('l', 'o') -> lo, è¯æ±‡è¡¨å¤§å°: 13\n",
      "\n",
      "æœ€ç»ˆè¯æ±‡è¡¨ V:\n",
      "{'n', 'es', 'i', 'est', 'lo', 't', 's', 'w', 'e', 'r', 'd', 'o', 'l'}\n",
      "\n",
      "åˆå¹¶è®°å½•:\n",
      "Merge 1: ('e', 's') -> es\n",
      "Merge 2: ('es', 't') -> est\n",
      "Merge 3: ('l', 'o') -> lo\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_char_pairs(word_freq):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å­—ç¬¦å¯¹çš„é¢‘æ¬¡ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i], word[i + 1])\n",
    "            pair_freq[pair] += freq\n",
    "    return pair_freq\n",
    "\n",
    "def find_best_pair(freq):\n",
    "    \"\"\"\n",
    "    æ‰¾åˆ°é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        freq: å­—ç¬¦å¯¹é¢‘æ¬¡çš„å­—å…¸\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹åŠå…¶é¢‘æ¬¡\n",
    "    \"\"\"\n",
    "    if not freq:\n",
    "        return None, 0\n",
    "    best_pair = max(freq, key=freq.get)\n",
    "    return best_pair, freq[best_pair]\n",
    "\n",
    "def merge_pair(word_freq, pair_to_merge):\n",
    "    \"\"\"\n",
    "    åˆå¹¶æŒ‡å®šçš„å­—ç¬¦å¯¹åˆ°æ–°ç¬¦å·ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡\n",
    "        pair_to_merge: è¦åˆå¹¶çš„å­—ç¬¦å¯¹\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        æ›´æ–°åçš„å•è¯é¢‘æ¬¡åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    merged_word_freq = []\n",
    "    pair_str = ''.join(pair_to_merge)\n",
    "    for word, freq in word_freq:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            # æ£€æŸ¥å½“å‰å­—ç¬¦å’Œä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯è¦åˆå¹¶çš„å­—ç¬¦å¯¹\n",
    "            if i < len(word) - 1 and word[i] == pair_to_merge[0] and word[i + 1] == pair_to_merge[1]:\n",
    "                new_word.append(pair_str)\n",
    "                i += 2  # è·³è¿‡ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œå› ä¸ºå·²åˆå¹¶\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        merged_word_freq.append((new_word, freq))\n",
    "    return merged_word_freq\n",
    "\n",
    "def bpe_merge(word_freq, vocab_size):\n",
    "    \"\"\"\n",
    "    æ‰§è¡Œ BPE åˆå¹¶æ“ä½œï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°é¢„å®šå¤§å°ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_freq: List of tuples, æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰å’Œå…¶é¢‘æ¬¡\n",
    "        vocab_size: é¢„å®šçš„è¯æ±‡è¡¨å¤§å°\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        æœ€ç»ˆè¯æ±‡è¡¨å’Œåˆå¹¶è®°å½•\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–è¯æ±‡è¡¨\n",
    "    vocab = set()\n",
    "    for word, _ in word_freq:\n",
    "        vocab.update(word)\n",
    "    merges = []\n",
    "    \n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_freq = count_char_pairs(word_freq)\n",
    "        best_pair, best_freq = find_best_pair(pair_freq)\n",
    "        if not best_pair:\n",
    "            break\n",
    "        # åˆå¹¶æœ€ä½³å­—ç¬¦å¯¹\n",
    "        word_freq = merge_pair(word_freq, best_pair)\n",
    "        new_symbol = ''.join(best_pair)\n",
    "        vocab.add(new_symbol)\n",
    "        merges.append((best_pair, new_symbol))\n",
    "        print(f\"Merge: {best_pair} -> {new_symbol}, è¯æ±‡è¡¨å¤§å°: {len(vocab)}\")\n",
    "            \n",
    "    return vocab, merges\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "word_freq = [\n",
    "    (['l', 'o', 'w'], 5),\n",
    "    (['l', 'o', 'w', 'e', 'r'], 2),\n",
    "    (['n', 'e', 'w', 'e', 's', 't'], 6),\n",
    "    (['w', 'i', 'd', 'e', 's', 't'], 3)\n",
    "]\n",
    "\n",
    "# é¢„å®šè¯æ±‡è¡¨å¤§å°ä¸º13\n",
    "final_vocab, merge_records = bpe_merge(word_freq, 13)\n",
    "\n",
    "print(\"\\næœ€ç»ˆè¯æ±‡è¡¨ V:\")\n",
    "print(final_vocab)\n",
    "\n",
    "print(\"\\nåˆå¹¶è®°å½•:\")\n",
    "for idx, (pair, new_sym) in enumerate(merge_records, 1):\n",
    "    print(f\"Merge {idx}: {pair} -> {new_sym}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e876d7-d9de-4c89-9727-d0330309edec",
   "metadata": {},
   "source": [
    "**Q2. å¦‚æœä»¥`</w>`ï¼ˆend-of-wordï¼‰ä½œä¸ºæ¯ä¸ªè¯­æ–™åº“ä¸­å•è¯çš„ç»“å°¾ï¼Œæœ€åˆçš„è¯æ±‡è¡¨ä¼šå—åˆ°ä»€ä¹ˆå½±å“ï¼Œåç»­çš„è¿‡ç¨‹å‘¢ï¼Ÿå‡è®¾é¢„å®šå¤§å°ä¸º 14ï¼Œå½“å‰çš„åˆå¹¶è®°å½•æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "- **åˆå§‹è¯æ±‡è¡¨ $V$**ï¼š\n",
    "\n",
    "  æ·»åŠ  `</w>` åï¼Œè¯æ±‡è¡¨å˜ä¸ºï¼š\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', '</w>'}\n",
    "  ```\n",
    "\n",
    "  å¤§å°ä¸º 11ã€‚\n",
    "\n",
    "- **å½±å“**ï¼š\n",
    "\n",
    "  åˆå¹¶è¿‡ç¨‹å’Œåˆå¹¶è®°å½•å°†ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå› ä¸º `</w>` çš„å­˜åœ¨ä¼šå½±å“å­—ç¬¦å¯¹çš„é¢‘æ¬¡ç»Ÿè®¡å’Œåˆå¹¶é¡ºåºã€‚\n",
    "\n",
    "- **åˆå¹¶è®°å½•**ï¼š\n",
    "\n",
    "  1. åˆå¹¶ `(\"e\", \"s\")` -> `es`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 12ã€‚\n",
    "  2. åˆå¹¶ `(\"es\", \"t\")` -> `est`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 13ã€‚\n",
    "  3. åˆå¹¶ `(\"est\", \"<\\w>\")` -> `est<\\w>`ï¼Œè¯æ±‡è¡¨å¤§å°å¢åŠ åˆ° 14ã€‚\n",
    "\n",
    "è¿è¡Œä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc46f02a-2533-423b-aa70-68d4f668a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('e', 's') -> es, è¯æ±‡è¡¨å¤§å°: 12\n",
      "Merge: ('es', 't') -> est, è¯æ±‡è¡¨å¤§å°: 13\n",
      "Merge: ('est', '</w>') -> est</w>, è¯æ±‡è¡¨å¤§å°: 14\n",
      "\n",
      "æœ€ç»ˆè¯æ±‡è¡¨ V:\n",
      "{'n', 'es', '</w>', 'i', 'est', 'est</w>', 't', 's', 'w', 'e', 'r', 'd', 'o', 'l'}\n",
      "\n",
      "åˆå¹¶è®°å½•:\n",
      "Merge 1: ('e', 's') -> es\n",
      "Merge 2: ('es', 't') -> est\n",
      "Merge 3: ('est', '</w>') -> est</w>\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹\n",
    "word_freq = [\n",
    "    (['l', 'o', 'w', '</w>'], 5),\n",
    "    (['l', 'o', 'w', 'e', 'r', '</w>'], 2),\n",
    "    (['n', 'e', 'w', 'e', 's', 't', '</w>'], 6),\n",
    "    (['w', 'i', 'd', 'e', 's', 't', '</w>'], 3)\n",
    "]\n",
    "\n",
    "# é¢„å®šè¯æ±‡è¡¨å¤§å°ä¸º14\n",
    "final_vocab, merge_records = bpe_merge(word_freq, 14)\n",
    "\n",
    "print(\"\\næœ€ç»ˆè¯æ±‡è¡¨ V:\")\n",
    "print(final_vocab)\n",
    "\n",
    "print(\"\\nåˆå¹¶è®°å½•:\")\n",
    "for idx, (pair, new_sym) in enumerate(merge_records, 1):\n",
    "    print(f\"Merge {idx}: {pair} -> {new_sym}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a88f0-86c3-4625-9b7d-c9f5e7c10909",
   "metadata": {},
   "source": [
    "### æ ‡è®°æ–‡æœ¬\n",
    "\n",
    "ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæ¯æ¬¡åˆå¹¶æ—¶æˆ‘ä»¬éƒ½ä¼šè®°å½•å¯¹åº”çš„ **merge** è§„åˆ™ï¼Œä½†å¹¶æœªè¯¦ç»†è¯´æ˜å…¶ä½œç”¨ï¼Œä¸‹é¢å°†ä»¥ BPE ä¸ºä¾‹è¿›è¡Œè§£é‡Šã€‚\n",
    "\n",
    "#### BPE\n",
    "\n",
    "åœ¨ä¹‹å‰çš„ç¤ºä¾‹ä¸­ï¼Œä¸‰è½®åˆå¹¶åå°†å¾—åˆ°ä»¥ä¸‹åˆå¹¶è§„åˆ™ï¼ˆæŒ‰åˆå¹¶é¡ºåºæ’åˆ—ï¼‰ï¼š  \n",
    "\n",
    "1. åˆå¹¶å­—ç¬¦å¯¹ `'e'` å’Œ `'s'`ï¼Œå¾—åˆ° `'es'`ã€‚\n",
    "2. åˆå¹¶å­—ç¬¦å¯¹ `'es'` å’Œ `'t'`ï¼Œå¾—åˆ° `'est'`ã€‚\n",
    "3. åˆå¹¶å­—ç¬¦å¯¹ `'l'` å’Œ `'o'`ï¼Œå¾—åˆ° `'lo'`ã€‚\n",
    "\n",
    "å‡è®¾å½“å‰è¯æ±‡è¡¨åŒ…å«æ‰€æœ‰å•ä¸ªå­—ç¬¦ï¼Œä¿®æ”¹[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/learn/nlp-course/en/chapter6/5)æœ€åæä¾›çš„ tokenize() ç¤ºä¾‹ä»£ç è¿›è¡Œæ¼”ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9511ff0-6d34-442e-80cd-b1119b896eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹é¢„åˆ†è¯ç»“æœ:\n",
      "['estimate', ',', 'Ä local']\n",
      "\n",
      "åˆå§‹æ‹†åˆ†ç»“æœ:\n",
      "[['e', 's', 't', 'i', 'm', 'a', 't', 'e'], [','], ['Ä ', 'l', 'o', 'c', 'a', 'l']]\n",
      "\n",
      "åº”ç”¨åˆå¹¶è§„åˆ™: ('e', 's') -> es\n",
      "  åˆå¹¶å‰ç¬¬ 1 ä¸ªå•è¯: ['e', 's', 't', 'i', 'm', 'a', 't', 'e']\n",
      "    åœ¨ä½ç½® 0 å¤„åˆå¹¶: ['es', 't', 'i', 'm', 'a', 't', 'e']\n",
      "  åˆå¹¶å‰ç¬¬ 2 ä¸ªå•è¯: [',']\n",
      "  åˆå¹¶å‰ç¬¬ 3 ä¸ªå•è¯: ['Ä ', 'l', 'o', 'c', 'a', 'l']\n",
      "\n",
      "åº”ç”¨åˆå¹¶è§„åˆ™: ('es', 't') -> est\n",
      "  åˆå¹¶å‰ç¬¬ 1 ä¸ªå•è¯: ['es', 't', 'i', 'm', 'a', 't', 'e']\n",
      "    åœ¨ä½ç½® 0 å¤„åˆå¹¶: ['est', 'i', 'm', 'a', 't', 'e']\n",
      "  åˆå¹¶å‰ç¬¬ 2 ä¸ªå•è¯: [',']\n",
      "  åˆå¹¶å‰ç¬¬ 3 ä¸ªå•è¯: ['Ä ', 'l', 'o', 'c', 'a', 'l']\n",
      "\n",
      "åº”ç”¨åˆå¹¶è§„åˆ™: ('l', 'o') -> lo\n",
      "  åˆå¹¶å‰ç¬¬ 1 ä¸ªå•è¯: ['est', 'i', 'm', 'a', 't', 'e']\n",
      "  åˆå¹¶å‰ç¬¬ 2 ä¸ªå•è¯: [',']\n",
      "  åˆå¹¶å‰ç¬¬ 3 ä¸ªå•è¯: ['Ä ', 'l', 'o', 'c', 'a', 'l']\n",
      "    åœ¨ä½ç½® 1 å¤„åˆå¹¶: ['Ä ', 'lo', 'c', 'a', 'l']\n",
      "\n",
      "æœ€ç»ˆæ‹†åˆ†ç»“æœ:\n",
      "[['est', 'i', 'm', 'a', 't', 'e'], [','], ['Ä ', 'lo', 'c', 'a', 'l']]\n",
      "\n",
      "æœ€ç»ˆç”Ÿæˆçš„ Tokens:\n",
      "['est', 'i', 'm', 'a', 't', 'e', ',', 'Ä ', 'lo', 'c', 'a', 'l']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # é¢„åˆ†è¯å¤„ç†ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºåˆæ­¥çš„å•è¯åˆ—è¡¨\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "\n",
    "    print(\"åˆå§‹é¢„åˆ†è¯ç»“æœ:\")\n",
    "    print(pre_tokenized_text)\n",
    "\n",
    "    # å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åˆ—è¡¨\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    print(\"\\nåˆå§‹æ‹†åˆ†ç»“æœ:\")\n",
    "    print(splits)\n",
    "\n",
    "    # éå†æ‰€æœ‰åˆå¹¶è§„åˆ™ï¼ˆmergesï¼‰ï¼Œé€æ­¥åº”ç”¨åˆ°æ‹†åˆ†åçš„ç»“æœä¸­\n",
    "    for pair, merge in merges.items():\n",
    "        print(f\"\\nåº”ç”¨åˆå¹¶è§„åˆ™: {pair} -> {merge}\")\n",
    "\n",
    "        # éå†æ¯ä¸ªå·²æ‹†åˆ†çš„å•è¯\n",
    "        for idx, split in enumerate(splits):\n",
    "            print(f\"  åˆå¹¶å‰ç¬¬ {idx+1} ä¸ªå•è¯: {split}\")\n",
    "            i = 0\n",
    "            # åœ¨å½“å‰æ‹†åˆ†çš„å­—ç¬¦ä¸­æŸ¥æ‰¾åŒ¹é…çš„å­—ç¬¦å¯¹\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    # åˆå¹¶å­—ç¬¦å¯¹\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    print(f\"    åœ¨ä½ç½® {i} å¤„åˆå¹¶: {split}\")\n",
    "                else:\n",
    "                    i += 1\n",
    "            # æ›´æ–°æ‹†åˆ†åçš„ç»“æœ\n",
    "            splits[idx] = split\n",
    "\n",
    "    print(\"\\næœ€ç»ˆæ‹†åˆ†ç»“æœ:\")\n",
    "    print(splits)\n",
    "\n",
    "    # å°†æ‰€æœ‰æ‹†åˆ†åçš„ç»“æœåˆå¹¶ä¸ºä¸€ä¸ª Token åˆ—è¡¨å¹¶è¿”å›\n",
    "    return sum(splits, [])\n",
    "\n",
    "# ç¤ºä¾‹ merges å­—å…¸\n",
    "merges = {\n",
    "    ('e', 's'): 'es',\n",
    "    ('es', 't'): 'est',\n",
    "    ('l', 'o'): 'lo'\n",
    "}\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬\n",
    "text = \"estimate, local\"\n",
    "\n",
    "# è°ƒç”¨ tokenize å‡½æ•°ï¼Œå¹¶æ‰“å°ä¸­é—´è¿‡ç¨‹\n",
    "tokens = tokenize(text)\n",
    "print(\"\\næœ€ç»ˆç”Ÿæˆçš„ Tokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951e873-4468-4a40-b959-f8ee645a4ec9",
   "metadata": {},
   "source": [
    "ä¸è¿‡ï¼Œåœ¨ä¹‹å‰çš„è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æœ€ç»ˆè¯æ±‡è¡¨ $V$ å¹¶æœªåŒ…å«æ‰€æœ‰å•ä¸ªå­—ç¬¦ï¼Œè€Œæ˜¯ï¼š  \n",
    "\n",
    "```\n",
    "{'e', 'r', 's', 'est', 'w', 'l', 'o', 'lo', 'es', 'i', 'n', 't', 'd'}\n",
    "```\n",
    "\n",
    "å› æ­¤ï¼Œå¯¹äºè¾“å…¥ `\"estimate, local\"`ï¼Œå…¶æ ‡è®°ç»“æœä¸ºï¼š  \n",
    "\n",
    "```\n",
    "['est', 'i', '[UNK]', 'a', 't', 'e', '[UNK]', 'lo', '[UNK]', '[UNK]', l]\n",
    "```\n",
    "\n",
    "è¿™é‡Œçš„ `'[UNK]'`ï¼ˆUNKNOWNï¼‰è¡¨ç¤ºè¯¥å­è¯ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå³å±äº **OOVï¼ˆOut-of-Vocabularyï¼‰** çš„æƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85de72-6c72-414e-a2ef-574f3f745350",
   "metadata": {},
   "source": [
    "#### WordPiece\n",
    "\n",
    "å’Œ BPE ä¸åŒï¼ŒWordPiece å¯¹ OOV é‡‡å–çš„æ˜¯ã€Œå®æ€é”™ä¸æ”¾è¿‡ã€ç­–ç•¥ï¼Œå³åªè¦æœ‰ä¸€ä¸ªå­—ç¬¦æ²¡è§è¿‡ï¼Œæ•´ä¸ªå•è¯éƒ½æ ‡è®°ä¸º `'[UNK]'`ã€‚\n",
    "\n",
    "ä¿®æ”¹[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/learn/nlp-course/en/chapter6/6)æœ€åæä¾›çš„ tokenize() ç¤ºä¾‹ä»£ç è¿›è¡Œæ¼”ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59dd5189-0e5c-430b-90bf-8794cdcd8410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "åˆå§‹é¢„åˆ†è¯ç»“æœ:\n",
      "['estimate', ',', 'local', ',', 'lows']\n",
      "\n",
      "æ­£åœ¨æ ‡è®°å•è¯: estimate\n",
      "  [UNK] æ ‡è®°: estimate\n",
      "  æ ‡è®°ç»“æœ: ['[UNK]']\n",
      "\n",
      "æ­£åœ¨æ ‡è®°å•è¯: ,\n",
      "  [UNK] æ ‡è®°: ,\n",
      "  æ ‡è®°ç»“æœ: ['[UNK]']\n",
      "\n",
      "æ­£åœ¨æ ‡è®°å•è¯: local\n",
      "  åŒ¹é…åˆ° Token: lo\n",
      "  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: ##cal\n",
      "  [UNK] æ ‡è®°: ##cal\n",
      "  æ ‡è®°ç»“æœ: ['[UNK]']\n",
      "\n",
      "æ­£åœ¨æ ‡è®°å•è¯: ,\n",
      "  [UNK] æ ‡è®°: ,\n",
      "  æ ‡è®°ç»“æœ: ['[UNK]']\n",
      "\n",
      "æ­£åœ¨æ ‡è®°å•è¯: lows\n",
      "  åŒ¹é…åˆ° Token: lo\n",
      "  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: ##ws\n",
      "  åŒ¹é…åˆ° Token: ##w\n",
      "  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: ##s\n",
      "  åŒ¹é…åˆ° Token: ##s\n",
      "  æ ‡è®°ç»“æœ: ['lo', '##w', '##s']\n",
      "\n",
      "æœ€ç»ˆæ ‡è®°ç»“æœ:\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', 'lo', '##w', '##s']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize(text):\n",
    "    # é¢„åˆ†è¯å¤„ç†ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºåˆæ­¥çš„å•è¯åˆ—è¡¨\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "\n",
    "    print(\"\\nåˆå§‹é¢„åˆ†è¯ç»“æœ:\")\n",
    "    print(pre_tokenized_text)\n",
    "\n",
    "    # å¯¹æ¯ä¸ªå•è¯è¿›è¡Œæ ‡è®°\n",
    "    tokenized_words = []\n",
    "    for word in pre_tokenized_text:\n",
    "        tokens = []\n",
    "        print(f\"\\næ­£åœ¨æ ‡è®°å•è¯: {word}\")\n",
    "        \n",
    "        while len(word) > 0:\n",
    "            i = len(word)\n",
    "            # å°è¯•åŒ¹é…è¯æ±‡è¡¨ä¸­çš„æœ€é•¿å­è¯\n",
    "            while i > 0 and word[:i] not in vocab:\n",
    "                i -= 1\n",
    "            if i == 0:\n",
    "                print(f\"  [UNK] æ ‡è®°: {word}\")\n",
    "                tokens = [\"[UNK]\"]  # æ²¡æœ‰åŒ¹é…åˆ°åˆ™è¿”å› [UNK]\n",
    "                break  # è·³å‡ºå¾ªç¯ï¼Œä¸å†ç»§ç»­å¤„ç†è¯¥å•è¯\n",
    "\n",
    "            # åŒ¹é…åˆ°å­è¯ï¼Œæ·»åŠ åˆ° tokens åˆ—è¡¨ä¸­\n",
    "            matched_token = word[:i]\n",
    "            tokens.append(matched_token)\n",
    "            print(f\"  åŒ¹é…åˆ° Token: {matched_token}\")\n",
    "\n",
    "            # æ›´æ–°å‰©ä½™éƒ¨åˆ†ï¼Œå¹¶æ·»åŠ â€œ##â€ä½œä¸ºå‰ç¼€\n",
    "            word = word[i:]\n",
    "            if len(word) > 0:\n",
    "                word = f\"##{word}\"\n",
    "                print(f\"  å‰©ä½™éƒ¨åˆ†æ·»åŠ å‰ç¼€: {word}\")\n",
    "\n",
    "        print(f\"  æ ‡è®°ç»“æœ: {tokens}\")\n",
    "        tokenized_words.append(tokens)\n",
    "\n",
    "    print(\"\\næœ€ç»ˆæ ‡è®°ç»“æœ:\")\n",
    "    flattened_tokens = sum(tokenized_words, [])  # å±•å¹³æˆå•å±‚åˆ—è¡¨\n",
    "    print(flattened_tokens)\n",
    "\n",
    "    return flattened_tokens\n",
    "\n",
    "# ç¤ºä¾‹è¯æ±‡è¡¨\n",
    "vocab = {'##st', 'n', '##i', '##s', 'wid', '##d', 'wi', '##r', '##o', \n",
    "         'lo', 'w', '##e', '##w', '##t', 'l'}\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬\n",
    "text = \"estimate, local, lows\"\n",
    "\n",
    "# ä½¿ç”¨ BERT çš„åˆ†è¯å™¨ï¼ˆWordPieceï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# è°ƒç”¨ tokenize å‡½æ•°ï¼Œå¹¶æ‰“å°ä¸­é—´è¿‡ç¨‹\n",
    "tokens = tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32bb25-8169-4237-9216-05df067be214",
   "metadata": {},
   "source": [
    "## æ˜ å°„ï¼ˆMappingï¼‰\n",
    "\n",
    "ä»¥ BPE ä¸ºä¾‹ï¼Œæœ€ç»ˆè¯æ±‡è¡¨ $V$ ä¸­çš„ Token å’Œå¯¹åº”çš„é¢‘æ¬¡åˆ†åˆ«ä¸ºï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28723d53-2500-4986-a363-70688c16ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'lo': 7,\n",
    "    'w': 16,\n",
    "    'e': 8,\n",
    "    'r': 2,\n",
    "    'n': 6,\n",
    "    'est': 9,\n",
    "    'i': 3,\n",
    "    'd': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e716182-86fc-47d7-b9cc-6d06ff85d9aa",
   "metadata": {},
   "source": [
    "ç®€å•å®ç° Token å’Œ ID ä¹‹é—´çš„æ˜ å°„å…³ç³»çš„ä»£ç ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff684d26-c327-4d87-8aad-a31ecf11f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID: {'lo': 0, 'w': 1, 'e': 2, 'r': 3, 'n': 4, 'est': 5, 'i': 6, 'd': 7}\n",
      "ID to Token: {0: 'lo', 1: 'w', 2: 'e', 3: 'r', 4: 'n', 5: 'est', 6: 'i', 7: 'd'}\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»º token åˆ° ID çš„æ˜ å°„\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "# åˆ›å»º ID åˆ° token çš„æ˜ å°„\n",
    "id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
    "\n",
    "# æ‰“å°æ˜ å°„å…³ç³»\n",
    "print(\"Token to ID:\", token_to_id)\n",
    "print(\"ID to Token:\", id_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903c7c9-c792-4520-ab64-6fe7dfd456c8",
   "metadata": {},
   "source": [
    "å½“ç„¶ï¼Œä¹Ÿå¯ä»¥æ ¹æ®é¢‘æ¬¡æˆ–è€…å…¶ä»–è§„åˆ™è¿›è¡Œç‰¹æ®Šå¤„ç†ã€‚\n",
    "\n",
    "\n",
    "ä»¥ä¸Šæ˜¯ç¼–ç éƒ¨åˆ†çš„æ¦‚è¿°ï¼Œå®é™…ä¸Šåœ¨æ–‡æœ¬é¢„å¤„ç†çš„æ—¶å€™è¿˜ä¼šå¢åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä½†è¿™äº›ä»¥åŠåç»­çš„è§£ç éƒ¨åˆ†å¤§å¤šæ˜¯ä¸€äº›æ–‡æœ¬å¤„ç†çš„è§„åˆ™ï¼Œè¿™é‡Œå°±ä¸è¿‡å¤šèµ˜è¿°äº†ï¼ŒTokenizer ä¹‹é—´çš„æ ¸å¿ƒå·®å¼‚åœ¨äºä½¿ç”¨çš„åˆ†å‰²æ–¹æ³•å’Œè¯æ±‡è¡¨çš„æ„å»ºç­–ç•¥ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f0d6b-fccd-45ed-92c2-127512850764",
   "metadata": {},
   "source": [
    "## æ‹“å±•ï¼ˆTransformersï¼‰\n",
    "\n",
    "åœ¨ Transformers ä¸­ï¼Œ**åˆ†è¯ï¼ˆtokenizationï¼‰** å®é™…ä¸ŠåŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š  \n",
    "\n",
    "1. **æ ‡å‡†åŒ–ï¼ˆNormalizationï¼‰**ï¼šå¯¹æ–‡æœ¬è¿›è¡Œå¿…è¦çš„æ¸…ç†æ“ä½œï¼Œä¾‹å¦‚åˆ é™¤å¤šä½™ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€è¿›è¡Œ Unicode æ ‡å‡†åŒ–ç­‰ã€‚\n",
    "2. **é¢„åˆ†è¯ï¼ˆPre-tokenizationï¼‰**ï¼šå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ã€‚\n",
    "3. **é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥ï¼ˆRunning the input through the modelï¼‰**ï¼šä½¿ç”¨é¢„åˆ†è¯åçš„å•è¯ç”Ÿæˆä¸€ç³»åˆ—è¯å…ƒï¼ˆtokensï¼‰ã€‚\n",
    "4. **åå¤„ç†ï¼ˆPost-processingï¼‰**ï¼šæ·»åŠ åˆ†è¯å™¨çš„ç‰¹æ®Šæ ‡è®°ï¼Œç”Ÿæˆæ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰å’Œè¯å…ƒç±»å‹ IDï¼ˆtoken type IDsï¼‰ã€‚\n",
    "\n",
    "[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/learn/nlp-course/en/chapter6/8)ç»™å‡ºäº†ä¸€å¼ æ•´ä½“æµç¨‹å›¾ï¼š\n",
    "\n",
    "![en_chapter6_tokenization_pipeline](../Guide/assets/en_chapter6_tokenization_pipeline.svg)\n",
    "\n",
    "è¿è¡Œä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0385adda-9eea-456e-99b3-a8cca01ad27c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ–‡æœ¬: Hello how are U tday\n",
      "æ ‡å‡†åŒ–åçš„æ–‡æœ¬: hello how are u tday\n",
      "é¢„åˆ†è¯ç»“æœ: [('hello', (0, 5)), ('how', (6, 9)), ('are', (10, 13)), ('u', (14, 15)), ('tday', (16, 20))]\n",
      "è¯å…ƒï¼ˆTokensï¼‰: ['hello', 'how', 'are', 'u', 'td', '##ay']\n",
      "è¯å…ƒ IDï¼ˆToken IDsï¼‰: [7592, 2129, 2024, 1057, 14595, 4710]\n",
      "ç¼–ç ç»“æœ: {'input_ids': tensor([[  101,  7592,  2129,  2024,  1057, 14595,  4710,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰: tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "è§£ç åçš„æ–‡æœ¬: hello how are u tday\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# åŠ è½½ BERT çš„åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# åŸå§‹æ–‡æœ¬\n",
    "text = \"Hello how are U tday\"\n",
    "print(\"åŸå§‹æ–‡æœ¬:\", text)\n",
    "\n",
    "# 1. æ ‡å‡†åŒ–ï¼šè½¬æ¢ä¸ºå°å†™\n",
    "normalized_text = text.lower()\n",
    "print(\"æ ‡å‡†åŒ–åçš„æ–‡æœ¬:\", normalized_text)\n",
    "\n",
    "# 2. é¢„åˆ†è¯ï¼ˆPre-tokenizationï¼‰ï¼šå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯\n",
    "pre_tokenized = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized_text)\n",
    "print(\"é¢„åˆ†è¯ç»“æœ:\", pre_tokenized)\n",
    "\n",
    "# 3. åˆ†è¯ï¼šå°†é¢„åˆ†è¯åçš„ç»“æœè½¬æ¢ä¸ºå­è¯çº§è¯å…ƒ\n",
    "tokens = tokenizer.tokenize(normalized_text)\n",
    "print(\"è¯å…ƒï¼ˆTokensï¼‰:\", tokens)\n",
    "\n",
    "# 4. å°† tokens è½¬æ¢ä¸º token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"è¯å…ƒ IDï¼ˆToken IDsï¼‰:\", token_ids)\n",
    "\n",
    "# 5. ç¼–ç ï¼ˆåŒ…å«ç‰¹æ®Šæ ‡è®°å’Œåå¤„ç†ï¼‰\n",
    "encoded = tokenizer(normalized_text, return_tensors=\"pt\")\n",
    "print(\"ç¼–ç ç»“æœ:\", encoded)\n",
    "\n",
    "# 6. æ‰“å°æ³¨æ„åŠ›æ©ç å’Œè¯å…ƒç±»å‹ IDï¼ˆåå¤„ç†éƒ¨åˆ†ï¼‰\n",
    "print(\"æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰:\", encoded[\"attention_mask\"])\n",
    "print(\"è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰:\", encoded[\"token_type_ids\"])\n",
    "\n",
    "# 7. è§£ç ï¼šå°† token IDs è½¬æ¢å›æ–‡æœ¬\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"è§£ç åçš„æ–‡æœ¬:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6a2ed-cec9-4b68-8957-c89adf62bd0f",
   "metadata": {},
   "source": [
    "### Qï¼šæ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰å’Œè¯å…ƒç±»å‹ ID ï¼ˆToken Type IDsï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "**æ³¨æ„åŠ›æ©ç **ç¡®ä¿æ¨¡å‹åªå…³æ³¨å®é™…çš„è¯å…ƒï¼Œå¿½ç•¥å¡«å……éƒ¨åˆ†ï¼Œä»è€Œé¿å…æ— æ•ˆçš„è®¡ç®—ï¼š\n",
    "\n",
    "- **1**ï¼šè¡¨ç¤ºæ¨¡å‹åº”å…³æ³¨çš„è¯å…ƒï¼ˆTokensï¼‰\n",
    "- **0**ï¼šè¡¨ç¤ºæ¨¡å‹åº”å¿½ç•¥çš„è¯å…ƒï¼ˆé€šå¸¸æ˜¯å¡«å…… `padding` çš„éƒ¨åˆ†ï¼‰ã€‚\n",
    "\n",
    "åœ¨ä¹‹å‰çš„[æ–‡ç« ](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/16.%20ç”¨%20LoRA%20å¾®è°ƒ%20Stable%20Diffusionï¼šæ‹†å¼€ç‚¼ä¸¹ç‚‰ï¼ŒåŠ¨æ‰‹å®ç°ä½ çš„ç¬¬ä¸€æ¬¡%20AI%20ç»˜ç”».md#æ€ä¹ˆè®©æ¨¡å‹ç†è§£æ–‡æœ¬)ä¸­æ›¾å±•ç¤ºè¿‡æ³¨æ„åŠ›æ©ç åœ¨ `padding=\"max_length\"` ä¸‹çš„è¡¨ç°ã€‚\n",
    "\n",
    "**è¯å…ƒç±»å‹ ID** ç”¨äºåŒºåˆ†è¾“å…¥ä¸­çš„ä¸åŒå¥å­æˆ–æ®µè½ï¼š\n",
    "\n",
    "- **0**ï¼šè¡¨ç¤ºç¬¬ä¸€ä¸ªå¥å­çš„è¯å…ƒã€‚\n",
    "- **1**ï¼šè¡¨ç¤ºç¬¬äºŒä¸ªå¥å­çš„è¯å…ƒã€‚\n",
    "\n",
    "è¿è¡Œä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9398d22e-7fa6-465d-a187-d3f664d2085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰: tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "è§£ç åçš„æ–‡æœ¬: [CLS] hello how are you [SEP] i am fine thank you [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# åŠ è½½ BERT çš„åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ä¸¤ä¸ªå¥å­\n",
    "text_a = \"Hello how are you\"\n",
    "text_b = \"I am fine thank you\"\n",
    "\n",
    "# ç¼–ç ä¸¤ä¸ªå¥å­\n",
    "encoded = tokenizer(text_a, text_b, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# æ‰“å°è¯å…ƒç±»å‹ ID\n",
    "print(\"è¯å…ƒç±»å‹ IDï¼ˆToken Type IDsï¼‰:\", encoded[\"token_type_ids\"])\n",
    "\n",
    "# è§£ç \n",
    "decoded_text = tokenizer.decode(encoded[\"input_ids\"][0])\n",
    "print(\"è§£ç åçš„æ–‡æœ¬:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b802a-34a8-4251-a76e-144ed3c49daa",
   "metadata": {},
   "source": [
    "## å‚è€ƒé“¾æ¥\n",
    "\n",
    "- [Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    "- [WordPiece tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebfbb6-fa74-4449-9acc-da34b7ef9f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
