{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186f8d20-3165-4ea1-96d3-6dfa9b8859b4",
   "metadata": {},
   "source": [
    "# b. ä½¿ç”¨ Llama-cpp-python åŠ è½½é‡åŒ–åçš„ LLM å¤§æ¨¡å‹ï¼ˆGGUFï¼‰\n",
    "\n",
    "> å¼•å¯¼æ–‡ç« ï¼š[19b. ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨ Llama-cpp-python æœ¬åœ°è¿è¡Œé‡åŒ– LLM å¤§æ¨¡å‹ï¼ˆGGUFï¼‰](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19b.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Llama-cpp-python%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGGUFï¼‰.md)\n",
    "\n",
    "ä»£ç æ–‡ä»¶æ²¡æœ‰æ˜¾å¡è¦æ±‚ï¼Œåœ¨ä¸ªäººè®¡ç®—æœºä¸Šå‡å¯è¿›è¡Œå¯¹è¯ã€‚\n",
    "\n",
    "**æ¨¡å‹æ–‡ä»¶çº¦ä¸º 4 GB**ã€‚\n",
    "\n",
    "è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç®€å•çš„ [ğŸ¡ AI Chat è„šæœ¬](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/chat.py)ä¾›ä½ å°è¯•ï¼Œè¯¦è§ï¼š[CodePlayground](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/CodePlayground/README.md#å½“å‰çš„ç©å…·)ï¼Œç‚¹å‡» `â–º` æˆ–å¯¹åº”çš„æ–‡æœ¬å±•å¼€ã€‚\n",
    "\n",
    "Transformers å…³äº GPTQ & AWQ åŠ è½½çš„ç›¸å…³é“¾æ¥ï¼š[æ–‡ç«  19a](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/19a.%20ä»åŠ è½½åˆ°å¯¹è¯ï¼šä½¿ç”¨%20Transformers%20æœ¬åœ°è¿è¡Œé‡åŒ–%20LLM%20å¤§æ¨¡å‹ï¼ˆGPTQ%20%26%20AWQï¼‰.md) | [ä»£ç æ–‡ä»¶ 16a](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Demos/16a.%20ä½¿ç”¨%20Transformers%20åŠ è½½é‡åŒ–åçš„%20LLM%20å¤§æ¨¡å‹ï¼ˆGPTQ%20%26%20AWQï¼‰.ipynb)\n",
    "\n",
    "åœ¨çº¿é“¾æ¥ï¼š[Kaggle - b](https://www.kaggle.com/code/aidemos/16b-llama-cpp-python-llm-gguf) | [Colab - b](https://colab.research.google.com/drive/1AhgC0qDaqWBXAI9eSbwTStGgvgFfLOpf?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d1707-d7e2-4d0b-8fa6-807892c52bc2",
   "metadata": {},
   "source": [
    "## Llama-cpp-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d8b2a",
   "metadata": {},
   "source": [
    "### ç¯å¢ƒé…ç½®\n",
    "\n",
    "ä¸ºäº†ç¡®ä¿ä¹‹åçš„ \"offload\" æ­£å¸¸å·¥ä½œï¼Œéœ€è¦è¿›è¡Œä¸€äº›é¢å¤–çš„å·¥ä½œã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œæ‰¾åˆ° CUDA çš„å®‰è£…è·¯å¾„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /usr/local -name \"cuda\" -exec readlink -f {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48546e57",
   "metadata": {},
   "source": [
    "å¤åˆ¶å¯¹åº”ï¼ˆæœ€çŸ­ï¼‰çš„è·¯å¾„ï¼Œä¿®æ”¹ `cuda_home` ç¯å¢ƒå˜é‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d687df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "cuda_home = \"/usr/local/cuda-12.1\"  # è¯·æ›¿æ¢ä¸ºä½ çš„è·¯å¾„\n",
    "\n",
    "command = f\"\"\"\n",
    "CMAKE_ARGS=\"-DGGML_CUDA=on \\\n",
    "            -DCUDA_PATH={cuda_home} \\\n",
    "            -DCUDAToolkit_ROOT={cuda_home} \\\n",
    "            -DCUDAToolkit_INCLUDE_DIR={cuda_home}/include \\\n",
    "            -DCUDAToolkit_LIBRARY_DIR={cuda_home}/lib64 \\\n",
    "            -DCMAKE_CUDA_COMPILER={cuda_home}/bin/nvcc\" \\\n",
    "FORCE_CMAKE=1 \\\n",
    "pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir --verbose\n",
    "\"\"\"\n",
    "\n",
    "subprocess.run(command, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352a092",
   "metadata": {},
   "source": [
    "### GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5047589",
   "metadata": {},
   "source": [
    "#### å®‰è£…åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3af47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d751d",
   "metadata": {},
   "source": [
    "#### å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460326a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd1c6e",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä»‹ç»ä¸¤ç§å¯¼å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œå®é™…æ‰§è¡Œæ—¶æœ¬åœ°/è‡ªåŠ¨å¯¼å…¥äºŒé€‰ä¸€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52283fcc",
   "metadata": {},
   "source": [
    "#### æœ¬åœ°å¯¼å…¥æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœä½ å·²ç»é…ç½®è¿‡äº†ï¼Œå¯ä»¥ç›´æ¥åœ¨ Notebook ä¸­æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ä¸‹è½½ã€‚\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "!./hfd.sh bartowski/Mistral-7B-Instruct-v0.3-GGUF --include Mistral-7B-Instruct-v0.3-Q4_K_M.gguf --tool aria2c -x 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05857047",
   "metadata": {},
   "source": [
    "æ ¹æ®æ¨¡å‹è·¯å¾„å¯¼å…¥æ¨¡å‹ï¼Œæ³¨æ„ï¼Œæ–‡ä»¶ä½äº `<repo_id>` æ–‡ä»¶å¤¹ä¸‹ï¼Œä»¥å½“å‰ä¸‹è½½çš„æ–‡ä»¶ä¸ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b244a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„\n",
    "model_path = \"./Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    # n_gpu_layers=-1,  # å–æ¶ˆæ³¨é‡Šä½¿ç”¨ GPU åŠ é€Ÿ\n",
    "    # verbose=False,  # å–æ¶ˆæ³¨é‡Šå‡å°‘æ¨¡å‹æ–¹é¢çš„ä¿¡æ¯æ‰“å°\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfa7fc",
   "metadata": {},
   "source": [
    "#### è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å…¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1c736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# æŒ‡å®šä»“åº“çš„åç§°å’Œæ–‡ä»¶å\n",
    "repo_id = \"bartowski/Mistral-7B-Instruct-v0.3-GGUF\"\n",
    "filename = \"Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n",
    "#filename = \"*Q4_K_M.gguf\"  # ä½¿ç”¨é€šé…ç¬¦ä¹Ÿæ˜¯å¯ä»¥çš„\n",
    "\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=repo_id,\n",
    "    filename=filename,\n",
    "    #n_gpu_layers=-1,  # å–æ¶ˆæ³¨é‡Šä½¿ç”¨ GPU åŠ é€Ÿ\n",
    "    #verbose=False,  # å–æ¶ˆæ³¨é‡Šå‡å°‘æ¨¡å‹æ–¹é¢çš„ä¿¡æ¯æ‰“å°\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f7cf7",
   "metadata": {},
   "source": [
    "#### æ¨ç†æµ‹è¯•\n",
    "\n",
    "ç®€å•ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†æµ‹è¯•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e190a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¾“å…¥æ–‡æœ¬\n",
    "input_text = \"Hello, World!\"\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡º\n",
    "output = llm(input_text, max_tokens=50)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314951dd-5b1e-4dc1-ac87-d920a4ab9e48",
   "metadata": {},
   "source": [
    "### æµå¼è¾“å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355e797-8025-4019-890f-4d5b10bf49d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }],\n",
    "    max_tokens=200,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in output:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'role' in delta:\n",
    "        print(delta['role'], end=': ', flush=True)\n",
    "    elif 'content' in delta:\n",
    "        print(delta['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f36bc-82f8-47da-84b4-785ef804ad61",
   "metadata": {},
   "source": [
    "æŸ¥çœ‹ output çš„æ„é€ ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f847f8-73dd-4ba8-b524-7b4a85e96665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "prompt = \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }],\n",
    "    max_tokens=200,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(type(output))\n",
    "\n",
    "# å°†ç”Ÿæˆå™¨è½¬æ¢ä¸ºåˆ—è¡¨\n",
    "output_list = list(islice(output, 3))\n",
    "\n",
    "# è·å–å‰ 3 ä¸ªæ¡ç›®\n",
    "output_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1806f8-9509-4dbf-9662-c1bf0934fef1",
   "metadata": {},
   "source": [
    "å°†åˆšåˆšå¯¹äºæµå¼è¾“å‡ºçš„å¤„ç†æŠ½è±¡ä¸ºå‡½æ•°ä¾¿äºåç»­è°ƒç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9145b07-1cd3-43df-8eba-21794ac91fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_stream_output(output):\n",
    "    \"\"\"\n",
    "    å¤„ç†æµå¼è¾“å‡ºï¼Œå°†ç”Ÿæˆçš„å†…å®¹é€æ­¥æ‰“å°å‡ºæ¥ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        output: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œæ¥è‡ª create_chat_completion çš„æµå¼è¾“å‡º\n",
    "    \"\"\"\n",
    "    for chunk in output:\n",
    "        delta = chunk['choices'][0]['delta']\n",
    "        if 'role' in delta:\n",
    "            print(f\"{delta['role']}: \", end='', flush=True)\n",
    "        elif 'content' in delta:\n",
    "            print(delta['content'], end='', flush=True)\n",
    "            \n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "prompt = \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }],\n",
    "    max_tokens=200,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "handle_stream_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554686f8-2b6f-422a-8701-e8af93f1c2b5",
   "metadata": {},
   "source": [
    "### å¤šè½®å¯¹è¯\n",
    "\n",
    "è®©æˆ‘ä»¬è‡ªå®šä¹‰ä¸€ä¸ªäº¤äº’çš„å¯¹è¯ç±»ï¼ˆéœ€è¦æ³¨æ„åˆ° handle_stream_output() æœ‰æ‰€ä¿®æ”¹ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8d8f6-2656-43af-a53e-b49cca1ec166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "def handle_stream_output(output):\n",
    "    \"\"\"\n",
    "    å¤„ç†æµå¼è¾“å‡ºï¼Œå°†ç”Ÿæˆçš„å†…å®¹é€æ­¥æ‰“å°å‡ºæ¥ï¼Œå¹¶æ”¶é›†å®Œæ•´çš„å›å¤ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        output: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œæ¥è‡ª create_chat_completion çš„æµå¼è¾“å‡º\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        response: å®Œæ•´çš„å›å¤æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    for chunk in output:\n",
    "        delta = chunk['choices'][0]['delta']\n",
    "        if 'role' in delta:\n",
    "            print(f\"{delta['role']}: \", end='', flush=True)\n",
    "        elif 'content' in delta:\n",
    "            content = delta['content']\n",
    "            print(content, end='', flush=True)\n",
    "            response += content\n",
    "    return response\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.messages = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"\n",
    "        æ·»åŠ ä¸€æ¡æ¶ˆæ¯åˆ°ä¼šè¯ä¸­ã€‚\n",
    "\n",
    "        å‚æ•°ï¼š\n",
    "            role: æ¶ˆæ¯è§’è‰²ï¼Œé€šå¸¸ä¸º 'user' æˆ– 'assistant'\n",
    "            content: æ¶ˆæ¯å†…å®¹\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_response_stream(self, user_input):\n",
    "        \"\"\"\n",
    "        è·å–æ¨¡å‹å¯¹ç”¨æˆ·è¾“å…¥çš„å“åº”ï¼ˆæµå¼è¾“å‡ºï¼‰ã€‚\n",
    "\n",
    "        å‚æ•°ï¼š\n",
    "            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬\n",
    "\n",
    "        è¿”å›ï¼š\n",
    "            response: å®Œæ•´çš„å›å¤æ–‡æœ¬\n",
    "        \"\"\"\n",
    "        self.add_message(\"user\", user_input)\n",
    "        \n",
    "        try:\n",
    "            output = self.llm.create_chat_completion(\n",
    "                messages=self.messages,\n",
    "                stream=True  # å¼€å¯æµå¼è¾“å‡º\n",
    "            )\n",
    "            \n",
    "            response = handle_stream_output(output)  # åŒæ—¶æ‰“å°å’Œæ”¶é›†å›å¤\n",
    "            \n",
    "            self.add_message(\"assistant\", response.strip())\n",
    "            # print(len(response),len(response.strip()))\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nå‘ç”Ÿé”™è¯¯: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ï¼ˆå‡è®¾ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰\n",
    "model_path = \"./Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,  # æ ¹æ®éœ€è¦å¸è½½åˆ° GPU\n",
    "    n_ctx=4096,       # è®¾ç½®ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œå°è¯•æ³¨é‡Šè¿™è¡Œè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆ\n",
    "    verbose=False,    # ç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º\n",
    ")\n",
    "\n",
    "# åˆ›å»ºä¼šè¯å®ä¾‹\n",
    "chat = ChatSession(llm)\n",
    "        \n",
    "# å¼€å§‹å¯¹è¯\n",
    "while True:\n",
    "    prompt = input(\"User: \")\n",
    "    # é€€å‡ºå¯¹è¯æ¡ä»¶ï¼ˆå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ç»ˆæ­¢ä»£ç å—ï¼‰\n",
    "    if prompt.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    chat.get_response_stream(prompt)\n",
    "    print()  # æ¢è¡Œä»¥ä¾¿ä¸‹ä¸€æ¬¡è¾“å…¥ï¼Œè¿™æ˜¯å› ä¸ºä¹‹å‰çš„ print éƒ½è®¾ç½®äº† end=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62026caf-d39c-4e86-80f9-526d49765c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ bye é€€å‡ºä¸Šé¢çš„å¯¹è¯åæ‰“å°æŸ¥çœ‹\n",
    "print(chat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c516583-336c-411b-afc3-0e53f3b20ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
