{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13831c9-2a09-40e7-98ae-3ee6be8a52cd",
   "metadata": {},
   "source": [
    "# å°è¯•ä½¿ç”¨ LoRA å¾®è°ƒ Stable Diffusion æ¨¡å‹ï¼ˆæ–‡ç”Ÿå›¾ï¼‰\n",
    "\n",
    "> [HW10: Stable Diffusion Fine-tuning](https://colab.research.google.com/drive/1dI_-HVggxyIwDVoreymviwg6ZOvEHiLS?usp=sharing#scrollTo=CnJtiRaRuTFX) ä¸­æ–‡é•œåƒç‰ˆ\n",
    ">\n",
    "> è¿™æ¬¡ä¸æ˜¯1:1çš„é•œåƒï¼Œæˆ‘é‡æ„äº†ä¸»è¦çš„ä»£ç é€»è¾‘ï¼Œä»¥ä¾¿äºé˜…è¯»ã€‚å¯¹äºå¯ä»¥è®¿é—® Colab çš„åŒå­¦æ¥è¯´ï¼Œä¸€æ ·å»ºè®®ä½¿ç”¨ä¸‹é¢è¿™ä»½ä»£ç è¿›è¡Œå­¦ä¹ ã€‚\n",
    ">\n",
    "> ä¿®æ”¹ï¼š\n",
    "> 1. Colab ä¸­çš„ä»£ç æ¯æ¬¡è®­ç»ƒæ—¶å…ˆå¯¼å…¥äº† checkpoint-last æ–‡ä»¶å¤¹ä¸‹çš„ unet å’Œ text_encoderï¼Œè¿™å°±æ„å‘³ç€æ¯æ¬¡é‡æ–°è¿è¡Œæ˜¯å¼ºåˆ¶ resume çš„ã€‚\n",
    "> æˆ‘å¯¹è¿™ä¸ªè¡Œä¸ºè¿›è¡Œäº†ä¿®æ”¹ï¼Œä½ ç°åœ¨å¯ä»¥é€šè¿‡æŒ‡å®š prepare_lora_model ä¸­çš„ resume å‚æ•°æ¥é€‰æ‹©æ˜¯å¦æ¥ç€ä¹‹å‰çš„è¿›è¡Œè®­ç»ƒã€‚\n",
    "> å»ºè®®è®¾ç½®ä¸º Falseï¼Œæ¯æ¬¡é‡æ–°è¿è¡Œæ‰èƒ½å¯¹ä¸åŒå‚æ•°æœ‰ç›´è§‚çš„æ„Ÿå—ã€‚\n",
    ">\n",
    "> 2. åŸä»£ç ä¸­å¯¼å…¥äº† PEFT å¹¶è®¾ç½®äº† LoRA å‚æ•°å´æ²¡æœ‰ç”¨åˆ°ï¼Œå‡ºäºå­¦ä¹ è€Œéæäº¤ä½œä¸šçš„ç›®çš„ï¼Œæˆ‘ä¼šé‡æ–°å†™è¿™ä¸€æ¨¡å—ã€‚\n",
    "> é‡å†™ä¹‹åçš„å¯è®­ç»ƒå‚æ•°å±‚ä¸æ•°é‡å·²ç»ç»è¿‡éªŒè¯ï¼Œä¸ [yahcreeper/GenAI-HW10-Model](https://huggingface.co/yahcreeper/GenAI-HW10-Model/tree/main/checkpoint-last) å®Œå…¨ä¸€è‡´ï¼Œå¯æ”¾å¿ƒä½¿ç”¨å½“å‰ç‰ˆæœ¬ï¼ˆå…³äºå¯è®­ç»ƒæ•°é‡ï¼Œå¯ä»¥å‚è€ƒ[ã€Š14. PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRA.mdã€‹](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/14.%20PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨%20LoRA.md)ä½¿ç”¨ `print_trainable_parameters()` è¿›è¡Œæ‰“å°ï¼‰ã€‚\n",
    ">\n",
    "> 3. ä¼šå¢åŠ æ³¨é‡Šä»¥ä¾¿ç†è§£ã€‚\n",
    "> \n",
    "\n",
    "ä½ å°†å¾®è°ƒä½ è‡ªå·±çš„ Stable Diffusion æ¨¡å‹ï¼Œä»ç»™å®šçš„æ–‡æœ¬æè¿°ç”Ÿæˆè‡ªå®šä¹‰å›¾åƒï¼Œåœ¨åˆå§‹åŒ–é¡¹ç›®ä¹‹åï¼Œä½ å°†å¯ä»¥åœ¨ `SD/Datasets/Brad` ä¸‹çœ‹åˆ°å›¾åƒå’Œå¯¹åº”çš„æ–‡æœ¬æè¿°ã€‚\n",
    "\n",
    "å½“å‰ä»£ç ä½¿ç”¨åˆ°çš„æ•°æ®é›† `dataset.zip` å·²ç»æ”¾åœ¨ Demos/data/14 ä¸‹é¢ã€‚\n",
    "\n",
    "ä½ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä»½ä»£ç å€¾å‘äºå¦‚ä½•å¾®è°ƒè€ŒéåŠ è½½æ•°æ®é›†æˆ–è€…è¯„ä¼°ï¼Œå¯¹äºåˆå­¦è€…ï¼Œå¯ä»¥ä»…å…³æ³¨è®­ç»ƒå‚æ•°å’ŒLoRAéƒ¨åˆ†ï¼Œå…¶ä½™éƒ¨åˆ†çš„ä»£ç ç»†èŠ‚ä¸ç”¨å…³æ³¨ã€‚\n",
    "\n",
    "å½“å‰ç‰ˆæœ¬ä¸Šä¼ ç”¨äºæå‰å­¦ä¹ ï¼Œå¼•å¯¼æ–‡ç« æ­£åœ¨æ”¥å†™ï¼Œä½ å¯ä»¥ç­‰å¾…å¼•å¯¼æ–‡ç« çš„å‘å¸ƒå†è¿›è¡Œã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8deb6-6844-400c-bb10-388d89d3a020",
   "metadata": {},
   "source": [
    "## å®‰è£…å¿…è¦çš„åº“\n",
    "\n",
    "æœ¬å•å…ƒä¸­å°†å®‰è£…ä¸€äº›åº“ç”¨äºå¾®è°ƒã€‚\n",
    "\n",
    "å®‰è£…å¤§çº¦éœ€è¦ 5 åˆ†é’Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629cf04-6988-483d-bfc9-509f8b59a9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: timm==1.0.7 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (1.0.7)\n",
      "Requirement already satisfied: torch in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from timm==1.0.7) (2.3.0)\n",
      "Requirement already satisfied: torchvision in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from timm==1.0.7) (0.18.0)\n",
      "Requirement already satisfied: pyyaml in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from timm==1.0.7) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from timm==1.0.7) (0.23.4)\n",
      "Requirement already satisfied: safetensors in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from timm==1.0.7) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface_hub->timm==1.0.7) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface_hub->timm==1.0.7) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface_hub->timm==1.0.7) (24.0)\n",
      "Requirement already satisfied: requests in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface_hub->timm==1.0.7) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface_hub->timm==1.0.7) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface_hub->timm==1.0.7) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (1.12)\n",
      "Requirement already satisfied: networkx in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch->timm==1.0.7) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm==1.0.7) (12.4.127)\n",
      "Requirement already satisfied: numpy in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torchvision->timm==1.0.7) (1.25.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torchvision->timm==1.0.7) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from jinja2->torch->timm==1.0.7) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->huggingface_hub->timm==1.0.7) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->huggingface_hub->timm==1.0.7) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->huggingface_hub->timm==1.0.7) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->huggingface_hub->timm==1.0.7) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from sympy->torch->timm==1.0.7) (1.3.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: fairscale==0.4.13 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (0.4.13)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from fairscale==0.4.13) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from fairscale==0.4.13) (1.25.2)\n",
      "Requirement already satisfied: filelock in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (1.12)\n",
      "Requirement already satisfied: networkx in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->fairscale==0.4.13) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->fairscale==0.4.13) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from sympy->torch>=1.8.0->fairscale==0.4.13) (1.3.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers==4.41.2 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from transformers==4.41.2) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->transformers==4.41.2) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hoperj/miniconda3/envs/UQ/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install timm==1.0.7\n",
    "# !pip install fairscale==0.4.13\n",
    "# !pip install transformers==4.41.2\n",
    "# !pip install requests==2.31.0\n",
    "# !pip install accelerate==0.31.0\n",
    "# !pip install diffusers==0.29.1\n",
    "# !pip install einop==0.0.1\n",
    "# !pip install safetensors==0.4.3\n",
    "# !pip install voluptuous==0.15.1\n",
    "# !pip install jax==0.4.26\n",
    "# !pip install peft==0.11.1\n",
    "# !pip install deepface==0.0.92\n",
    "# !pip install tensorflow==2.15.0\n",
    "# !pip install keras==2.15.0\n",
    "# #!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a92e17-0911-4f29-9bc1-6f81e1dc287d",
   "metadata": {},
   "source": [
    "## å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2c745-a37a-42d8-940c-2cdcbc784b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ ‡å‡†åº“æ¨¡å— ==========\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# ========== ç¬¬ä¸‰æ–¹åº“ ==========\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# ========== æ·±åº¦å­¦ä¹ ç›¸å…³åº“ ==========\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "import transformers\n",
    "from transformers import AutoProcessor, AutoModel, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Diffusers (Hugging Face)\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DiffusionPipeline,\n",
    "    StableDiffusionPipeline,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import convert_state_dict_to_diffusers\n",
    "from diffusers.training_utils import compute_snr\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "\n",
    "# ========== PEFT æ¨¡å‹ ==========\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ========== DeepFace åº“ ==========\n",
    "from deepface import DeepFace\n",
    "\n",
    "# ========== IPython å’Œ Widgets ==========\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62438d83-b6c5-43a3-afe9-30ceebc50d46",
   "metadata": {},
   "source": [
    "## å‡†å¤‡é¡¹ç›®\n",
    "\n",
    "ç›´æ¥è¿è¡Œä»£ç ï¼Œè¿™ä¸ªæ¨¡å—ä¸ç”¨ä¿®æ”¹å‚æ•°ï¼Œä¸ç”¨å…³å¿ƒè¿™é‡Œçš„ä»£ç ç»†èŠ‚ï¼Œé™¤éä½ å¯¹äº¤äº’æ„Ÿå…´è¶£ã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç æ‰§è¡Œæ—¶é—´å®Œå…¨å–å†³äºä½ çš„ç½‘é€Ÿï¼Œå› ä¸º [yahcreeper/GenAI-HW10-Model](https://huggingface.co/yahcreeper/GenAI-HW10-Model/tree/main/checkpoint-last) çš„å¤§å°ä¸º2Gã€‚\n",
    "\n",
    "æ³¨æ„ï¼šè®°å¾—ç‚¹å‡»**åˆå§‹åŒ–é¡¹ç›®**\n",
    "\n",
    "å½“ä½ çœ‹è§âœ…æ—¶ï¼Œä»£è¡¨é¡¹ç›®å·²ç»å‡†å¤‡å¥½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbde642-16f6-4728-8b7b-05d220644de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5d120ec7e4459ebf9e39f9a73dbeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Brad', description='é¡¹ç›®åç§°:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299d23bbe6f94fc59137a5c97653e09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Brad', description='æ•°æ®é›†:', disabled=True, style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40306bb1a46e4c0882f72d7ae1948973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='åˆå§‹åŒ–é¡¹ç›®', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf691afd99e44682916125686c3e0e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# åˆ›å»ºé¡¹ç›®åç§°è¾“å…¥æ¡†\n",
    "project_name_widget = widgets.Text(\n",
    "    value=\"Brad\",\n",
    "    description=\"é¡¹ç›®åç§°:\",\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†åç§°è¾“å…¥æ¡†\n",
    "dataset_name_widget = widgets.Text(\n",
    "    value=\"Brad\",  # \"Brad-512\", \"Anne-512\"\n",
    "    description=\"æ•°æ®é›†:\",\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæäº¤æŒ‰é’®\n",
    "submit_button = widgets.Button(description=\"åˆå§‹åŒ–é¡¹ç›®\")\n",
    "\n",
    "# è¾“å‡ºåŒºåŸŸ\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# å®šä¹‰é¡¹ç›®åˆå§‹åŒ–é€»è¾‘\n",
    "def initialize_project(b):\n",
    "    global project_name, dataset_name, root_dir, main_dir, project_dir, model_path, images_folder, prompts_folder, captions_folder\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        project_name = project_name_widget.value.strip()\n",
    "        dataset_name = dataset_name_widget.value.strip()\n",
    "        \n",
    "        if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
    "            print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„é¡¹ç›®åç§°ã€‚\")\n",
    "        else:\n",
    "            # åˆ›å»ºé¡¹ç›®ç›®å½•\n",
    "            project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
    "            project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
    "            root_dir = \"./\"  # å½“å‰ç›®å½•\n",
    "            main_dir = os.path.join(root_dir, \"SD\")  # ä¸»ç›®å½•\n",
    "            project_dir = os.path.join(main_dir, project_name)  # é¡¹ç›®ç›®å½•\n",
    "            \n",
    "            # ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "            os.makedirs(main_dir, exist_ok=True)\n",
    "            os.makedirs(project_dir, exist_ok=True)\n",
    "            \n",
    "            # å®šä¹‰æ•°æ®é›†å’Œæ¨¡å‹è·¯å¾„\n",
    "            zip_file = os.path.join(\"./\", \"data/14/Datasets.zip\")\n",
    "            log_file = os.path.join(project_dir, \"logs.zip\")\n",
    "            log_dir = os.path.join(project_dir, \"logs\")\n",
    "            model_path = os.path.join(project_dir, \"logs\", \"checkpoint-last\")\n",
    "            images_folder = os.path.join(main_dir, \"Datasets\", dataset_name)\n",
    "            prompts_folder = os.path.join(main_dir, \"Datasets\", \"prompts\")\n",
    "            captions_folder = images_folder\n",
    "\n",
    "            # ä¸‹è½½æ•°æ®é›†\n",
    "            print(\"ğŸ“‚ æ­£åœ¨ä¸‹è½½æ•°æ®é›†...\")\n",
    "            # subprocess.run(f\"gdown 1OXPG2vNb8bG2334HML8vKpqo8UbAEV3d -O {zip_file}\", shell=True)\n",
    "            subprocess.run(f\"unzip -q -o {zip_file} -d {main_dir}\", shell=True)\n",
    "\n",
    "            # # å…‹éš†æ¨¡å‹æ–‡ä»¶\n",
    "            print(\"ğŸ“‚ é»˜è®¤ä¸å…‹éš†LoRAæ¨¡å‹æ–‡ä»¶è€Œæ˜¯è‡ªå·±è®­ç»ƒï¼Œå¦‚æœéœ€è¦å…‹éš†è¯·å›åˆ°ä»£ç å–æ¶ˆæ³¨é‡Šï¼Œä½¿ç”¨éœ€è¦è®¾ç½®ä¹‹åçš„resume=Trueã€‚\")\n",
    "            # subprocess.run(f\"export HF_ENDPOINT=https://hf-mirror.com\", shell=True)\n",
    "            # subprocess.run(f\"huggingface-cli download yahcreeper/GenAI-HW10-Model --local-dir {log_dir}\", shell=True)\n",
    "\n",
    "            # åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹\n",
    "            os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "            print(f\"âœ… é¡¹ç›® {project_name} å·²å‡†å¤‡å¥½ï¼\")\n",
    "\n",
    "# å°†æäº¤æŒ‰é’®ä¸é¡¹ç›®åˆå§‹åŒ–é€»è¾‘ç»‘å®š\n",
    "submit_button.on_click(initialize_project)\n",
    "\n",
    "# æ˜¾ç¤ºæ§ä»¶\n",
    "display(project_name_widget, dataset_name_widget, submit_button, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44210b9-245d-4640-ae1c-d4a55420c984",
   "metadata": {},
   "source": [
    "## è®¾ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f253cc2c-42eb-476d-aa9b-a740c432caf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# è¯·å‹¿éšæ„æ›´æ”¹ä»¥ä¸‹å‚æ•°ï¼Œé™¤éä½ è‡ªå·±çŸ¥é“æ”¹çš„å‚æ•°å¯¹åº”çš„ä»€ä¹ˆï¼Œå¦åˆ™å¯èƒ½å›  GPU å†…å­˜ä¸è¶³å¯¼è‡´è¿›ç¨‹å´©æºƒã€‚\n",
    "output_folder = os.path.join(project_dir, \"logs\")  # å­˜æ”¾ model checkpoints å’Œ validation çš„æ–‡ä»¶å¤¹\n",
    "seed = 1126  # éšæœºç§å­\n",
    "train_batch_size = 2  # è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "resolution = 512  # å›¾åƒå°ºå¯¸\n",
    "weight_dtype = torch.bfloat16  # æƒé‡æ•°æ®ç±»å‹\n",
    "snr_gamma = 5  # SNR å‚æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ee76f-e838-4296-bf6d-fe175d9e08b8",
   "metadata": {},
   "source": [
    "### Stable Diffusion LoRA çš„å¾®è°ƒå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07a3378-e074-4793-ac5e-f8ba7bab2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾®è°ƒçš„å¯¹è±¡\n",
    "pretrained_model_name_or_path = \"stablediffusionapi/cyberrealistic-41\"\n",
    "\n",
    "# LoRA é…ç½®\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # LoRA çš„ç§©\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\",  # Text encoder çš„æ¨¡å—\n",
    "        \"to_k\", \"to_q\", \"to_v\", \"to_out.0\"  # UNet çš„æ¨¡å—\n",
    "    ],\n",
    "    lora_dropout=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d27ec-5425-454a-bb6c-506db30fe69c",
   "metadata": {},
   "source": [
    "### å…¶ä»–è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b00988-3201-4c16-b363-17528f5cadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ ç‡è°ƒåº¦å™¨éƒ¨åˆ†\n",
    "lr_scheduler_name = \"cosine_with_restarts\" # è®¾ç½®å­¦ä¹ ç‡çš„è°ƒåº¦å™¨\n",
    "lr_warmup_steps = 100 # è®¾ç½®å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°\n",
    "\n",
    "# prompt å¤„ç†\n",
    "validation_prompt_name = \"validation_prompt.txt\"\n",
    "validation_prompt_path = os.path.join(prompts_folder, validation_prompt_name)\n",
    "with open(validation_prompt_path, \"r\") as f:\n",
    "    validation_prompt = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e1b82-00a7-4e4e-92cb-8aaf50aa2a60",
   "metadata": {},
   "source": [
    "ç›´æ¥è¿è¡Œä»£ç ï¼Œäº¤äº’å¼ä¿®æ”¹å‚æ•°ï¼Œä¸ç”¨å…³å¿ƒè¿™é‡Œçš„ä»£ç ç»†èŠ‚ï¼Œé™¤éä½ å¯¹äº¤äº’æ„Ÿå…´è¶£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70fdd8b-3e5b-4657-bacd-206b087375b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "â–¶ï¸ **Learning Rate**<br>å­¦ä¹ ç‡æ˜¯å½±å“ç»“æœæœ€é‡è¦çš„å‚æ•°ï¼Œé»˜è®¤ä¸‰è€…ä¸€è‡´ã€‚<br>å¦‚æœä½ æƒ³æ…¢é€Ÿè®­ç»ƒä¸”æœ‰å¤§é‡å›¾åƒï¼Œæˆ–è€…å¦‚æœ dim å’Œ alpha å€¼è¾ƒé«˜ï¼Œå»ºè®®å°† unet çš„å­¦ä¹ ç‡è°ƒè‡³ 2e-4 æˆ–æ›´ä½ã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d459c7a855547fab0fc50722560d783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='learning_rate:', layout=Layout(width='300px'), style=DescriptionStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0300b97b65485b8ee543f076ff64c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='unet_learning_rate:', layout=Layout(width='300px'), style=DescriptionStylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9276dbe8889b4e4f8d54af0444dc49d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='text_encoder_learning_rate:', layout=Layout(width='300px'), style=Descripâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "â–¶ï¸ **Steps**<br>é€‰æ‹©è®­ç»ƒæ­¥éª¤å’Œæ¯æ¬¡éªŒè¯ç”Ÿæˆçš„å›¾åƒæ•°é‡"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850243b89d7c4b9395360ce12ee559d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=200, continuous_update=False, description='max_train_steps:', layout=Layout(width='400px'), maâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d062f34ab2934211b926771be27706b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=3, description='validation_prompt_num:', layout=Layout(width='400px'), max=5, min=1, style=Sliâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0658e1b42b9640bda8c10ed2ef51627a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=1.0, description='validation_step_ratio:', layout=Layout(width='400px'), max=1.0, style=Slidâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3364770e6b49c28a34fa7fb1f22476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='./SD/Datasets/prompts/validation_prompt.txt', description='validation_prompt_path:', disabled=Trueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4b197e26ba4dbcbe180867f5cb913f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='æäº¤é…ç½®', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38102ffa970405db71cbaa697fcfe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å­¦ä¹ ç‡éƒ¨åˆ†\n",
    "learning_rate_markdown = Markdown(\"â–¶ï¸ **Learning Rate**<br>å­¦ä¹ ç‡æ˜¯å½±å“ç»“æœæœ€é‡è¦çš„å‚æ•°ï¼Œé»˜è®¤ä¸‰è€…ä¸€è‡´ã€‚<br>å¦‚æœä½ æƒ³æ…¢é€Ÿè®­ç»ƒä¸”æœ‰å¤§é‡å›¾åƒï¼Œæˆ–è€…å¦‚æœ dim å’Œ alpha å€¼è¾ƒé«˜ï¼Œå»ºè®®å°† unet çš„å­¦ä¹ ç‡è°ƒè‡³ 2e-4 æˆ–æ›´ä½ã€‚\")\n",
    "learning_rate_widget = widgets.FloatText(\n",
    "    value=1e-4,\n",
    "    description='learning_rate:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "unet_learning_rate_widget = widgets.FloatText(\n",
    "    value=1e-4,\n",
    "    description='unet_learning_rate:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "text_encoder_learning_rate_widget = widgets.FloatText(\n",
    "    value=1e-4,\n",
    "    description='text_encoder_learning_rate:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# è®­ç»ƒæ­¥éª¤éƒ¨åˆ†\n",
    "max_train_steps_markdown = Markdown(\"â–¶ï¸ **Steps**<br>é€‰æ‹©è®­ç»ƒæ­¥éª¤å’Œæ¯æ¬¡éªŒè¯ç”Ÿæˆçš„å›¾åƒæ•°é‡\")\n",
    "max_train_steps_widget = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=200,\n",
    "    max=2000,\n",
    "    step=100,\n",
    "    description='max_train_steps:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "\n",
    "validation_prompt_num_widget = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='validation_prompt_num:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "\n",
    "validation_step_ratio_widget = widgets.FloatSlider(\n",
    "    value=1,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=0.1,\n",
    "    description='validation_step_ratio:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "\n",
    "validation_prompt_widget = widgets.Text(\n",
    "    value = validation_prompt_path,\n",
    "    description='validation_prompt_path:',\n",
    "    style={'description_width': '200px'},\n",
    "    layout=widgets.Layout(width='550px'),\n",
    "    disabled=True  # ç¦ç”¨æ‰‹åŠ¨ä¿®æ”¹\n",
    ")\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºåŒºåŸŸæ¥æ˜¾ç¤ºæ‰“å°å†…å®¹\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# è·å–ç”¨æˆ·è¾“å…¥å€¼å¹¶æ‰“å°é…ç½®\n",
    "def on_button_click(b):\n",
    "    global learning_rate, unet_learning_rate, text_encoder_learning_rate, max_train_steps, validation_prompt, validation_prompt_num, validation_step_ratio\n",
    "    with output_area:  # ä½¿ç”¨ Output å°éƒ¨ä»¶æ•è·è¾“å‡º\n",
    "        output_area.clear_output()  # æ¸…é™¤ä¹‹å‰çš„è¾“å‡º\n",
    "        learning_rate = learning_rate_widget.value\n",
    "        unet_learning_rate = unet_learning_rate_widget.value\n",
    "        text_encoder_learning_rate = text_encoder_learning_rate_widget.value\n",
    "        max_train_steps = max_train_steps_widget.value\n",
    "        validation_prompt_num = validation_prompt_num_widget.value\n",
    "        validation_step_ratio = validation_step_ratio_widget.value\n",
    "\n",
    "        # æ‰“å°é…ç½®\n",
    "        print(f\"å­¦ä¹ ç‡: {learning_rate}\")\n",
    "        print(f\"UNet å­¦ä¹ ç‡: {unet_learning_rate}\")\n",
    "        print(f\"æ–‡æœ¬ç¼–ç å™¨å­¦ä¹ ç‡: {text_encoder_learning_rate}\")\n",
    "        print(f\"è®­ç»ƒæ­¥éª¤: {max_train_steps}\")\n",
    "        print(f\"éªŒè¯promptæ•°é‡: {validation_prompt_num}\")\n",
    "        print(f\"éªŒè¯æ­¥éª¤æ¯”ä¾‹: {validation_step_ratio}\")\n",
    "        print(f\"ç”¨äºéªŒè¯çš„promptæ–‡ä»¶ä½ç½®: {validation_prompt_path}\")\n",
    "\n",
    "# åˆ›å»ºæäº¤æŒ‰é’®\n",
    "submit_button = widgets.Button(description=\"æäº¤é…ç½®\")\n",
    "submit_button.on_click(on_button_click)\n",
    "\n",
    "# æ˜¾ç¤ºå¸¦æœ‰è¯´æ˜çš„æ‰€æœ‰å°éƒ¨ä»¶\n",
    "display(learning_rate_markdown, learning_rate_widget, unet_learning_rate_widget, text_encoder_learning_rate_widget)\n",
    "display(max_train_steps_markdown, max_train_steps_widget, validation_prompt_num_widget, validation_step_ratio_widget, validation_prompt_widget)\n",
    "\n",
    "# æ˜¾ç¤ºæäº¤æŒ‰é’®\n",
    "display(submit_button)\n",
    "\n",
    "# æ˜¾ç¤ºè¾“å‡ºåŒºåŸŸ\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98bedfc-4764-4a33-b883-cc1e8f3a89dc",
   "metadata": {},
   "source": [
    "## å®šä¹‰ä¸€äº›æœ‰ç”¨çš„å‡½æ•°å’Œç±»\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953edb1b-a08f-4cd0-876d-a0b352ba7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\", \".BMP\"]\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°\n",
    "        transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ\n",
    "        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬\n",
    "        transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡\n",
    "        transforms.Normalize([0.5], [0.5]),  # å½’ä¸€åŒ–å›¾åƒæ•°æ®ï¼ŒèŒƒå›´è°ƒæ•´åˆ° [-1, 1]\n",
    "    ]\n",
    ")\n",
    "class Text2ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºæ„å»ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¾®è°ƒæ•°æ®é›†\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
    "        \"\"\"\n",
    "        (2) å‚æ•°:\n",
    "            - images_folder: str, å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„\n",
    "            - captions_folder: str, æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„\n",
    "            - transform: function, å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º torch.tensor\n",
    "            - tokenizer: CLIPTokenizer, å°†æ–‡æœ¬æ ‡æ³¨è½¬ä¸º word ids\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–å›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ‰©å±•åæ‰¾åˆ°æ‰€æœ‰å›¾åƒæ–‡ä»¶\n",
    "        self.image_paths = []\n",
    "        for ext in IMAGE_EXTENSIONS:\n",
    "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
    "        self.image_paths = sorted(self.image_paths)\n",
    "\n",
    "        # é€šè¿‡ DeepFace åº“æå–æ¯å¼ å›¾åƒçš„é¢éƒ¨åµŒå…¥ï¼ˆç‰¹å¾å‘é‡ï¼‰\n",
    "        self.train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in self.image_paths])\n",
    "\n",
    "        # åŠ è½½å¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨ï¼Œä¾æ¬¡è¯»å–æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­çš„å†…å®¹\n",
    "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
    "        captions = []\n",
    "        for p in caption_paths:\n",
    "            with open(p, \"r\") as f:\n",
    "                captions.append(f.readline())\n",
    "\n",
    "        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º word ids\n",
    "        inputs = tokenizer(\n",
    "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        self.input_ids = inputs.input_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        input_id = self.input_ids[idx]\n",
    "        try:\n",
    "            # åŠ è½½å›¾åƒå¹¶å°†å…¶è½¬æ¢ä¸º RGB æ¨¡å¼ï¼Œç„¶ååº”ç”¨æ•°æ®å¢å¼º\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            tensor = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"æ— æ³•åŠ è½½å›¾åƒè·¯å¾„: {img_path}, é”™è¯¯: {e}\")\n",
    "            return None\n",
    "\n",
    "        return tensor, input_id  # è¿”å›å¤„ç†åçš„å›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æ ‡æ³¨\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "def prepare_lora_model(lora_config, pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\", model_path=None, resume=False):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºåŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ Lora å±‚ï¼Œå¹¶å†»ç»“é Lora å‚æ•°ã€‚è¿™åŒ…æ‹¬ Tokenizerã€å™ªå£°è°ƒåº¦å™¨ã€UNetã€VAE å’Œ æ–‡æœ¬ç¼–ç å™¨ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - pretrained_model_name_or_path: str, ä» Hugging Face è·å–çš„æ¨¡å‹åç§°\n",
    "        - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„\n",
    "        - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - è¾“å‡º: Tokenizer, å™ªå£°è°ƒåº¦å™¨, UNet, VAE, æ–‡æœ¬ç¼–ç å™¨\n",
    "\n",
    "    \"\"\"\n",
    "    # åŠ è½½å™ªå£°è°ƒåº¦å™¨ï¼Œç”¨äºæ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å™ªå£°æ·»åŠ å’Œç§»é™¤è¿‡ç¨‹\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "    # åŠ è½½ Tokenizerï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º tokens\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸ºç‰¹å¾å‘é‡\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        subfolder=\"text_encoder\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ VAE æ¨¡å‹ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¤„ç†å›¾åƒçš„æ½œåœ¨è¡¨ç¤º\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"vae\"\n",
    "    )\n",
    "\n",
    "    # åŠ è½½ UNet æ¨¡å‹ï¼Œè´Ÿè´£å¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒç”Ÿæˆå’Œæ¨ç†è¿‡ç¨‹\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        subfolder=\"unet\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet\n",
    "    text_encoder = get_peft_model(text_encoder, lora_config)\n",
    "    unet = get_peft_model(unet, lora_config)\n",
    "    \n",
    "    # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡\n",
    "    text_encoder.print_trainable_parameters()\n",
    "    unet.print_trainable_parameters()\n",
    "    \n",
    "    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡ï¼Œå½“ç„¶ï¼Œä½ å¯ä»¥ä¿®æ”¹ model_path æ¥æŒ‡å®šå…¶ä»–çš„è·¯å¾„\n",
    "    if resume:\n",
    "        if model_path is None or not os.path.exists(model_path):\n",
    "            raise ValueError(\"å½“resumeè®¾ç½®ä¸ºTrueçš„æ—¶å€™ä½ å¿…é¡»æä¾›æœ‰æ•ˆçš„model_path\")\n",
    "            \n",
    "        # åŠ è½½ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹æƒé‡ï¼Œæ³¨æ„è¿™é‡ŒåªåŠ è½½æƒé‡ï¼Œè€Œä¸æ˜¯è¦†ç›–æ•´ä¸ªæ¨¡å‹ï¼Œè¦†ç›–ï¼šmodel = torch.load(...)\n",
    "        text_encoder.load_state_dict(torch.load(os.path.join(model_path, \"text_encoder.pt\")))\n",
    "        unet.load_state_dict(torch.load(os.path.join(model_path, \"unet.pt\")))\n",
    "    \n",
    "    # å†»ç»“ VAE å‚æ•°\n",
    "    vae.requires_grad_(False)\n",
    "\n",
    "    # ä½¿ç”¨ get_peft_model() åä¼šè‡ªåŠ¨å†»ç»“å…¶ä¸­çš„é LoRA å±‚ï¼Œæ‰€ä»¥å¯ä»¥æ³¨é‡Šæ‰\n",
    "    # # å†»ç»“ UNet å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­çš„é Lora å‚æ•°\n",
    "    # for name, param in unet.named_parameters():\n",
    "    #     if \"lora\" in name:\n",
    "    #         param.requires_grad_(True)\n",
    "    #     else:\n",
    "    #         param.requires_grad_(False)\n",
    "    # for name, param in text_encoder.named_parameters():\n",
    "    #     if \"lora\" in name:\n",
    "    #         param.requires_grad_(True)\n",
    "    #     else:\n",
    "    #         param.requires_grad_(False)\n",
    "\n",
    "    # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹\n",
    "    unet.to(DEVICE, dtype=weight_dtype)\n",
    "    vae.to(DEVICE, dtype=weight_dtype)\n",
    "    text_encoder.to(DEVICE, dtype=weight_dtype)\n",
    "    \n",
    "    return tokenizer, noise_scheduler, unet, vae, text_encoder\n",
    "\n",
    "\n",
    "def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºå°† UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°åˆ†åˆ«ä¼ å…¥ä¼˜åŒ–å™¨ï¼Œå¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - unet: UNet2DConditionModel, Hugging Face çš„ UNet æ¨¡å‹\n",
    "        - text_encoder: CLIPTextModel, Hugging Face çš„æ–‡æœ¬ç¼–ç å™¨\n",
    "        - unet_learning_rate: float, UNet çš„å­¦ä¹ ç‡\n",
    "        - text_encoder_learning_rate: float, æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - è¾“å‡º: ä¼˜åŒ–å™¨ Optimizer\n",
    "\n",
    "    \"\"\"\n",
    "    # ç­›é€‰å‡º UNet ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°\n",
    "    unet_lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))\n",
    "    \n",
    "    # ç­›é€‰å‡ºæ–‡æœ¬ç¼–ç å™¨ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°\n",
    "    text_encoder_lora_layers = list(filter(lambda p: p.requires_grad, text_encoder.parameters()))\n",
    "    \n",
    "    # å°†éœ€è¦è®­ç»ƒçš„å‚æ•°åˆ†ç»„å¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡\n",
    "    trainable_params = [\n",
    "        {\"params\": unet_lora_layers, \"lr\": unet_learning_rate},\n",
    "        {\"params\": text_encoder_lora_layers, \"lr\": text_encoder_learning_rate}\n",
    "    ]\n",
    "    \n",
    "    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        trainable_params,\n",
    "        lr=unet_learning_rate,\n",
    "    )\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
    "    \"\"\"\n",
    "    (1) ç›®æ ‡:\n",
    "        - ç”¨äºåŠ è½½ç»™å®šè·¯å¾„ä¸­çš„ UNet å’Œæ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ï¼Œç”Ÿæˆå›¾åƒå¹¶è®¡ç®—é¢éƒ¨ç›¸ä¼¼æ€§ã€CLIP åˆ†æ•°å’Œæ— é¢éƒ¨å›¾åƒçš„æ•°é‡ã€‚\n",
    "\n",
    "    (2) å‚æ•°:\n",
    "        - pretrained_model_name_or_path: str, Hugging Face çš„æ¨¡å‹åç§°\n",
    "        - weight_dtype: torch.dtype, æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹\n",
    "        - seed: int, éšæœºç§å­\n",
    "        - unet_path: str, UNet æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "        - text_encoder_path: str, æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "        - validation_prompt: list, å­˜å‚¨éªŒè¯æ–‡æœ¬çš„å­—ç¬¦ä¸²åˆ—è¡¨\n",
    "        - output_folder: str, ç”¨äºä¿å­˜ç”Ÿæˆå›¾åƒçš„ç›®å½•\n",
    "        - train_emb: tensor, è®­ç»ƒå›¾åƒçš„é¢éƒ¨ç‰¹å¾\n",
    "\n",
    "    (3) è¿”å›:\n",
    "        - è¾“å‡º: é¢éƒ¨ç›¸ä¼¼æ€§ã€CLIP åˆ†æ•°ã€æ— é¢éƒ¨å›¾åƒæ•°é‡\n",
    "\n",
    "    \"\"\"\n",
    "    # åŠ è½½ DiffusionPipeline å¹¶è®¾ç½® UNet å’Œæ–‡æœ¬ç¼–ç å™¨\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        torch_dtype=weight_dtype,\n",
    "        safety_checker=None,\n",
    "    )\n",
    "    pipeline.unet = torch.load(unet_path)  # åŠ è½½ UNet æ¨¡å‹\n",
    "    pipeline.text_encoder = torch.load(text_encoder_path)  # åŠ è½½æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹\n",
    "    pipeline = pipeline.to(DEVICE)\n",
    "\n",
    "    # åŠ è½½ CLIP æ¨¡å‹ç”¨äºè®¡ç®—å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼æ€§åˆ†æ•°\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    clip_model = AutoModel.from_pretrained(clip_model_name)\n",
    "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
    "\n",
    "    # æ¨ç†ç”Ÿæˆå›¾åƒ\n",
    "    with torch.no_grad():\n",
    "        generator = torch.Generator(device=DEVICE)\n",
    "        generator = generator.manual_seed(seed)\n",
    "        face_score = 0\n",
    "        clip_score = 0\n",
    "        mis = 0  # è®°å½•æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾åƒæ•°é‡\n",
    "        print(\"æ­£åœ¨ç”ŸæˆéªŒè¯å›¾åƒ......\")\n",
    "        images = []\n",
    "        for i in range(0, len(validation_prompt), 4):\n",
    "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
    "        \n",
    "        # è®¡ç®—é¢éƒ¨ç›¸ä¼¼æ€§å’Œ CLIP åˆ†æ•°\n",
    "        print(\"æ­£åœ¨è®¡ç®—éªŒè¯åˆ†æ•°......\")\n",
    "        valid_emb = []\n",
    "        for i, image in enumerate(tqdm(images)):\n",
    "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
    "            image.save(save_file)  # å°†ç”Ÿæˆçš„å›¾åƒä¿å­˜åˆ°æŒ‡å®šç›®å½•\n",
    "            opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # ä½¿ç”¨ DeepFace æ£€æµ‹é¢éƒ¨ç‰¹å¾\n",
    "            emb = DeepFace.represent(\n",
    "                opencvImage,\n",
    "                detector_backend=\"ssd\",\n",
    "                model_name=\"GhostFaceNet\",\n",
    "                enforce_detection=False,\n",
    "            )\n",
    "            # å¦‚æœæ— æ³•æ£€æµ‹åˆ°é¢éƒ¨ï¼Œè®¡å…¥ mis\n",
    "            if emb == [] or emb[0]['face_confidence'] == 0:\n",
    "                mis += 1\n",
    "                continue\n",
    "\n",
    "            # è®¡ç®— CLIP åˆ†æ•°ï¼Œè¡¡é‡å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼åº¦\n",
    "            emb = emb[0]\n",
    "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = clip_model(**inputs)\n",
    "            sim = outputs.logits_per_image  # è·å–å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼æ€§å¾—åˆ†\n",
    "            clip_score += sim.item()\n",
    "            valid_emb.append(emb['embedding'])\n",
    "\n",
    "        # å¦‚æœç”Ÿæˆçš„å›¾ç‰‡éƒ½æ²¡æœ‰äººè„¸ï¼Œç›´æ¥ 0 åˆ†\n",
    "        if len(valid_emb) == 0:\n",
    "            return 0, 0, mis\n",
    "        \n",
    "        # è®¡ç®—é¢éƒ¨ç›¸ä¼¼æ€§åˆ†æ•°\n",
    "        valid_emb = torch.tensor(valid_emb)\n",
    "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).cuda()\n",
    "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).cuda()\n",
    "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
    "        clip_score /= len(validation_prompt) - mis\n",
    "    \n",
    "    return face_score, clip_score, mis   # è¿”å›é¢éƒ¨ç›¸ä¼¼æ€§ã€CLIP åˆ†æ•°å’Œæ— é¢éƒ¨å›¾åƒçš„æ•°é‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b834a7-3f78-4d6f-9432-17444e7c29bd",
   "metadata": {},
   "source": [
    "ç»§ç»­ä¸ºå¾®è°ƒ Stable Diffusion æ¨¡å‹å‡†å¤‡æ•°æ®é›†ã€LoRA æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼š\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73b8290-6b15-4171-bc1b-403de3abe81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch stablediffusionapi/cyberrealistic-41: stablediffusionapi/cyberrealistic-41 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch stablediffusionapi/cyberrealistic-41: stablediffusionapi/cyberrealistic-41 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 125,419,776 || trainable%: 1.8811\n",
      "trainable params: 6,377,472 || all params: 865,898,436 || trainable%: 0.7365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 15:27:02.601406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.601926: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.601989: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.602557: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.602627: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.602671: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.602759: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.602806: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.602846: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-09-27 15:27:02.602856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 15:27:02.602897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7821 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-09-27 15:27:04.537225: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å†³å®šæ˜¯å¦ç»§ç»­ä¹‹å‰çš„è®­ç»ƒ\n",
    "resume = False\n",
    "\n",
    "# å‡†å¤‡å¾®è°ƒæ‰€éœ€çš„ tokenizer, å™ªå£°è°ƒåº¦å™¨, UNet, VAE å’Œæ–‡æœ¬ç¼–ç å™¨\n",
    "tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(lora_config, pretrained_model_name_or_path, model_path, resume)\n",
    "\n",
    "# å‡†å¤‡ä¼˜åŒ–å™¨ï¼Œç”¨äºæ›´æ–° UNet å’Œ æ–‡æœ¬ç¼–ç å™¨ä¸­ç»è¿‡ LoRA å±‚è°ƒæ•´çš„å¯è®­ç»ƒå‚æ•°\n",
    "optimizer = prepare_optimizer(unet, text_encoder, unet_learning_rate, text_encoder_learning_rate)\n",
    "\n",
    "# è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ§åˆ¶å­¦ä¹ ç‡çš„å˜åŒ–\n",
    "# get_scheduler æ ¹æ®æŒ‡å®šçš„è°ƒåº¦å™¨ç±»å‹ï¼ˆä¾‹å¦‚çº¿æ€§ã€ä½™å¼¦ç­‰ï¼‰ï¼Œä¸ºä¼˜åŒ–å™¨æä¾›ä¸€ä¸ªåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡çš„ç­–ç•¥\n",
    "# num_warmup_steps: åœ¨åˆå§‹é˜¶æ®µè¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°\n",
    "# num_training_steps: æ€»çš„è®­ç»ƒæ­¥æ•°\n",
    "# num_cycles: ç”¨äºä½™å¼¦è°ƒåº¦å™¨ï¼ŒæŒ‡å®šä½™å¼¦è¡°å‡å‘¨æœŸçš„æ•°é‡\n",
    "lr_scheduler = get_scheduler(\n",
    "    lr_scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    "    num_cycles=3\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ•°æ®é›†\n",
    "# Text2ImageDataset ä¼šæ ¹æ®æŒ‡å®šçš„å›¾åƒæ–‡ä»¶å¤¹å’Œæ–‡æœ¬æ ‡æ³¨æ–‡ä»¶å¤¹ï¼ŒåŠ è½½å¹¶å¤„ç†æ•°æ®\n",
    "# å…·ä½“ï¼šä½¿ç”¨æŒ‡å®šçš„ transform æ¥é¢„å¤„ç†å›¾åƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨ tokenizer æ¥å°†æ–‡æœ¬è½¬æ¢ä¸º tokens\n",
    "dataset = Text2ImageDataset(\n",
    "    images_folder=images_folder,\n",
    "    captions_folder=captions_folder,\n",
    "    transform=train_transform,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# è®¾ç½®æ•°æ®åŠ è½½å™¨ï¼Œè´Ÿè´£å°†æ•°æ®é›†åˆ†æ‰¹æ¬¡åŠ è½½åˆ°æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒ\n",
    "# collate_fn: è‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œå°†æ•°æ®é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨åˆå¹¶æˆæ‰¹æ¬¡\n",
    "def collate_fn(examples):\n",
    "    pixel_values = []\n",
    "    input_ids = []\n",
    "    \n",
    "    # å°†æ¯ä¸ªæ ·æœ¬ä¸­çš„å›¾åƒ tensor å’Œæ–‡æœ¬æ ‡æ³¨ input_ids åˆ†åˆ«æ”¶é›†åˆ°åˆ—è¡¨ä¸­\n",
    "    for tensor, input_id in examples:\n",
    "        pixel_values.append(tensor)  # å°†å›¾åƒå¼ é‡æ”¶é›†åˆ° pixel_values åˆ—è¡¨ä¸­\n",
    "        input_ids.append(input_id)  # å°†æ–‡æœ¬æ ‡æ³¨æ”¶é›†åˆ° input_ids åˆ—è¡¨ä¸­\n",
    "    \n",
    "    # å°†åˆ—è¡¨ä¸­çš„æ‰€æœ‰å›¾åƒå¼ é‡å †å æˆä¸€ä¸ªå¤§çš„ batch tensor\n",
    "    pixel_values = torch.stack(pixel_values, dim=0).float()  # å½¢çŠ¶ä¸º (batch_size, channels, height, width)\n",
    "    input_ids = torch.stack(input_ids, dim=0)  # å½¢çŠ¶ä¸º (batch_size, max_seq_length)\n",
    "    \n",
    "    # è¿”å›æ‰¹å¤„ç†åçš„å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "# ä½¿ç”¨ PyTorch DataLoader åŠ è½½æ•°æ®é›†ï¼Œè®¾ç½®æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°å’Œçº¿ç¨‹æ•°\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    shuffle=True,  # æ‰“ä¹±æ•°æ®\n",
    "    collate_fn=collate_fn,  # è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°\n",
    "    batch_size=train_batch_size,  # æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡\n",
    "    num_workers=8,  # ä½¿ç”¨ 8 ä¸ªçº¿ç¨‹å¹¶è¡ŒåŠ è½½æ•°æ®ï¼ŒåŠ å¿«æ•°æ®åŠ è½½é€Ÿåº¦\n",
    ")\n",
    "\n",
    "print(\"å‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27875-b3b2-4b03-a554-4675667e4415",
   "metadata": {},
   "source": [
    "æœ€åæ˜¯å¼€å§‹è¿›è¡Œå¾®è°ƒçš„éƒ¨åˆ†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8836a890-8e06-4aed-bd11-71df8fd5bf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8889154e9c9f456693352db695ef106c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "æ­¥éª¤:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹åˆ° ./SD/Brad/logs/checkpoint-last ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "text_encoder/model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fe0658532e42f683c6c7b45d2552c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ç”ŸæˆéªŒè¯å›¾åƒ......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bb10133fcb4b4da03953024a0f1d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è®¡ç®—éªŒè¯åˆ†æ•°......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f559bbcb0a43c397733ee2c33c83a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­¥éª¤: 200 é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ†: 1.3246982097625732 CLIPè¯„åˆ†: 31.03872776031494 æ— é¢éƒ¨å›¾åƒæ•°é‡: 1\n",
      "å¾®è°ƒå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å¼€å§‹å¾®è°ƒ Stable Diffusion æ¨¡å‹\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
    "progress_bar = tqdm(\n",
    "    range(0, max_train_steps),\n",
    "    initial=0,\n",
    "    desc=\"æ­¥éª¤\",\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–å…¨å±€æ­¥æ•°å’Œå…¶ä»–æ§åˆ¶å˜é‡\n",
    "global_step = 0\n",
    "num_epochs = math.ceil(max_train_steps / len(train_dataloader))  # æ ¹æ®æœ€å¤§è®­ç»ƒæ­¥æ•°è®¡ç®—æ€»å…±çš„ epoch æ•°é‡\n",
    "validation_step = int(max_train_steps * validation_step_ratio)  # æ ¹æ®è®¾ç½®çš„éªŒè¯æ­¥æ•°æ¯”ä¾‹è®¡ç®—æ¯éš”å¤šå°‘æ­¥è¿›è¡ŒéªŒè¯\n",
    "best_face_score = float(\"inf\")  # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§ï¼Œå­˜å‚¨æœ€ä½³é¢éƒ¨ç›¸ä¼¼åº¦åˆ†æ•°\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯ï¼Œéå†æ‰€æœ‰çš„ epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # å°†æ¨¡å‹åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
    "    unet.train()\n",
    "    text_encoder.train()\n",
    "    \n",
    "    # éå†æ•°æ®åŠ è½½å™¨ï¼Œè¿›è¡Œé€æ­¥è®­ç»ƒ\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if global_step >= max_train_steps:\n",
    "            break  # å½“è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•°æ—¶ï¼Œç»ˆæ­¢è®­ç»ƒ\n",
    "        \n",
    "        # ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰ï¼Œé€šè¿‡ VAE å¯¹å›¾åƒè¿›è¡Œç¼–ç \n",
    "        latents = vae.encode(batch[\"pixel_values\"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor  # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨ç©ºé—´\n",
    "\n",
    "        # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ\n",
    "        noise = torch.randn_like(latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°\n",
    "        bsz = latents.shape[0]  # æ‰¹æ¬¡å¤§å°\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()  # éšæœºé€‰æ‹©æ—¶é—´æ­¥\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)  # æ·»åŠ å™ªå£°åˆ°æ½œåœ¨è¡¨ç¤º\n",
    "\n",
    "        # è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º\n",
    "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(latents.device), return_dict=False)[0]\n",
    "        \n",
    "        # è®¡ç®—ç›®æ ‡å€¼ï¼ŒåŸºäºé¢„æµ‹ç±»å‹ï¼ˆepsilon æˆ– v_predictionï¼‰\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise  # é¢„æµ‹å™ªå£°\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡\n",
    "\n",
    "        # UNet æ¨¡å‹é¢„æµ‹ï¼Œè¾“å…¥å™ªå£°æ½œåœ¨ç©ºé—´ã€æ—¶é—´æ­¥å’Œæ–‡æœ¬åµŒå…¥\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±ï¼ŒåŸºäºæ˜¯å¦è®¾ç½®äº† snr_gammaï¼Œè¿™ä¸ªå‚æ•°åœ¨æœ€å¼€å§‹çš„éƒ¨åˆ†è¿›è¡Œäº†è®¾ç½®\n",
    "        if not snr_gamma:\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")  # ä½¿ç”¨å‡æ–¹è¯¯å·® (MSE) ä½œä¸ºæŸå¤±\n",
    "        else:\n",
    "            # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±\n",
    "            snr = compute_snr(noise_scheduler, timesteps)\n",
    "            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                mse_loss_weights = mse_loss_weights / snr\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                mse_loss_weights = mse_loss_weights / (snr + 1)\n",
    "            \n",
    "            # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()  # è®¡ç®—æ¢¯åº¦\n",
    "        optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "        lr_scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡\n",
    "        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\n",
    "        progress_bar.update(1)  # æ›´æ–°è¿›åº¦æ¡\n",
    "        global_step += 1  # æ›´æ–°å…¨å±€æ­¥æ•°\n",
    "\n",
    "\n",
    "        # éªŒè¯æ¨¡å‹æ€§èƒ½\n",
    "        if global_step % validation_step == 0 or global_step == max_train_steps:\n",
    "            # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹ï¼ˆåŒ…å« UNet å’Œ æ–‡æœ¬ç¼–ç å™¨çš„æ¨¡å‹å‚æ•°ï¼‰\n",
    "            save_path = os.path.join(output_folder, f\"checkpoint-last\")\n",
    "            unet_path = os.path.join(save_path, \"unet.pt\")\n",
    "            text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
    "            print(f\"æ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {save_path} ......\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            torch.save(unet, unet_path)\n",
    "            torch.save(text_encoder, text_encoder_path)\n",
    "            \n",
    "            # è¿›è¡ŒéªŒè¯\n",
    "            save_path = os.path.join(output_folder, f\"checkpoint-{global_step}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            face_score, clip_score, mis = evaluate(\n",
    "                pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "                weight_dtype=weight_dtype,\n",
    "                seed=seed,\n",
    "                unet_path=unet_path,\n",
    "                text_encoder_path=text_encoder_path,\n",
    "                validation_prompt=validation_prompt[:validation_prompt_num],  # éªŒè¯æç¤ºæ–‡æœ¬\n",
    "                output_folder=save_path,  # ä¿å­˜éªŒè¯ç»“æœçš„æ–‡ä»¶å¤¹\n",
    "                train_emb=dataset.train_emb  # è®­ç»ƒæ•°æ®ä¸­çš„é¢éƒ¨ç‰¹å¾åµŒå…¥\n",
    "            )\n",
    "            print(\"æ­¥éª¤:\", global_step, \"é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ†:\", face_score, \"CLIPè¯„åˆ†:\", clip_score, \"æ— é¢éƒ¨å›¾åƒæ•°é‡:\", mis)\n",
    "            \n",
    "            # å¦‚æœå½“å‰é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ†ä¼˜äºä¹‹å‰çš„æœ€ä½³è®°å½•ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if face_score < best_face_score:\n",
    "                best_face_score = face_score\n",
    "                save_path = os.path.join(output_folder, f\"checkpoint-best\")\n",
    "                unet_path = os.path.join(save_path, \"unet.pt\")\n",
    "                text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                torch.save(unet, unet_path)  # ä¿å­˜æœ€ä½³çš„ UNet æ¨¡å‹\n",
    "                torch.save(text_encoder, text_encoder_path)  # ä¿å­˜æœ€ä½³çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹\n",
    "\n",
    "print(\"å¾®è°ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf90ba1-3511-425c-b278-a3c6da887ed5",
   "metadata": {},
   "source": [
    "## æµ‹è¯•\n",
    "\n",
    "å¾®è°ƒè¿‡ç¨‹å·²ç»å®Œæˆã€‚æ¥ä¸‹æ¥å°†æµ‹è¯•æ¨¡å‹ã€‚\n",
    "\n",
    "é¦–å…ˆåŠ è½½ä¹‹å‰ä¿å­˜çš„å¾®è°ƒæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œå¹¶è®¡ç®—é¢éƒ¨ç›¸ä¼¼åº¦ã€CLIP è¯„åˆ†ä»¥åŠæ²¡æœ‰é¢éƒ¨çš„å›¾åƒæ•°é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb883783-f35b-46b7-9279-2cf96fdd5d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "text_encoder/model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a362aa3cb8a34bffa1a0c277f51dce20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/hoperj/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/31259688a2398b11f5e7156bac475c459afaccd8/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ç”ŸæˆéªŒè¯å›¾åƒ......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0156b2ad90f4986813fcc74b3a1a725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013ea44eea524611882c21f202099f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693d1b8e5ad54278a8207c21b6c52ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26325dee7a24a50bec4dfa270dd4b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e188d41e420d40ac843b3c06440a9497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc8e5e9792c49029db380da13a72563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5dc3af72394ae2b50b3f5335b21c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è®¡ç®—éªŒè¯åˆ†æ•°......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553cef0454724ceda683313b81768595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ†: 1.3408372402191162\n",
      "CLIP è¯„åˆ†: 31.772010258265905\n",
      "æ— é¢éƒ¨å›¾åƒæ•°é‡: 4\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®ç”¨äºæ¨ç†çš„ checkpoint è·¯å¾„\n",
    "checkpoint_path = os.path.join(output_folder, f\"checkpoint-last\")  # è®¾ç½®æ¨ç†æ—¶ä½¿ç”¨æœ€åä¿å­˜çš„ checkpoint\n",
    "\n",
    "# è®¾ç½®è·¯å¾„\n",
    "unet_path = os.path.join(checkpoint_path, \"unet.pt\")\n",
    "text_encoder_path = os.path.join(checkpoint_path, \"text_encoder.pt\")\n",
    "inference_path = os.path.join(project_dir, \"inference\")\n",
    "os.makedirs(inference_path, exist_ok=True)\n",
    "\n",
    "# è·å–è®­ç»ƒå›¾åƒçš„è·¯å¾„\n",
    "train_image_paths = []  # åˆå§‹åŒ–å­˜å‚¨æ‰€æœ‰è®­ç»ƒå›¾åƒè·¯å¾„çš„åˆ—è¡¨\n",
    "for ext in IMAGE_EXTENSIONS:\n",
    "    # é€šè¿‡æ‰©å±•åéå†å›¾åƒæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒ\n",
    "    train_image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
    "train_image_paths = sorted(train_image_paths)  # å¯¹å›¾åƒè·¯å¾„è¿›è¡Œæ’åº\n",
    "\n",
    "# åˆå§‹åŒ–åˆ—è¡¨å­˜å‚¨é¢éƒ¨ç‰¹å¾åµŒå…¥\n",
    "train_emb_list = []\n",
    "\n",
    "# éå†è®­ç»ƒå›¾åƒè·¯å¾„å¹¶æå–é¢éƒ¨ç‰¹å¾åµŒå…¥\n",
    "for img_path in train_image_paths:\n",
    "    # ä½¿ç”¨ DeepFace ä»æ¯å¼ å›¾åƒä¸­æå–é¢éƒ¨ç‰¹å¾\n",
    "    face_representation = DeepFace.represent(\n",
    "        img_path, \n",
    "        detector_backend=\"ssd\",  # ä½¿ç”¨ ssd æ£€æµ‹å™¨\n",
    "        model_name=\"GhostFaceNet\",  # ä½¿ç”¨ GhostFaceNet æ¨¡å‹\n",
    "        enforce_detection=False  # å…³é—­å¼ºåˆ¶æ£€æµ‹\n",
    "    )\n",
    "    \n",
    "    # å¦‚æœæå–åˆ°çš„ç‰¹å¾éç©ºï¼Œåˆ™è·å–åµŒå…¥å‘é‡\n",
    "    if face_representation:\n",
    "        embedding = face_representation[0]['embedding']\n",
    "        train_emb_list.append(embedding)\n",
    "\n",
    "# å°†æ‰€æœ‰é¢éƒ¨åµŒå…¥è½¬æ¢ä¸º tensor\n",
    "train_emb = torch.tensor(train_emb_list)\n",
    "\n",
    "# è°ƒç”¨ evaluate å‡½æ•°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½\n",
    "face_score, clip_score, mis = evaluate(\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,  # é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„\n",
    "    weight_dtype=weight_dtype,  # æ¨¡å‹çš„æƒé‡æ•°æ®ç±»å‹\n",
    "    seed=seed,  # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤çš„æ¨ç†ç»“æœ\n",
    "    unet_path=unet_path,  # è®­ç»ƒä¿å­˜çš„ UNet æ¨¡å‹çš„æƒé‡æ–‡ä»¶è·¯å¾„\n",
    "    text_encoder_path=text_encoder_path,  # è®­ç»ƒä¿å­˜çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹çš„æƒé‡æ–‡ä»¶è·¯å¾„\n",
    "    validation_prompt=validation_prompt,  # éªŒè¯æ—¶ä½¿ç”¨çš„æ–‡æœ¬æç¤ºï¼Œè¾“å…¥æ¨¡å‹ç”Ÿæˆå›¾åƒ\n",
    "    output_folder=inference_path,  # æ¨ç†æ—¶ç”Ÿæˆçš„å›¾åƒä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶å¤¹\n",
    "    train_emb=train_emb  # ä½¿ç”¨çš„è®­ç»ƒå›¾åƒçš„é¢éƒ¨ç‰¹å¾åµŒå…¥ï¼Œç”¨äºè¯„ä¼°é¢éƒ¨ç›¸ä¼¼åº¦\n",
    ")\n",
    "\n",
    "# æ‰“å°è¯„ä¼°ç»“æœ\n",
    "print(\"é¢éƒ¨ç›¸ä¼¼åº¦è¯„åˆ†:\", face_score)\n",
    "print(\"CLIP è¯„åˆ†:\", clip_score)\n",
    "print(\"æ— é¢éƒ¨å›¾åƒæ•°é‡:\", mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cea9f-314a-49ed-9b7c-20a590b298bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
