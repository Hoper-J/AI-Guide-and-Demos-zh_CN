> æ€ä¹ˆåˆ¤æ–­ Prompt çš„å¥½åï¼Œæœ‰ä»€ä¹ˆé—®é¢˜æœ‰ç€æ ‡å‡†ç­”æ¡ˆä¹ˆï¼Ÿ
>
> ç­”ï¼šè®©å¤§æ¨¡å‹æ±‚è§£æ•°å­¦é—®é¢˜ã€‚
>
> æå®æ¯…è€å¸ˆçš„ [HW4](https://www.bilibili.com/video/BV1BJ4m1e7g8?p=15&vd_source=436107f586d66ab4fcf756c76eb96c35) æ­£å¥½æåˆ°äº†æœ‰å…³æ•°å­¦é—®é¢˜çš„ Promptï¼Œæ‰€ä»¥æˆ‘å†³å®šä¸­é—´æ’ä¸€ç¯‡è¿™æ ·çš„æ–‡ç« ã€‚é€šè¿‡æœ¬æ–‡ä½ å°†ï¼š
>
> 1. äº†è§£å„ç§ Prompt å¦‚ä½•å½±å“å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚
> 2. è®¾è®¡ Prompt æé«˜å¤§æ¨¡å‹è§£å†³æ•°å­¦é—®é¢˜çš„æ­£ç¡®æ€§ã€‚
> 3. äº†è§£å ä½ç¬¦çš„ä½¿ç”¨ 
> 4. äº†è§£å¦‚ä½•ä½¿ç”¨ ipywidgets åˆ›å»ºäº¤äº’æ¨¡å—ã€‚
>
> å¦‚æœä½ å¯ä»¥è®¿é—® Geminiï¼Œå¯ä»¥åœ¨ [Google AI Studio](https://aistudio.google.com/app/apikey?hl=zh-cn) ä¸­éå¸¸å¿«é€Ÿçš„è·å– Gemini APIï¼Œä»è€Œåœ¨ Colab ä¸Šè¿›è¡Œå­¦ä¹ ï¼š[HW4 - Colab](https://colab.research.google.com/drive/16JzVN_Mu4mJfyHQpQEuDx1q6jI-cAnEl?hl=zh-tw#scrollTo=ZvU3-01m3wmy&uniqifier=1)ã€‚
>
> å› ä¸ºå›½å®¶æ”¿ç­–åŸå› ï¼Œè¿™é‡Œä¸ä¼šæä¾›ğŸªœçš„æ•™ç¨‹ï¼Œä½†æœ¬æ–‡ä¼šæ ¹æ® HW4 çš„å®Œæ•´å†…å®¹è¿›è¡Œç»„ç»‡ï¼Œå¹¶å°† Colab ä¸­çš„ @param ä½¿ç”¨ ipywidgets è¿›è¡Œæ›¿ä»£ï¼Œæä¾›ä¸€ä¸ªå¤§é™†ç‰ˆæœ¬çš„ä¸­æ–‡ä½œä¸šé•œåƒã€‚
>
> æ³¨æ„ï¼Œå°½ç®¡æˆ‘æåˆ°çš„æ˜¯**ä½œä¸š**ï¼Œä½†å¹¶ä¸æ„å‘³ç€ä½ éœ€è¦è§†é¢‘åŸºç¡€ï¼Œå…¶å®ä½ ä¹Ÿå¯ä»¥ç›´æ¥è¿›è¡Œå­¦ä¹ ï¼Œè¿™æ²¡æœ‰é—¨æ§›ã€‚
>
> [ä»£ç æ–‡ä»¶ä¸‹è½½](https://github.com/Hoper-J/LLM-Guide-and-Demos/blob/master/Demos/3.%20è‡ªå®šä¹‰%20Prompt%20æå‡å¤§æ¨¡å‹è§£é¢˜èƒ½åŠ›â€”â€”Gradio%20ä¸%20ipywidgetsç‰ˆ.ipynb)

[TOC]

# ä¸‹è½½ï¼Œå¯¼å…¥å’Œé…ç½®

```bash
pip install tqdm jinja2 gradio tiktoken openai
```

```python
import os
import time
import re
import pickle
import json
import traceback

import openai
import tiktoken  # ç”¨äº prompt_token_num()
import jinja2
from tqdm import tqdm
```

å¡«å……ä½ çš„`API`å¹¶è¿è¡Œä»£ç ï¼š
![image-20240911181401534](./assets/image-20240911181401534.png)

## åˆå§‹åŒ– OpenAI æ¨¡å‹

å®é™…ä¸Šå¦‚æœä¸“æ³¨äº Promptï¼Œå¯ä»¥æš‚æ—¶è·³è¿‡è¿™éƒ¨åˆ†ã€‚

```python
class OpenAIModel():
    def __init__(self, cache_file="openai_cache"):
        # åˆå§‹åŒ– OpenAI æ¨¡å‹å¯¹è±¡ï¼Œå¹¶è®¾ç½®ç¼“å­˜æ–‡ä»¶
        self.cache_file = cache_file
        self.cache_dict = self.load_cache()  # åŠ è½½ç¼“å­˜

    def save_cache(self):
        # å°†å½“å‰ç¼“å­˜ä¿å­˜åˆ°æ–‡ä»¶
        with open(self.cache_file, "wb") as f:
            pickle.dump(self.cache_dict, f)

    def load_cache(self, allow_retry=True):
        # ä»æ–‡ä»¶åŠ è½½ç¼“å­˜ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶
        if os.path.exists(self.cache_file):
            while True:
                try:
                    with open(self.cache_file, "rb") as f:
                        cache = pickle.load(f)
                    break
                except Exception:
                    if not allow_retry:
                        assert False
                    print("Pickle Error: 5ç§’åé‡è¯•...")
                    time.sleep(5)
        else:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ç¼“å­˜
            cache = {}
        return cache

    def set_cache_file(self, file_name):
        # è®¾ç½®ç¼“å­˜æ–‡ä»¶åå¹¶åŠ è½½ç¼“å­˜
        self.cache_file = file_name
        self.cache_dict = self.load_cache()

    def get_completion(self, content):
        # è·å–æ¨¡å‹å®Œæˆçš„æ–‡æœ¬ï¼Œå…ˆæ£€æŸ¥ç¼“å­˜ï¼Œè‹¥æ— åˆ™è¯·æ±‚ç”Ÿæˆ
        # å¦‚æœé€‰æ‹©æ£€æŸ¥ç¼“å­˜ï¼Œåˆ™ä¼šå¯¼è‡´åŒé—®é¢˜ä¸åŒtrialçš„ç»“æœç›¸åŒï¼Œè¿™ä¸å®é™…æƒ³è¡¨è¾¾çš„å†…å®¹ä¸ç¬¦ï¼Œæ•…æ³¨é‡Š
        # if content in self.cache_dict:
        #     return self.cache_dict[content]
        for _ in range(3):
            try:
                # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹
                response = client.chat.completions.create(
                    model="qwen-turbo",
                    messages=[{'role': 'user', 'content': content}],
                    temperature=1.0,
                )
                completion = response.choices[0].message.content
                self.cache_dict[content] = completion
                return completion
            except Exception as e:
                print(e, "\n")
                time.sleep(1)
        return None

    def is_valid_key(self):
        # æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æœ‰æ•ˆ
        for _ in range(4):
            try:
                response = client.chat.completions.create(
                    model="qwen-turbo",
                    messages=[{'role': 'user', 'content': "hi there"}],
                    temperature=1.0,
                    max_tokens=1
                )
                return True
            except Exception as e:
                traceback.print_exc()
                time.sleep(1)
        return False

    def prompt_token_num(self, prompt):
        # ä½¿ç”¨ tiktoken æ¥è®¡ç®— token æ•°é‡
        try:
            # ä½¿ç”¨ gpt-3.5-turbo çš„ç¼–ç å™¨ï¼Œå› ä¸º tiktoken åº“ä¸æ”¯æŒè‡ªåŠ¨è¯†åˆ« qwen-turbo æ¨¡å‹
            encoding = tiktoken.get_encoding("cl100k_base")  # è¿™æ˜¯ GPT-3.5-turbo æ‰€ä½¿ç”¨çš„ç¼–ç å™¨
            # å°† prompt ç¼–ç æˆ tokenï¼Œå¹¶è¿”å› token æ•°é‡
            tokens = encoding.encode(prompt)
            return len(tokens)
        except Exception as e:
            print(f"è®¡ç®— token æ•°é‡æ—¶å‡ºé”™: {e}")
            return 0

    def two_stage_completion(self, question, content):
        # ä¸¤é˜¶æ®µå®Œæˆï¼šé¦–å…ˆè·å–æ¨ç†ï¼Œå†è·å–æœ€ç»ˆç­”æ¡ˆ
        rationale = self.get_completion(content)
        if not rationale:
            return {
                'prompt': content,
                'rationale': None,
                'answer': None
            }

        ans = self.get_completion(content=f"Q:{question}\nA:{rationale}\nThe answer to the original question is (a number only): ")
        return {
            'prompt': content,
            'rationale': rationale,
            'answer': ans
        }

# åˆå§‹åŒ–æ¨¡å‹
my_model = OpenAIModel()
```

# ğŸ§™ åˆ›å»ºä½ çš„è‡ªå®šä¹‰ Promptï¼ˆGradio ç‰ˆæœ¬ï¼‰

ä»¥ä¸‹å®Œå…¨åŸºäºä½ ä¸‹è½½äº†ä»£ç æ–‡ä»¶æˆ–è€…ä½¿ç”¨äº†Colabè¿›è¡Œã€‚ä½ éœ€è¦æš‚æ—¶å¿½ç•¥ä»£ç ç»†èŠ‚ï¼Œä¸“æ³¨äº Prompt è®¾è®¡ã€‚

ä½ éœ€è¦ä¸€ç›´è¿è¡Œæä¾›çš„ä»£ç ï¼š

![image-20240911203422857](./assets/image-20240911203422857.png)

ç›´åˆ°çœ‹åˆ°ä¸€ä¸ªè¿™æ ·çš„äº¤äº’ç•Œé¢ï¼š

![image-20240911180721090](./assets/image-20240911180721090.png)

## è®¾è®¡Promptè§£å†³æ•°å­¦é—®é¢˜

ç°åœ¨éœ€è¦è®¾è®¡ä½ è‡ªå·±çš„  Promptï¼Œå¡«å†™åœ¨ `Custom Prompt` ä¸­ï¼Œæ³¨æ„ï¼Œä½ çš„ Prompt ä¸­éœ€è¦åŒ…å« `{{question}}`ï¼Œè¿™å°†ä½œä¸ºä¸€ä¸ªå ä½ç¬¦ï¼Œåç»­è¢« `Demo Example` æ˜¾ç¤ºçš„é—®é¢˜æ›¿æ¢ã€‚

![å ä½ç¬¦](./assets/%E5%8D%A0%E4%BD%8D%E7%AC%A6-6055722.png)

ä½¿ç”¨ `Shift+Enter` å¯ä»¥åœ¨æ–‡æœ¬æ¡†ä¸­æ¢è¡Œã€‚åœ¨è®¾è®¡å®Œæˆä¹‹åï¼Œç‚¹å‡» `Set Prompt`è®¾ç½®å½“å‰ Promptã€‚

![image-20240911191726959](./assets/image-20240911191726959.png)

è®¾ç½®çš„ç»“æœå¯ä»¥ç‚¹å‡» `Log` æŸ¥çœ‹ï¼š

![image-20240911192313607](./assets/image-20240911192313607.png)

å›åˆ° `Console` ç•Œé¢ï¼Œå¦‚æœæƒ³é‡æ–°è®¾ç½® Promptï¼Œç‚¹å‡» `Clear Prompt` æ¸…é™¤å·²è¾“å…¥çš„ä»»ä½•è‡ªå®šä¹‰æç¤ºè¯ï¼š

![image-20240911192803999](./assets/image-20240911192803999.png)

åœ¨ç‚¹å‡» `Evalute` è¿›è¡Œè¯„ä¼°ä¹‹å‰ï¼Œä½ éœ€è¦äº†è§£å¯¹åº”çš„æ¦‚å¿µï¼š

- `Number of prompt tokens`
  æ˜¾ç¤ºå½“å‰ Prompt çš„ Token æ•°é‡ï¼Œä½œä¸šä¼šé™åˆ¶æœ€å¤§é•¿åº¦ä¸º 1024ã€‚

- `Number of examples used for evaluation` 
  æ„å‘³ç€æˆ‘ä»¬å°†å»è¯„ä¼°å¤šå°‘ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚

- `Trail ID`
  å¯¹æŒ‡å®šè¯„ä¼°çš„é—®é¢˜ï¼Œå°†è¿›è¡Œä¸‰æ¬¡æµ‹è¯•ã€‚

- `Question ID`

  å— `Number of examples used for evaluation `é™åˆ¶ï¼Œä¸ºé—®é¢˜æ•°é‡ã€‚

å‡è®¾æˆ‘ä»¬ä»…è¯„ä¼°å‰10ä¸ªé—®é¢˜ï¼Œè®¾ç½®`Number of examples used for evaluation` ä¸º10ï¼Œç‚¹å‡» `Evaluate`ã€‚

![image-20240911202157024](./assets/image-20240911202157024.png)

ä½ å¯ä»¥æ”¹å˜`Trail ID`å’Œ`Question ID`æ¥æŸ¥çœ‹æŸæ¬¡æµ‹è¯•ä¸‹å¯¹åº”é—®é¢˜çš„ç»“æœï¼Œå¹¶å¯ä»¥å¾—åˆ° 3 æ¬¡æµ‹è¯•ä¸‹ Prompt çš„å‡†ç¡®ç‡ã€‚

![image-20240911202343357](./assets/image-20240911202343357.png)

ä½ è¿˜å¯ä»¥ç‚¹å‡» `Log` è¿›ä¸€æ­¥æŸ¥çœ‹ç»†èŠ‚ï¼š

![image-20240911202538505](./assets/image-20240911202538505.png)

# ğŸ§™ åˆ›å»ºä½ çš„è‡ªå®šä¹‰ Promptï¼ˆé Gradio ç‰ˆæœ¬ï¼‰

è¿™ä¸ªç‰ˆæœ¬å°†ä¸æ¶‰åŠ Gradioï¼Œä½¿ç”¨ ipywidgets æ¥åˆ›å»ºäº¤äº’ç•Œé¢ã€‚

å®é™…ä¸Šè¿™é‡Œåªæ˜¯ä¸€ä¸ªæ‹“å±•ï¼Œå¦‚æœåªæ˜¯æƒ³ç»ƒä¹  Prompt çš„ä½¿ç”¨ï¼ŒæŸ¥çœ‹ Gradio ç‰ˆæœ¬å³å¯ã€‚

## å¯¼å…¥

```python
import ipywidgets as widgets
from IPython.display import display
```

## è‡ªå®šä¹‰ Prompt

```python
import ipywidgets as widgets
from IPython.display import display

# åˆ›å»ºæ–‡æœ¬åŒºåŸŸã€æŒ‰é’®å’Œè¾“å‡ºåŒºåŸŸ
prompt_area = widgets.Textarea(placeholder="åœ¨æ­¤è¾“å…¥ä½ çš„è‡ªå®šä¹‰æç¤ºè¯")
prompt_area_desc = widgets.HTML(value="<p><b>Custom Prompt:</b></p>")
setprompt_btn = widgets.Button(description="Set Prompt")
resetprompt_btn = widgets.Button(description="Clear Prompt")
display_output = widgets.Output()

# åˆå§‹åŒ–è‡ªå®šä¹‰æç¤ºè¯å˜é‡
custom_prompt = ""

# å®šä¹‰â€œAssign Promptâ€æŒ‰é’®ç‚¹å‡»äº‹ä»¶
def set_prompt_clk(b):
    global custom_prompt
    custom_prompt = prompt_area.value  # è·å–è¾“å…¥æ¡†ä¸­çš„æç¤ºè¯
    prompt_area.disabled = True  # ç¦ç”¨è¾“å…¥æ¡†
    with display_output:
        display_output.clear_output()  # æ¸…é™¤ä¹‹å‰çš„è¾“å‡º
        print("Prompt å·²åˆ†é…ï¼š", custom_prompt)  # æ‰“å°å·²åˆ†é…çš„æç¤ºè¯

# å®šä¹‰â€œClear Promptâ€æŒ‰é’®ç‚¹å‡»äº‹ä»¶
def reset_prompt_clk(b):
    prompt_area.disabled = False  # é‡æ–°å¯ç”¨è¾“å…¥æ¡†
    prompt_area.value = ""  # æ¸…ç©ºè¾“å…¥æ¡†
    with display_output:
        display_output.clear_output()  # æ¸…é™¤ä¹‹å‰çš„è¾“å‡º
        print("æç¤ºè¯å·²é‡ç½®")  # æç¤ºå·²é‡ç½®

# ç»‘å®šæŒ‰é’®ç‚¹å‡»äº‹ä»¶
setprompt_btn.on_click(set_prompt_clk)
resetprompt_btn.on_click(reset_prompt_clk)

# æ˜¾ç¤ºç»„ä»¶
display(prompt_area_desc, prompt_area, setprompt_btn, resetprompt_btn, display_output)
```

åœ¨æ–‡æœ¬æ¡†ä¸­å¡«å†™ä½ çš„ Promptï¼š

![image-20240911203200685](./assets/image-20240911203200685.png)

### åˆ›å»ºä¸‹æ‹‰é€‰é¡¹é€‰æ‹©é—®é¢˜

è¿™éƒ¨åˆ†å®ç°Colabä¸­çš„ `Demo_Example = "7" # @param [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30] {type:"string"}`

```python
# åˆ›å»ºä¸‹æ‹‰èœå•ï¼Œå…è®¸ç”¨æˆ·é€‰æ‹© 1 åˆ° 30 ä¹‹é—´çš„æ•°å­—
demo_example_dropdown = widgets.Dropdown(
    options=[str(i) for i in range(1, 31)],  # é€‰é¡¹ä¸ºå­—ç¬¦ä¸²
    value="7",  # é»˜è®¤å€¼
    description='ç¤ºä¾‹ç¼–å·:',
)

# åˆ›å»ºè¾“å‡ºåŒºåŸŸ
output_demo_example = widgets.Output()

# å®šä¹‰ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œå½“ç”¨æˆ·é€‰æ‹©æ–°å€¼æ—¶è§¦å‘
def on_dropdown_change(change):
    global Demo_Example  # ä½¿ç”¨å…¨å±€å˜é‡
    Demo_Example = change['new']  # è·å–ä¸‹æ‹‰èœå•çš„æ–°å€¼
    with output_demo_example:
        output_demo_example.clear_output()  # æ¸…é™¤ä¹‹å‰çš„è¾“å‡º
        print(f"å·²é€‰æ‹©çš„ç¤ºä¾‹ç¼–å·æ˜¯: {Demo_Example}")

# ç›‘å¬ä¸‹æ‹‰èœå•çš„å˜åŒ–
demo_example_dropdown.observe(on_dropdown_change, names='value')

# æ˜¾ç¤ºä¸‹æ‹‰èœå•å’Œè¾“å‡ºåŒºåŸŸ
display(demo_example_dropdown, output_demo_example)
```

é»˜è®¤é€‰æ‹©ç¤ºä¾‹7:

![image-20240911210412806](./assets/image-20240911210412806.png)

æŸ¥çœ‹å ä½ç¬¦çš„ä½œç”¨ï¼š

```python
# ä»æ–‡æœ¬æ¡†è·å–ç”¨æˆ·è¾“å…¥çš„è‡ªå®šä¹‰æç¤ºè¯
custom_prompt = prompt_area.value
assert "{{question}}" in custom_prompt, "æç¤ºè¯ä¸­å¿…é¡»åŒ…å« '{{question}}' å ä½ç¬¦ï¼"

# é€šè¿‡ä¸Šé¢çš„ä¸‹æ‹‰é€‰é¡¹é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹ï¼Œä½ å¯ä»¥é€‰æ‹©1åˆ°30ä¹‹é—´çš„ç¼–å· 
demo_index = eval(Demo_Example)  # å°†å­—ç¬¦ä¸²å½¢å¼çš„æ•°å­—è½¬ä¸ºæ•´æ•°

# åˆå§‹åŒ– jinja2 ç¯å¢ƒå¹¶æ¸²æŸ“æ¨¡æ¿
environment = jinja2.Environment()
template = environment.from_string(custom_prompt)

# è¾“å‡ºç”Ÿæˆçš„è‡ªå®šä¹‰æç¤ºè¯ç¤ºä¾‹
print(f"è‡ªå®šä¹‰æç¤ºè¯ç¤ºä¾‹ï¼š\n\n{template.render(question=questions[demo_index-1])}")
```

å¯ä»¥çœ‹åˆ°åŸæ¥å ä½ç¬¦çš„ä½ç½®è¢«æ›¿æ¢ä¸ºäº†ç¬¬7ä¸ªé—®é¢˜ã€‚

![image-20240911210824494](./assets/image-20240911210824494.png)

## è¯„ä¼°

### åˆ›å»ºæ»‘å—é€‰æ‹©è¯„ä¼°çš„æ•°é‡

è¿™éƒ¨åˆ†ç”¨äºå®ç°Colabä¸­çš„ `eval_num = 5 # @param {type:"slider", min:1, max:30, step:1}`

```python
# åˆ›å»ºæ»‘å—ï¼ŒèŒƒå›´ä¸º 1 åˆ° 30ï¼Œæ­¥é•¿ä¸º 1ï¼Œé»˜è®¤å€¼ä¸º 5
eval_slider = widgets.IntSlider(
    value=5,
    min=1,
    max=30,
    step=1,
    description='é€‰æ‹©è¯„ä¼°æ•°:', 
    continuous_update=False  # æ»‘å—æ”¾å¼€åæ‰æ›´æ–°
)

# åˆ›å»ºè¾“å‡ºåŒºåŸŸ
output = widgets.Output()

# åˆå§‹åŒ–ä¸ºæ»‘å—çš„é»˜è®¤å€¼
eval_num = eval_slider.value  

# å®šä¹‰ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œæ»‘å—å˜åŒ–æ—¶ä¼šè§¦å‘
def on_slider_change(change):
    global eval_num
    eval_num = change['new']  # è·å–æ»‘å—çš„æ–°å€¼
    with output:
        output.clear_output()  # æ¸…é™¤ä¹‹å‰çš„è¾“å‡º
        print(f"å·²é€‰æ‹©çš„è¯„ä¼°æ•°æ˜¯: {eval_num}")

# ç›‘å¬æ»‘å—çš„å˜åŒ–
eval_slider.observe(on_slider_change, names='value')

# æ˜¾ç¤ºæ»‘å—å’Œè¾“å‡ºåŒºåŸŸ
display(eval_slider, output)
```

è¿™é‡Œçš„æ¼”ç¤ºé€‰æ‹©10ã€‚

![image-20240911211603141](./assets/image-20240911211603141.png)

å¼€å§‹è¯„ä¼°å‰10ä¸ªé—®é¢˜ä¸‹ Prompt çš„æ­£ç¡®ç‡ï¼š

```python
assert 1 <= eval_num <= 30

# å®šä¹‰æ˜¾ç¤ºç»“æœçš„æ¨¡æ¿
ans_template = """Prompt with Question:\n\n{{question}}\n\n--------------------\n\nProblem-solving Process:\n\n{{rationale}}\n\n--------------------\n\nFinal Answer\n\n{{answer}}"""

res_list = []
test_num = eval_num  # è¦è¯„ä¼°çš„é—®é¢˜æ•°é‡
total_count = test_num

# å°† ans_template å­—ç¬¦ä¸²è½¬æ¢ä¸º jinja2 æ¨¡æ¿å¯¹è±¡
environment = jinja2.Environment()
ans_template = environment.from_string(ans_template)

# åˆå§‹åŒ–è®¡æ•°å™¨ä»¥è·Ÿè¸ªå‡†ç¡®å›ç­”çš„æ¬¡æ•°
trial_num = 3  # è¿›è¡Œä¸‰æ¬¡è¯•éªŒ
trials = [[] for _ in range(trial_num)]
res_stats_str = ""


def clean_commas(text):
    # è¯¥å‡½æ•°ç”¨äºæ¸…ç†æ•°å­—ä¸­çš„é€—å·ï¼Œå¹¶ä¿ç•™æµ®ç‚¹æ•°ä¸­çš„é€—å·
    def process_match(match):
        number = match.group(0)
        if '.' in number:
            return number  # ä¿ç•™æµ®ç‚¹æ•°
        else:
            # å»æ‰æ•°å­—ä¸­çš„é€—å·
            number_list = number.split(",")
            new_string = number_list[0]
            for i in range(1, len(number_list)):
                if len(number_list[i]) == 3:  # è¿™æ˜¯åƒä½åˆ†éš”ç¬¦
                    new_string += number_list[i]
                else:
                    new_string += f",{number_list[i]}"
            return new_string

    pattern = r'\d+(?:,\d+)*(?:\.\d+)?'
    return re.sub(pattern, process_match, text)


def find_and_match_floats(input_string, ground_truth):
    # åŒ¹é…è¾“å…¥å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰æµ®ç‚¹æ•°å’Œæ•´æ•°
    pattern = re.compile(r"[-+]?\d*\.\d+|[-+]?\d+")
    found_numbers = pattern.findall(input_string)
    found_floats = [float(num) for num in found_numbers]
    return ground_truth in found_floats


for i in range(trial_num):

    print(f"Start trial {i+1}")
    my_model.set_cache_file(f"gemini_cache_trial_{i+1}")
    accurate_count = 0

    # éå†æ¯ä¸ªè¦è¯„ä¼°çš„ç¤ºä¾‹
    for idx, example in enumerate(questions[:test_num]):
        test_res = ""

        result = my_model.two_stage_completion(example, template.render(question=example))

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿”å›äº†æœ‰æ•ˆç­”æ¡ˆ
        if not result["answer"]:
            trials[i].append(0)
            test_res += f"Trial {i+1}\n\n Skip question {idx + 1}."
            test_res += "\n" + "<"*6 + "="*30 + ">"*6 + "\n\n"
            res_list.append(f"Trial {i+1}\n\n Skip question {idx + 1}.")
            continue

        # æ¸…ç†ç­”æ¡ˆä¸­çš„é€—å·å¹¶ä¸åœ°é¢çœŸå€¼è¿›è¡Œæ¯”è¾ƒ
        cleaned_result = clean_commas(result["answer"])
        if find_and_match_floats(cleaned_result, answers[idx]) or idx in [0, 26]:
            accurate_count += 1
            trials[i].append(1)
        else:
            trials[i].append(0)

        # ä¿å­˜æ¨¡å‹çš„ç¼“å­˜
        my_model.save_cache()

        test_res += f"Trial {i + 1}\n\n"
        test_res += f"Question {idx + 1}:\n" + '-'*20
        test_res += f'''\n\n{ans_template.render(question=result['prompt'], rationale=result['rationale'], answer=result['answer'])}\n'''
        test_res += "\n" + "<"*6 + "="*30 + ">"*6 + "\n\n"
        res_list.append(test_res)

        time.sleep(1)

    # æ‰“å°å‡†ç¡®ç‡ç»Ÿè®¡
    res_stats_str += f"Trial {i + 1}, accurate_count: {accurate_count}, total_count: {total_count}, accuracy: {accurate_count / total_count * 100}%\n"
    my_model.save_cache()

## å¤šæ•°æŠ•ç¥¨è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
voting_acc = 0
for i in range(total_count):
    count = 0
    for j in range(trial_num):
        if trials[j][i] == 1:
            count += 1
    if count >= 2:
        voting_acc += 1

res_stats_str += f"Final Accuracy: {voting_acc / total_count * 100}%"

print(f"Final accuracy: {res_stats_str}")
```

ç”¨å¤šæ•°æŠ•ç¥¨æ¥è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡ï¼š

![image-20240911221309441](./assets/image-20240911221309441.png)

## æ‰“å°æŒ‡å®šçš„è¯„ä¼°ç»“æœ

```python
# å®šä¹‰ trial_id å’Œ question_id çš„è¾“å…¥æ¡†
trial_id_input = widgets.IntText(
    value=3,  # é»˜è®¤å€¼
    description='Trial ID:',
)

question_id_input = widgets.IntText(
    value=1,  # é»˜è®¤å€¼
    description='Question ID:',
)

# å¦‚æœä½ æƒ³å®šä¹‰ trial_id å’Œ question_id çš„æ»‘å—çš„è¯ä½¿ç”¨ä¸‹é¢çš„ä»£ç 
"""
trial_id_input = widgets.IntSlider(
    value=3,  # é»˜è®¤å€¼
    min=1,    # æœ€å°å€¼
    max=3,    # æœ€å¤§å€¼
    step=1,   # æ­¥é•¿
    description='Trial ID:',
    continuous_update=False  # æ»‘å—æ”¾å¼€åæ‰æ›´æ–°
)

question_id_input = widgets.IntSlider(
    value=1,  # é»˜è®¤å€¼
    min=1,    # æœ€å°å€¼
    max=eval_num,   # æœ€å¤§å€¼ï¼ˆæ ¹æ®å®é™… eval_num çš„èŒƒå›´è°ƒæ•´ï¼‰
    step=1,   # æ­¥é•¿
    description='Question ID:',
    continuous_update=False  # æ»‘å—æ”¾å¼€åæ‰æ›´æ–°
)
"""

# æ˜¾ç¤ºè¾“å‡º
output_result = widgets.Output()

# å®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºè¯„ä¼°ç”¨æˆ·é€‰æ‹©çš„å€¼
def on_evaluate(change):
    with output_result:
        output_result.clear_output()  # æ¸…é™¤ä¹‹å‰çš„è¾“å‡º
        trial_id = trial_id_input.value
        question_id = question_id_input.value
        
        if trial_id not in [1, 2, 3]:
            print("trial_id åªèƒ½æ˜¯ 1, 2 æˆ– 3ã€‚")
        elif question_id not in [i for i in range(1, eval_num + 1)]:
            print(f"question_id åªèƒ½åœ¨ 1 åˆ° {eval_num} ä¹‹é—´ã€‚")
        else:
            result_index = (trial_id - 1) * eval_num + question_id - 1
            print(f"ç¬¬ {trial_id} æ¬¡è¯•éªŒä¸­ï¼Œç¬¬ {question_id} ä¸ªé—®é¢˜çš„è¯„ä¼°ç»“æœæ˜¯:\n{res_list[result_index]}")

# ç›‘å¬å€¼å˜åŒ–å¹¶æ‰§è¡Œè¯„ä¼°é€»è¾‘
trial_id_input.observe(on_evaluate, names='value')
question_id_input.observe(on_evaluate, names='value')

# æ˜¾ç¤ºæ»‘å—å’Œè¾“å‡º
display(trial_id_input, question_id_input, output_result)
```

å¯ä»¥çœ‹åˆ°å®é™…ä¸Š ipywidget ä¹Ÿå¯ä»¥æä¾›ä¸€ä¸ªéå¸¸ç›´è§‚çš„ç•Œé¢ï¼ˆè™½ç„¶ä¸å¤Ÿç¾è§‚ï¼‰ï¼š

![image-20240911212506784](./assets/image-20240911212506784.png)

## ä¿å­˜ä½ çš„ Propmt

å¦‚æœä½ éœ€è¦çš„è¯ã€‚

```python
prompt_dict = {
    'prompt': custom_prompt
}

with open('prompt.json', 'w') as f:
    json.dump(prompt_dict, f)

print("Prompt å·²ä¿å­˜ä¸º prompt.json æ–‡ä»¶")
```



# å‚è€ƒé“¾æ¥

[HW4 è§†é¢‘](https://www.bilibili.com/video/BV1BJ4m1e7g8?p=15&vd_source=436107f586d66ab4fcf756c76eb96c35) 

[HW4 - Colab](https://colab.research.google.com/drive/16JzVN_Mu4mJfyHQpQEuDx1q6jI-cAnEl?hl=zh-tw#scrollTo=ZvU3-01m3wmy&uniqifier=1)

